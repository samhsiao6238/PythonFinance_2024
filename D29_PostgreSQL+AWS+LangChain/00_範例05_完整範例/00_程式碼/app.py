import streamlit as st
from PyPDF2 import PdfReader
from langchain.vectorstores.pgvector import PGVector
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms.bedrock import Bedrock
from langchain.embeddings import BedrockEmbeddings
from langchain.document_loaders import S3FileLoader
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.document_loaders import YoutubeLoader
from langchain.document_loaders import UnstructuredPowerPointLoader
from langchain.document_loaders import Docx2txtLoader
from langchain.memory import PostgresChatMessageHistory
import boto3
import tempfile
import time
import hashlib
import secrets
import os
from dotenv import load_dotenv
import logging

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
# æ³¨æ„ï¼Œ`environ.get` æˆ– `os.getenv` å…©è€…åœ¨æœ¬è³ªä¸Šæ˜¯æœ‰å·®ç•°çš„
# ä½†æˆ‘åå‘ç¨‹å¼ç¢¼æ•´æ½”ï¼Œæ‰€ä»¥åœ¨ç¨‹åºä¸€é–‹å§‹ä¹‹è™•å°±è¼‰å…¥ `load_dotenv()`
# é€™æ¨£çš„è¼‰å…¥æ–¹å¼ä½¿ç”¨ `environ.get` æˆ– `os.getenv` ä¸¦ç„¡å·®ç•°
PGVECTOR_DRIVER = os.getenv("PGVECTOR_DRIVER")
PGVECTOR_USER = os.getenv("PGVECTOR_USER")
PGVECTOR_PASSWORD = os.getenv("PGVECTOR_PASSWORD")
PGVECTOR_HOST = "localhost"
PGVECTOR_PORT = "5432"
PGVECTOR_DATABASE = "mydatabase"

logging.getLogger("botocore").setLevel(logging.ERROR)


def debug_01():
    # æª¢æŸ¥è³‡æ–™åº«çš„ URI æ˜¯å¦æ­£ç¢º
    CONNECTION_STRING = f"://{PGVECTOR_USER}:{PGVECTOR_PASSWORD}@{PGVECTOR_HOST}:{PGVECTOR_PORT}/{PGVECTOR_DATABASE}"
    # ç¢ºä¿æ­£ç¢ºæ‰“å°é€£æ¥å­—ä¸²ï¼Œä¾¿æ–¼æª¢æŸ¥
    print(f"CONNECTION_STRING: {CONNECTION_STRING}")


def styled_header(text):
    header_html = f"""
    <div style="background-color:#4CAF50;text-align:center;padding:10px">
    <h1 style="color:white;text-align:center;">{text}</h1>
    </div>
    """
    return header_html


def styled_subheader(
    text,
    font_size="24px",
    color="#8A2BE2",
    background_color="#f0e5ff"
):
    subheader_html = f"""
    <div style="box-shadow:0 2px 10px #ddd; padding: 5px;"
    " background-color: {background_color};"
    " border-radius: 5px; margin: 10px 0; text-align: center;">
        <h3 style="color: {color}; font-size: {font_size};"
        " margin: 0;">{text}</h3>
    </div>
    """
    return subheader_html


def generate_session_id():
    t = int(time.time() * 1000)
    r = secrets.randbelow(1000000)
    return hashlib.md5(
        bytes(str(t) + str(r), "utf-8"), usedforsecurity=False
    ).hexdigest()


def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            # text += page.extract_text()
            # æ”¹ç”¨ page_textï¼Œå¾Œé¢å†å‚³å›
            page_text = page.extract_text()
            # éæ¿¾æ‰ NUL å­—ä¸²
            if page_text:
                text += page_text.replace('\x00', '')
    return text


def get_text_chunks(text):
    # ç¢ºä¿åœ¨åˆ†å‰²æ–‡æœ¬å¡Šå‰éæ¿¾æ‰ NUL å­—ä¸²
    cleaned_text = text.replace('\x00', '')
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
        chunk_size=512,
        chunk_overlap=103,
        length_function=len,
    )
    # chunks = text_splitter.split_text(text)
    chunks = text_splitter.split_text(cleaned_text)
    return chunks


def get_vectorstore(text_chunks):
    embeddings = BedrockEmbeddings(
        model_id="amazon.titan-embed-text-v2:0",
        # é è¨­æ˜¯ `-2`ï¼Œé€™è£¡æ”¹ä¸€ä¸‹
        region_name="us-east-1"
    )
    try:
        if text_chunks is None:
            return PGVector(
                connection_string=CONNECTION_STRING,
                embedding_function=embeddings,
            )
        return PGVector.from_texts(
            texts=text_chunks,
            embedding=embeddings,
            connection_string=CONNECTION_STRING
        )
    except Exception as e:
        # åŸæœ¬çš„è…³æœ¬æ˜¯å¼•ç™¼éŒ¯èª¤
        # raise e
        # æ”¹ç‚ºè¼¸å‡ºéŒ¯èª¤çœ‹ä¸€ä¸‹
        print(f'å–å¾—å‘é‡å„²å­˜ç™¼ç”ŸéŒ¯èª¤ï¼š{e}')


def get_bedrock_llm(selected_llm):
    print(f"[INFO] Selected LLM is : {selected_llm}")
    if selected_llm in [
        "anthropic.claude-v2",
        "anthropic.claude-v1",
        "anthropic.claude-instant-v1",
    ]:
        llm = Bedrock(
            model_id=selected_llm,
            model_kwargs={"max_tokens_to_sample": 4096}
        )

    elif selected_llm in [
        "amazon.titan-tg1-large",
        "amazon.titan-text-express-v1",
        "amazon.titan-text-lite-v1",
    ]:
        llm = Bedrock(
            model_id=selected_llm,
            model_kwargs={
                "maxTokenCount": 4096,
                "stopSequences": [],
                "temperature": 0,
                "topP": 1,
            },
        )
    else:
        raise ValueError(f"Unsupported LLM: {selected_llm}")

    return llm


def get_conversation_chain(vectorstore, selected_llm):
    # è¨»è§£æ‰
    # llm = Bedrock(
    #     model_id="anthropic.claude-instant-v1",
    #     region_name="us-west-2"
    # )
    llm = get_bedrock_llm(selected_llm)
    _connection_string = CONNECTION_STRING.replace(
        "+psycopg2", "").replace(":5432", "")
    message_history = PostgresChatMessageHistory(
        connection_string=_connection_string, session_id=generate_session_id()
    )
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        chat_memory=message_history,
        return_source_documents=True,
        return_messages=True,
    )
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm, retriever=vectorstore.as_retriever(), memory=memory
    )
    return conversation_chain


def color_text(text, color="black"):
    return f'<span style="color:{color}">{text}</span>'


bot_template = "ğŸ¤– BOT : {0}"
user_template = "ğŸ‘¤ USER : {0}"


def handle_userinput(user_question):
    bot_template = "ğŸ¤– BOT : {0}"
    user_template = "ğŸ‘¤ USER : {0}"
    try:
        response = st.session_state.conversation({"question": user_question})
        st.markdown(
            color_text(
                user_template.format(response["question"]),
                color="blue"
            ),
            unsafe_allow_html=True,
        )
        st.markdown(
            color_text(
                bot_template.format(response["answer"]),
                color="green"
            ),
            unsafe_allow_html=True,
        )
        print("Response", response)
    except ValueError as e:
        st.write(e)
        st.write("ğŸ˜ æŠ±æ­‰ï¼Œè«‹æ›å€‹æ–¹å¼å†å•ä¸€æ¬¡ã€‚")
        return
    st.session_state.chat_history = response["chat_history"]
    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.markdown(
                color_text(
                    user_template.format(message.content),
                    color="blue"
                ),
                unsafe_allow_html=True,
            )

        else:
            st.markdown(
                color_text(
                    bot_template.format(message.content),
                    color="green"
                ),
                unsafe_allow_html=True,
            )


def main():
    # Updated header styling
    st.markdown(
        styled_header(
            "Unified AI Q&A: Harnessing pgvector, Amazon Aurora"
            " & Amazon Bedrock ğŸ“šğŸ¦œ"
        ),
        unsafe_allow_html=True,
    )

    options = [
        "ğŸ“„ PDFs",
        "â˜ï¸ S3 Bucket",
        "ğŸ“º Youtube",
        "ğŸ“‘ CSV", "ğŸ–¼ï¸ PPT",
        "ğŸ“ Word"
    ]
    # ä½¿ç”¨å–®é¸æŒ‰éˆ•è€Œä¸æ˜¯é¸æ“‡æ¡†
    st.markdown(
        styled_subheader("ğŸ“Œ Select a source ğŸ“Œ"),
        unsafe_allow_html=True
    )
    selected_source = st.radio("", options)

    # æ·»åŠ  LLM é¸å–®å…ƒä»¶
    st.markdown(
        styled_subheader("ğŸ¤– Select the LLM ğŸ¤–"),
        unsafe_allow_html=True
    )
    llm_options = [
        "anthropic.claude-v2",
        "anthropic.claude-instant-v1",
        "amazon.titan-tg1-large",
        "amazon.titan-text-express-v1",
        "amazon.titan-text-lite-v1",
    ]

    selected_llm = st.radio("Choose an LLM", options=llm_options)

    # PDF
    if selected_source == "ğŸ“„ PDFs":
        pdf_docs = st.file_uploader(
            "ğŸ“¥ Upload your PDFs here:",
            type="pdf",
            accept_multiple_files=True
        )
        if st.button("ğŸ”„ Process"):
            with st.spinner("ğŸ”§ Processing..."):
                raw_text = get_pdf_text(pdf_docs)
                text_chunks = get_text_chunks(raw_text)
                vectorstore = get_vectorstore(text_chunks)
                if vectorstore is None:
                    st.write("Failed to initialize vector store.")
                    return
                st.session_state.conversation = get_conversation_chain(
                    vectorstore, selected_llm
                )
    elif selected_source == "â˜ï¸ S3 Bucket":
        s3_client = boto3.client("s3")
        # é€™äº›ç‰©ä»¶å„²å­˜åœ¨ aurora-genai-2023 å„²å­˜æ¡¶ä¸­ã€‚è¼¸å…¥é©ç•¶çš„å„²å­˜æ¡¶åç¨±
        response = s3_client.list_objects_v2(
            # å°‡å„²å­˜æ¡¶åç¨±è®Šæ›´ç‚ºè‡ªå·±çš„å„²å­˜æ¡¶åç¨±
            Bucket="aurora-genai-2023",
            Prefix="documentEmbeddings/"
        )
        document_keys = [
            obj["Key"].split("/")[1]
            for obj in response["Contents"]
        ][1:]
        user_input = st.selectbox(
            "Select an S3 document and click on 'Process'",
            document_keys
        )
        if st.button("Process"):
            with st.spinner("Processing"):
                prefix = "documentEmbeddings/" + user_input
                loader = S3FileLoader("aurora-genai-2023", prefix)
                docs = loader.load()
                for i in docs:
                    text_chunks = get_text_chunks(i.page_content)
                    vectorstore = get_vectorstore(text_chunks)
                    if vectorstore is None:
                        st.write("Failed to initialize vector store.")
                        return
                st.session_state.conversation = get_conversation_chain(
                    vectorstore, selected_llm
                )
    elif selected_source == "ğŸ“‘ CSV":
        csv_docs = st.file_uploader(
            "Upload your CSV here and click on 'Process'",
            type="csv",
            accept_multiple_files=False,
        )
        if st.button("Process"):
            with st.spinner("Processing"):
                with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                    tmp_file.write(csv_docs.getvalue())
                    tmp_file_path = tmp_file.name
                loader = CSVLoader(
                    file_path=tmp_file_path,
                    encoding="utf-8",
                    csv_args={"delimiter": ","},
                )
                docs = loader.load()
                for i in docs:
                    text_chunks = get_text_chunks(i.page_content)
                    vectorstore = get_vectorstore(text_chunks)
                    if vectorstore is None:
                        st.write("Failed to initialize vector store.")
                        return
                st.session_state.conversation = get_conversation_chain(
                    vectorstore, selected_llm
                )
    elif selected_source == "ğŸ“º Youtube":
        user_input = st.text_input("è¼¸å…¥ YouTube é€£çµä¸¦é»æ“Šã€ŒProcessã€")
        if st.button("Process"):
            with st.spinner("Processing"):
                loader = YoutubeLoader.from_youtube_url(user_input)
                transcript = loader.load()
                for i in transcript:
                    text_chunks = get_text_chunks(i.page_content)
                    vectorstore = get_vectorstore(text_chunks)
                    if vectorstore is None:
                        st.write("ç„¡æ³•åˆå§‹åŒ–å‘é‡å„²å­˜ã€‚")
                        return
                st.session_state.conversation = get_conversation_chain(
                    vectorstore, selected_llm
                )

    elif selected_source == "ğŸ–¼ï¸ PPT":
        ppt_docs = st.file_uploader(
            "åœ¨æ­¤ä¸Šå‚³æ‚¨çš„ PPTï¼Œç„¶å¾ŒæŒ‰ä¸€ä¸‹ã€ŒProcessã€",
            type=["ppt", "pptx"],
            accept_multiple_files=False,
        )
        if st.button("Process"):
            with st.spinner("Processing"):
                with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                    tmp_file.write(ppt_docs.getvalue())
                    tmp_file_path = tmp_file.name
                loader = UnstructuredPowerPointLoader(tmp_file_path)
                docs = loader.load()
                for i in docs:
                    text_chunks = get_text_chunks(i.page_content)
                    vectorstore = get_vectorstore(text_chunks)
                    if vectorstore is None:
                        st.write("ç„¡æ³•åˆå§‹åŒ–å‘é‡å„²å­˜ã€‚")
                        return
                st.session_state.conversation = get_conversation_chain(
                    vectorstore, selected_llm
                )

    elif selected_source == "ğŸ“ Word":
        word_docs = st.file_uploader(
            "åœ¨æ­¤è™•ä¸Šå‚³æ‚¨çš„ Word æ–‡ä»¶ï¼Œç„¶å¾ŒæŒ‰ä¸€ä¸‹ã€ŒProcessã€",
            type=["docx"],
            accept_multiple_files=False,
        )
        if st.button("Process"):
            with st.spinner("Processing"):
                with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                    tmp_file.write(word_docs.getvalue())
                    tmp_file_path = tmp_file.name
                loader = Docx2txtLoader(tmp_file_path)
                docs = loader.load()
                for i in docs:
                    text_chunks = get_text_chunks(i.page_content)
                    vectorstore = get_vectorstore(text_chunks)
                    if vectorstore is None:
                        st.write("ç„¡æ³•åˆå§‹åŒ–å‘é‡å„²å­˜ã€‚")
                        return
                st.session_state.conversation = get_conversation_chain(
                    vectorstore, selected_llm
                )

    st.sidebar.header("ğŸ—£ï¸ èˆ‡æ©Ÿå™¨äººèŠå¤©")
    user_question = st.sidebar.text_input("ğŸ’¬ è©¢å•æœ‰é—œä½ æ‰€æä¾›æ•¸æ“šçš„å•é¡Œï¼š")
    if user_question:
        handle_userinput(user_question)

    if "conversation" not in st.session_state:
        st.session_state.conversation = get_conversation_chain(
            get_vectorstore(None), selected_llm
        )
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None


# è¼¸å…¥è³‡æ–™åº«åç¨±
if __name__ == "__main__":
    # è‹¥åœ¨ `__main__` ä¸­è¼‰å…¥å‰‡å¿…é ˆä½¿ç”¨ `environ.get` ä¾†è®€å–ä¸¦å¯«å…¥ç’°å¢ƒåƒæ•¸
    # load_dotenv()

    CONNECTION_STRING = PGVector.connection_string_from_db_params(
        driver=PGVECTOR_DRIVER,
        user=PGVECTOR_USER,
        password=PGVECTOR_PASSWORD,
        host=PGVECTOR_HOST,
        port=PGVECTOR_PORT,
        database=PGVECTOR_DATABASE,
    )

    main()
