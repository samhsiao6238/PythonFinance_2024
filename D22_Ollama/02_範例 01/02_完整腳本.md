# 建立專案


## 四個主要腳本

1. `app.py`。

```python
# 回調管理器
from langchain.callbacks.manager import CallbackManager
# 可用於回調管理器的方法，功能是即時輸出在終端機以利調適
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
# 棄用
# from langchain.llms import Ollama
# 改用 LangChain 社區維護的語言模型來導入 Ollama
from langchain_community.llms import Ollama
import streamlit as st
# 以下是三個自定義模組：mongodb、llm_strings、utils
from mongodb import EasyMongo
from llm_strings import LLMStrings
from utils import output_text, simulate_response, create_message


if __name__ == "__main__":
    # 初始化 Ollama 模型，這是 `langchain_community.llms` 導入的類
    llm = Ollama(
        # 使用 llama3 模型
        model="llama3",
        # 設置回調管理器
        callback_manager=CallbackManager(
            # 添加輸出的回調管理器，如此模型運作時，可在終端機中先行看到輸出
            [StreamingStdOutCallbackHandler()]
        )
    )

    # 標題
    st.title(LLMStrings.APP_TITLE)

    # 搭配上下文管理，輸出預設的歡迎詞
    with st.chat_message(LLMStrings.AI_ROLE):
        st.write(LLMStrings.GREETINGS)

    # 初始化聊天記錄，檢查是否存在 session_state
    if LLMStrings.SESSION_STATES not in st.session_state:
        # 確定是初次啟動服務，建立空列表
        st.session_state.messages = []

    # 緊接著連線資料庫 MongoDB
    mongo_server = EasyMongo()
    # 取得 MongoDB 中的聊天記錄
    collection_name = mongo_server.get_collection()

    # 調用 fine() 方法取得集合內容
    messages = collection_name.find()
    # 遍歷對話紀錄
    for message in messages:
        # 依據不同角色
        with st.chat_message(message[LLMStrings.ROLE_ID]):
            # 輸出訊息內容
            st.markdown(message[LLMStrings.CONTENT])

    # 回應使用者輸入，這裡使用了型別檢查
    if prompt := st.chat_input(LLMStrings.INPUT_PLACEHOLDER):

        with st.chat_message(LLMStrings.USER_ROLE):
            # 顯示使用者出入的問題
            st.markdown(prompt)
            # 調用 create_message 生成用戶問題字典
            user_content = create_message(LLMStrings.USER_ROLE, prompt)
            # 將用戶問題以字典型態寫入紀錄
            st.session_state.messages.append(user_content)

        # 啟動 spinner 並搭配上下文管理
        with st.spinner(LLMStrings.WAIT_MESSAGE):
            with st.chat_message(LLMStrings.AI_ROLE):
                # output_text 方法會生成回答
                response = output_text(llm, prompt)
                # 將回應組合為字典
                ai_content = create_message(LLMStrings.AI_ROLE, response)
                # 將字典寫入紀錄
                st.session_state.messages.append(ai_content)
                # 這是一個自訂的 `模擬終端回應` 的函數
                simulate_response(response)
                # 將訊息寫入 MongoDB
                mongo_server.insert_many([user_content, ai_content])

```

<br>

2. `llm_strings.py`。

```python
"""
說明: 將用於 LLM 聊天機器人的各種字串的類別定義於此。
"""


# 定義常數字串類
class LLMStrings:

    # 問答字串
    PROMPT_TEMPLATE = """
    你是一個專門回答問題的聊天機器人，
    你的名字是「Yollama」，
    你擁有各種高階專業學術知識，
    請使用你最擅長的繁體中文回答以下問題：
    """
    # 歡迎詞
    GREETINGS = (
        "您好，親愛的訪客，歡迎來到這。"
        "我是 Yollama！將為您提供回答問題的服務。"
        "你有何問題呢？"
    )
    # 等待回應時的文字
    WAIT_MESSAGE = "坐好，等待 Yollama 的回應！"
    # 輸入框的占位符
    INPUT_PLACEHOLDER = "有問題就提出來，Yollama 會回答你！"

    # Streamlit 的字串常數
    APP_TITLE = "Llamagination Station"
    SESSION_STATES = "messages"

    # MongoDB 的字串常數
    USER_ROLE = "user"
    AI_ROLE = "assistant"
    ROLE_ID = "role"
    CONTENT = "content"

```

<br>

3. `mongodb.py`。

```python
from pymongo import MongoClient
from typing import List, Dict
import streamlit as st
import certifi


# 取得設定或密鑰
MONGO_URI = st.secrets["MONGO_URI"]
MONGO_DB = st.secrets["MONGO_DB"]
MONGO_COLLECTION = st.secrets["MONGO_COLLECTION"]


class EasyMongo:

    def __init__(self):
        self.URI = MONGO_URI
        self.DB = MONGO_DB
        self.COLLECTION = MONGO_COLLECTION

    def get_database(self):
        client = MongoClient(
            self.URI,
            tlsCAFile=certifi.where()
        )

        # 連線
        return client[self.DB]

    def get_collection(self):
        dbname = self.get_database()
        return dbname[self.COLLECTION]

    # 插入數據到 MongoDB.
    def insert_many(self, documents: List[Dict]):
        # 調用類的方法來取得資料
        collection = self.get_collection()
        # 嘗試寫入
        try:
            # 插入文件
            result = collection.insert_many(documents)
            print(f"Inserted {len(result.inserted_ids)} documents")
            print(f"Inserted document IDs: {result.inserted_ids}")
        except Exception as e:
            print(f"Error: {e}")

```

<br>

4. `utils.py`。

```python
import streamlit as st
import time
from llm_strings import LLMStrings
from langchain.llms import Ollama
from typing import Dict


# 建立一個 message 的工具函數
def create_message(role: str, content: str) -> Dict:
    return {
        LLMStrings.ROLE_ID: role,
        LLMStrings.CONTENT: content
    }


# 生成回應的函數
def output_text(llm_model: Ollama, text: str) -> str:

    prompt_template = f"{LLMStrings.PROMPT_TEMPLATE} 請只使用繁體中文回答： {text}"
    # 棄用
    # return llm_model(prompt_template)
    # 改用 invoke
    return llm_model.invoke(prompt_template)


# 模擬回應，還添加了延遲效果
def simulate_response(text: str):
    # 建立一個佔位符
    message_placeholder = st.empty()
    # 建立空白字串
    full_response = ""
    # 預設延遲百分之5秒
    time_delay = 0.05

    for chunk in text.split():
        full_response += chunk + " "
        time.sleep(time_delay)
        # 添加一個像是終端機的游標
        message_placeholder.markdown(full_response + "▌")

    # 透過佔位符寫入回應
    message_placeholder.markdown(full_response)

```

<br>

##