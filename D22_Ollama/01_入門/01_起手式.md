# 起手式

_Ollama 是由 Repl.it 支援的服務_

<br>

## 參考資料

1. [官網](https://ollama.com/)。 

    ![](images/img_10.png)

<br>

2. 建立帳號。 

    ![](images/img_11.png)

<br>

3. 參考 `LangChain` 的 [官網說明](https://python.langchain.com/v0.1/docs/integrations/llms/ollama/)。

    ![](images/img_01.png)

<br>

4. [下載](https://ollama.com/download/mac)

    ![](images/img_02.png)

<br>

## 彙整說明

1. `Ollama` 是可在本地運行的大型語言模型，例如 `Llama 2`。

<br>

2. `Ollama` 將 `模型權重`、`配置` 和 `數據` 打包成一個由 `Modelfile` 定義的單一包，優化了設置和配置細節，包括 GPU 的使用。

<br>

## 設置

_以下步驟設置並運行本地的 Ollama 實例_

<br>

1. [下載](https://ollama.com/download/mac) 並安裝 Ollama 到支持的平台。

<br>

2. 解壓縮。

    ![](images/img_03.png)

<br>

3. 開啟應用。

    ![](images/img_04.png)

<br>

4. 安裝。

    ![](images/img_05.png)

<br>

5. 完成。

    ![](images/img_06.png)

<br>

6. 運行。

    ```bash
    ollama run llama3
    ```

<br>

7. 會開始進行下載。

    ![](images/img_07.png)

<br>

8. 完成後會顯示。

    ![](images/img_08.png)

<br>

9. 可直接在終端機進行互動。

    ![](images/img_09.png)

<br>

## 終端機操作

1. 運行模型，如果模型尚未下載，會自動先進行下載，然後再運行；透過這個指令運行會先進行 `pulling manifest`，`manifest` 是模型的描述文件，包含模型的版本、依賴關係、大小等信息，這需要花一點時間。

    ```bash
    ollama run llama3
    ```

<br>

2. 下載 LLM 模型，執行指令後會將模型文件被保存在系統中，使其可以隨時被調用和使用；另外，`llama3` 屬於 `LLaMA` 聊天模型，啟動後可以使用 `ChatOllama` 界面，這包括系統消息和用戶輸入的特殊標記。

    ```bash
    ollama pull llama3
    ```

<br>

3. 假如要下載完整的 40GB 模型，可加入參數 `70b`。

    ```bash
    ollama run llama3:70b
    ```

<br>

4. 列出本地下載的模型列表。

    ```bash
    ollama list
    ```

<br>

5. 運行中的模型。

    ```bash
    ollama ps
    ```

<br>

6. 停止並刪除運行中的模型，特別注意，這個指令不僅僅是關閉模型，還會徹底將模型從本地刪除。

    ```bash
    ollama rm <模型名稱>
    ```

<br>

7. 查詢下載路徑，模型預設會下載標記為 `最新` 和 `最小` 參數大小的版本，在 Mac 上，模型將下載到 `~/.ollama/models`。

    ```bash
    ls ~/.ollama/models
    ```

<br>

## 模型互動方式

1. 終端機：所有本地模型會自動在端口 `localhost:11434` 啟動服務，透過終端機指令 `ollama run <模型名稱>` 可進行互動。

    ![](images/img_12.png)

<br>

2. 使用 API：可透過如下指令向 Ollama 的 API 端點發送 `application/json` 請求。
    ```bash
    curl http://localhost:11434/api/generate -d '{
    "model": "llama3",
    "prompt":"Why is the sky blue?"
    }'
    ```

<br>

3. 通過 LangChain：可在 Python 專案中結合 LangChain 使用 Ollama 聊天模型。

    ```python
    from langchain_community.llms import Ollama

    llm = Ollama(model="llama3")
    llm.invoke("Tell me a joke")
    ```

<br>

4. 流式輸出：在專案中使用 `.stream(...)` 方法。

    ```python
    query = "Tell me a joke"
    for chunks in llm.stream(query):
        print(chunks)
    ```

<br>

5. 多模態支持：`Ollama` 支持多模態 `LLMs`，如 `Bakllava` 和 `Llava`。

    ```python
    from langchain_community.llms import Ollama

    bakllava = Ollama(model="bakllava")
    ```

<br>

## 開發指引

_通過以下步驟來集成 Ollama 和 LangChain_

<br>

1. 設置環境：下載並配置需要的模型。

<br>

2. 整合模型到 LangChain，使用 LangChain 提供的 API 方法與模型進行互動，如 `invoke` 和 `stream`。

<br>

3. 程式碼

    ```python
    from langchain_community.llms import Ollama

    # 初始化模型
    llm = Ollama(model="llama3")

    # 直接調用模型
    response = llm.invoke("Tell me a joke")
    print(response)

    # 流式輸出
    query = "Tell me a joke"
    for chunk in llm.stream(query):
        print(chunk)
    ```

<br>

___

_END_