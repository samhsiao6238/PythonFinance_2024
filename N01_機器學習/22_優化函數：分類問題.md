# å„ªåŒ–å‡½æ•¸ï¼šåˆ†é¡å•é¡Œ

_ï¼ˆé‡è¤‡å‰ä¸€å°ç¯€çš„æè¿°ï¼‰ä¹Ÿç¨±ç‚ºæå¤±å‡½æ•¸ï¼Œç”¨æ–¼è¡¡é‡æ¨¡å‹çš„é æ¸¬çµæœèˆ‡å¯¦éš›çµæœä¹‹é–“çš„å·®ç•°ï¼Œæ¨¡å‹è¨“ç·´çš„ç›®æ¨™æ˜¯æœ€å°åŒ–é€™å€‹å·®ç•°ï¼Œä»¥æé«˜æ¨¡å‹çš„é æ¸¬èƒ½åŠ›ã€‚_

<br>

## äº¤å‰ç†µæå¤±ï¼ˆCross-Entropy Lossï¼‰

1. äº¤å‰ç†µæå¤±ä¸»è¦ç”¨æ–¼åˆ†é¡å•é¡Œï¼Œç‰¹åˆ¥æ˜¯äºŒåˆ†é¡æˆ–å¤šåˆ†é¡çš„ç¥ç¶“ç¶²çµ¡ï¼Œç”¨ä»¥åº¦é‡ `é æ¸¬æ©Ÿç‡åˆ†å¸ƒèˆ‡çœŸå¯¦åˆ†å¸ƒä¹‹é–“çš„å·®ç•°`ï¼Œç•¶çœŸå¯¦æ¨™ç±¤ç‚º 1 æ™‚ï¼Œé æ¸¬æ©Ÿç‡è¶Šæ¥è¿‘ 1ï¼Œæå¤±è¶Šå°ï¼›å…¬å¼å¦‚ä¸‹ã€‚

    ![](images/img_50.png)

<br>

2. åŸºç¤ç¨‹å¼ç¢¼ã€‚

    ```python
    from sklearn.metrics import log_loss

    # çœŸå¯¦æ¨™ç±¤ y
    y = np.array([1, 0, 1, 0])
    # é æ¸¬æ©Ÿç‡ y_pred_proba
    y_pred_proba = np.array([0.9, 0.1, 0.8, 0.4])

    # è¨ˆç®—äº¤å‰ç†µæå¤±
    cross_entropy = log_loss(y, y_pred_proba)
    print(f"Cross-Entropy Loss: {cross_entropy}")
    ```

<br>

3. ä»¥ä¸‹æ˜¯å€‹é€²éšç¯„ä¾‹ï¼Œè€ƒæ…®å¤šåˆ†é¡å•é¡Œï¼Œä¸¦è¨ˆç®—æ¯å€‹é¡åˆ¥çš„é æ¸¬æ©Ÿç‡ï¼Œç„¶å¾Œä½¿ç”¨ `sklearn.metrics.log_loss` ä¾†è¨ˆç®—å¤šé¡åˆ¥çš„äº¤å‰ç†µæå¤±ï¼Œä¸¦å¯è¦–åŒ–ä¸åŒé æ¸¬æ©Ÿç‡ä¸‹çš„äº¤å‰ç†µæå¤±ã€‚

    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.metrics import log_loss

    # çœŸå¯¦æ¨™ç±¤ y (å¤šåˆ†é¡æƒ…æ³ï¼Œæœ‰ä¸‰å€‹é¡åˆ¥: 0, 1, 2)
    y_true = np.array([0, 1, 2, 2, 1, 0])

    # é æ¸¬æ©Ÿç‡ y_pred_probaï¼Œé€™è£¡æ¯å€‹æ¨£æœ¬çš„é æ¸¬æ©Ÿç‡åˆ†ä½ˆåœ¨ä¸‰å€‹é¡åˆ¥ä¸Š
    y_pred_proba = np.array([
        [0.7, 0.2, 0.1],  # æ¨£æœ¬1çš„é æ¸¬æ©Ÿç‡
        [0.1, 0.8, 0.1],  # æ¨£æœ¬2çš„é æ¸¬æ©Ÿç‡
        [0.2, 0.2, 0.6],  # æ¨£æœ¬3çš„é æ¸¬æ©Ÿç‡
        [0.1, 0.3, 0.6],  # æ¨£æœ¬4çš„é æ¸¬æ©Ÿç‡
        [0.1, 0.7, 0.2],  # æ¨£æœ¬5çš„é æ¸¬æ©Ÿç‡
        [0.8, 0.1, 0.1]   # æ¨£æœ¬6çš„é æ¸¬æ©Ÿç‡
    ])

    # è¨ˆç®—äº¤å‰ç†µæå¤±
    cross_entropy = log_loss(y_true, y_pred_proba)
    print(f"Cross-Entropy Loss: {cross_entropy}")

    # å¯è¦–åŒ–ä¸åŒé æ¸¬æ©Ÿç‡å°äº¤å‰ç†µæå¤±çš„å½±éŸ¿
    pred_prob_1 = np.linspace(0.01, 0.99, 100)
    pred_prob_2 = 1 - pred_prob_1
    loss_list = []

    # å°æ–¼å›ºå®šçœŸå¯¦æ¨™ç±¤ y = 0ï¼Œè¨ˆç®—ä¸åŒæ©Ÿç‡ä¸‹çš„äº¤å‰ç†µæå¤±
    for p1, p2 in zip(pred_prob_1, pred_prob_2):
        # å…©å€‹é¡åˆ¥çš„é æ¸¬æ©Ÿç‡
        y_pred_example = np.array([[p1, p2, 0.0]])
        y_true_example = np.array([0])
        loss = log_loss(y_true_example, y_pred_example, labels=[0, 1, 2])
        loss_list.append(loss)

    # å¯è¦–åŒ–
    plt.figure(figsize=(8, 6))
    plt.plot(pred_prob_1, loss_list, label='True label: 0', color='blue')
    plt.xlabel('Predicted Probability for Class 0')
    plt.ylabel('Cross-Entropy Loss')
    plt.title('Cross-Entropy Loss vs. Predicted Probability for Class 0')
    plt.legend()
    plt.grid()
    plt.show()
    ```

<br>

4. è¼¸å‡ºå¦‚ä¸‹ï¼Œ`X è»¸` ä»£è¡¨é æ¸¬çš„é¡åˆ¥ 0 çš„æ©Ÿç‡ï¼ˆPredicted Probability for Class 0ï¼‰ï¼Œ`Y ç¸±` ä»£è¡¨äº¤å‰ç†µæå¤±ï¼ˆCross-Entropy Lossï¼‰ï¼›ç•¶é æ¸¬æ©Ÿç‡æ¥è¿‘ `0` æ™‚ï¼Œäº¤å‰ç†µæå¤±æ¥è¿‘ 0ï¼Œé€™è¡¨ç¤ºæ¨¡å‹çš„é æ¸¬éå¸¸æº–ç¢ºã€æå¤±å¾ˆå°ï¼›ç•¶é æ¸¬æ©Ÿç‡é€æ¼¸åå‘ `1` æ™‚ï¼Œäº¤å‰ç†µæå¤±æ€¥åŠ‡å¢åŠ ï¼Œé€™æ˜¯å› ç‚ºæ¨¡å‹è‡ªä¿¡åœ°é æ¸¬éŒ¯èª¤ï¼Œå°è‡´æå¤±å€¼è®Šå¤§ã€‚

    ![](images/img_54.png)

<br>

5. äº¤å‰ç†µæå¤±çš„ç‰¹æ€§æ˜¯åœ¨çœŸå¯¦æ¨™ç±¤ç‚º 0 æˆ– 1 çš„æƒ…æ³ä¸‹ï¼Œæå¤±çš„å€¼å–æ±ºæ–¼é æ¸¬æ©Ÿç‡ï¼Œç•¶é æ¸¬æ¥è¿‘çœŸå¯¦æ¨™ç±¤æ™‚ï¼Œæå¤±å€¼è¼ƒä½ï¼›ç•¶é æ¸¬åé›¢çœŸå¯¦æ¨™ç±¤æ™‚ï¼Œæå¤±å€¼å¢å¤§ï¼›é€šéæœ€å°åŒ–äº¤å‰ç†µæå¤±ï¼Œæ¨¡å‹èª¿æ•´æ¬Šé‡ï¼Œå°‡ä½¿é æ¸¬æ›´æ¥è¿‘çœŸå¯¦æ¨™ç±¤ã€‚

<br>

## äºŒå…ƒäº¤å‰ç†µï¼ˆBinary Cross-Entropy Lossï¼‰

1. é€™æ˜¯ `äº¤å‰ç†µæå¤±çš„ç‰¹ä¾‹`ï¼Œå°ˆé–€ç”¨æ–¼äºŒå…ƒåˆ†é¡å•é¡Œã€‚å®ƒåº¦é‡æ¨¡å‹é æ¸¬çš„æ©Ÿç‡åˆ†å¸ƒå’ŒçœŸå¯¦åˆ†å¸ƒä¹‹é–“çš„å·®ç•°ã€‚

<br>

2. åŸºç¤ç¨‹å¼ç¢¼ã€‚

    ```python
    # å°æ–¼äºŒå…ƒåˆ†é¡å•é¡Œï¼Œlog_loss ä¹Ÿå¯ç”¨æ–¼è¨ˆç®—äºŒå…ƒäº¤å‰ç†µæå¤±
    binary_cross_entropy = log_loss(y, y_pred_proba)
    print(f"Binary Cross-Entropy Loss: {binary_cross_entropy}")
    ```

<br>

3. é€²éšç¨‹å¼ç¢¼ã€‚

    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.metrics import log_loss

    # çœŸå¯¦æ¨™ç±¤ y ç‚º1çš„æƒ…æ³
    y_true_1 = np.array([1])
    # é æ¸¬æ©Ÿç‡å¾0åˆ°1
    y_pred_proba = np.linspace(0.01, 0.99, 100)

    # è¨ˆç®—äº¤å‰ç†µæå¤±ï¼Œç•¶çœŸå¯¦æ¨™ç±¤ç‚º1ï¼Œä¸¦æä¾›æ‰€æœ‰å¯èƒ½çš„æ¨™ç±¤ [0, 1]
    loss_1 = [
        log_loss(y_true_1, [p], labels=[0, 1]) for p in y_pred_proba
    ]

    # çœŸå¯¦æ¨™ç±¤ y ç‚º0çš„æƒ…æ³
    y_true_0 = np.array([0])

    # è¨ˆç®—äº¤å‰ç†µæå¤±ï¼Œç•¶çœŸå¯¦æ¨™ç±¤ç‚º0ï¼Œä¸¦æä¾›æ‰€æœ‰å¯èƒ½çš„æ¨™ç±¤ [0, 1]
    loss_0 = [
        log_loss(y_true_0, [p], labels=[0, 1]) for p in y_pred_proba
    ]

    # å¯è¦–åŒ–
    plt.figure(figsize=(8, 6))

    plt.plot(y_pred_proba, loss_1, label='True label: 1', color='blue')
    plt.plot(y_pred_proba, loss_0, label='True label: 0', color='red')
    plt.xlabel('Predicted Probability')
    plt.ylabel('Cross-Entropy Loss')
    plt.title('Cross-Entropy Loss vs. Predicted Probability')
    plt.legend()
    plt.grid()
    plt.show()
    ```

<br>

4. è¼¸å‡ºçš„çµæœå¦‚ä¸‹åœ–ï¼Œä¹Ÿå°±æ˜¯ `äº¤å‰ç†µæå¤±ï¼ˆCross-Entropy Lossï¼‰` ç›¸å°æ–¼ `é æ¸¬æ©Ÿç‡ï¼ˆPredicted Probabilityï¼‰` çš„åœ–ç¤ºï¼Œå‘ˆç¾çš„æ˜¯ `çœŸå¯¦æ¨™ç±¤ç‚º1ï¼ˆè—ç·šï¼‰` å’Œ `çœŸå¯¦æ¨™ç±¤ç‚º0ï¼ˆç´…ç·šï¼‰` æ™‚ï¼Œäº¤å‰ç†µæå¤±å¦‚ä½•éš¨è‘—é æ¸¬æ©Ÿç‡çš„è®ŠåŒ–è€Œè®ŠåŒ–ï¼›ç•¶ `çœŸå¯¦æ¨™ç±¤ç‚º1` æ™‚ï¼Œäº¤å‰ç†µæå¤±åœ¨æ¨¡å‹é æ¸¬æ©Ÿç‡æ¥è¿‘çœŸå¯¦å€¼ `1` æ™‚æœ€å°ï¼Œéš¨è‘—é æ¸¬æ©Ÿç‡æ¥è¿‘ `0`ï¼Œæå¤±å¿«é€Ÿå¢å¤§ï¼Œé€™è¡¨ç¤ºæ¨¡å‹çš„é æ¸¬è¶Šä¸æº–ç¢ºï¼Œä¹Ÿå°±æ˜¯é æ¸¬ç‚º `0` çš„æ©Ÿç‡è¶Šå¤§æ™‚ï¼Œæå¤±è¶Šé«˜ï¼›ç•¶ `çœŸå¯¦æ¨™ç±¤ç‚º0`æ™‚ï¼Œäº¤å‰ç†µæå¤±åœ¨æ¨¡å‹é æ¸¬æ©Ÿç‡æ¥è¿‘çœŸå¯¦å€¼ `0`æ™‚æœ€å°ï¼Œéš¨è‘—é æ¸¬æ©Ÿç‡æ¥è¿‘ `1`ï¼Œæå¤±å¿«é€Ÿå¢å¤§ï¼ŒåŒæ¨£è¡¨ç¤ºæ¨¡å‹é æ¸¬è¶Šä¸æº–ç¢ºï¼Œä¹Ÿå°±æ˜¯é æ¸¬ç‚º `1` çš„æ©Ÿç‡è¶Šå¤§ã€æå¤±è¶Šé«˜ã€‚

    ![](images/img_51.png)

<br>

## å°æ¯”æå¤±ï¼ˆContrastive Lossï¼‰

1. `å°æ¯”æå¤±` ä¸»è¦ç”¨æ–¼åº¦é‡å­¸ç¿’ï¼ˆMetric Learningï¼‰ä¸­çš„é›™è¼¸å…¥æ¨¡å‹ï¼Œä¾‹å¦‚å­¿ç”Ÿç¶²çµ¡ï¼ˆSiamese Networksï¼‰ï¼Œå®ƒç”¨æ–¼æœ€å°åŒ–ç›¸ä¼¼å°ä¹‹é–“çš„è·é›¢ï¼ŒåŒæ™‚æœ€å¤§åŒ–ä¸åŒå°ä¹‹é–“çš„è·é›¢ï¼›å…¬å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ $(D)$ æ˜¯æ¨£æœ¬å°çš„è·é›¢ï¼Œ$(m)$ æ˜¯é‚Šç•Œè·é›¢ã€‚

    ![](images/img_52.png)

<br>

2. åŸºç¤ç¯„ä¾‹ç¨‹å¼ç¢¼ã€‚

    ```python
    import torch
    import torch.nn.functional as F

    def contrastive_loss(y, d, margin=1.0):
        loss = 0.5 * (y * d ** 2 + (1 - y) * F.relu(margin - d) ** 2)
        return loss.mean()

    # å‡è¨­æœ‰ä¸€äº›ç›¸ä¼¼åº¦æ¨™ç±¤ y å’Œæ¨£æœ¬å°ä¹‹é–“çš„è·é›¢ d
    y = torch.Tensor([1, 0, 1, 0])
    d = torch.Tensor([0.5, 2.0, 1.0, 3.0])

    # è¨ˆç®—å°æ¯”æå¤±
    loss = contrastive_loss(y, d)
    print(f"Contrastive Loss: {loss.item()}")
    ```

<br>

3. é€²éšç¯„ä¾‹ï¼Œç¹ªè£½å°æ¯”æå¤±éš¨è‘—è·é›¢ ğ‘‘ çš„è®ŠåŒ–ä¾†è§€å¯Ÿæå¤±å‡½æ•¸çš„è¡Œç‚ºã€‚

    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    import torch
    import torch.nn.functional as F

    # å°æ¯”æå¤±å‡½æ•¸
    def contrastive_loss(y, d, margin=1.0):
        loss = 0.5 * (
            y * d ** 2 + (1 - y) * F.relu(margin - d) ** 2
        )
        return loss.mean()

    # ç”Ÿæˆè·é›¢ç¯„åœ d
    d_range = np.linspace(0, 3, 100)

    # è¨ˆç®—ä¸åŒè·é›¢ d ä¸‹çš„å°æ¯”æå¤±
    # æ¨™ç±¤ 1 è¡¨ç¤ºç›¸ä¼¼
    y_similar = torch.tensor([1.0])
    # æ¨™ç±¤ 0 è¡¨ç¤ºä¸ç›¸ä¼¼
    y_dissimilar = torch.tensor([0.0])
    loss_similar = [
        contrastive_loss(
            y_similar, torch.tensor([d])
        ).item() for d in d_range
    ]
    loss_dissimilar = [
        contrastive_loss(
            y_dissimilar, torch.tensor([d])
        ).item() for d in d_range
    ]

    # ç¹ªè£½çµæœ
    plt.figure(figsize=(8, 6))
    plt.plot(
        d_range, loss_similar, 
        label='y = 1 (Similar)', 
        color='blue'
    )
    plt.plot(
        d_range, loss_dissimilar, 
        label='y = 0 (Dissimilar)', 
        color='red'
    )
    plt.xlabel('Distance (d)')
    plt.ylabel('Contrastive Loss')
    plt.title('Contrastive Loss vs. Distance')
    plt.legend()
    plt.grid(True)
    plt.show()
    ```

<br>

4. è¼¸å‡ºçµæœå¦‚ä¸‹ï¼Œé€éåœ–å½¢é€™å¯ç†è§£ `å°æ¯”æå¤±ï¼ˆContrastive Loss` éš¨ `æ¨£æœ¬è·é›¢ï¼ˆğ‘‘ï¼‰` è®ŠåŒ–çš„æ›²ç·šã€‚

    ![](images/img_53.png)

<br>

5. é€²ä¸€æ­¥èªªæ˜è¼¸å‡ºçµæœï¼Œåœ¨è—ç·šï¼ˆy = 1ï¼ŒSimilarï¼‰éƒ¨åˆ†ï¼Œç•¶æ¨£æœ¬å°æ˜¯ç›¸ä¼¼çš„ $(ğ‘¦=1)$ï¼Œæå¤±å€¼éš¨è·é›¢ $(ğ‘‘)$ çš„å¢åŠ è€Œå¢åŠ ï¼Œé€™è¡¨ç¤ºæ¨¡å‹çš„ç›®æ¨™æ˜¯æœ€å°åŒ–ç›¸ä¼¼æ¨£æœ¬å°ä¹‹é–“çš„è·é›¢ï¼Œè®“å®ƒå€‘ç›¡å¯èƒ½æ¥è¿‘ã€‚å› æ­¤ï¼Œæå¤±æ›²ç·šæ˜¯å°è·é›¢ $(ğ‘‘)$ çš„å¹³æ–¹å‡½æ•¸ï¼Œè·é›¢è¶Šå¤§ï¼Œæå¤±è¶Šé«˜ã€‚

<br>

6. å»¶çºŒä¸Šä¸€é»ï¼›åœ¨ç´…ç·šï¼ˆy = 0ï¼ŒDissimilarï¼‰éƒ¨åˆ†ï¼Œ ç•¶æ¨£æœ¬å°æ˜¯ä¸ç›¸ä¼¼çš„ $(ğ‘¦=0)$ï¼Œæå¤±å€¼éš¨è·é›¢ ğ‘‘ çš„å¢åŠ è€Œæ¸›å°‘ï¼Œç›´åˆ°è·é›¢é”åˆ°è¨­å®šçš„ margin å€¼ï¼ˆå‡è¨­ margin = 1ï¼‰ï¼Œæ­¤å¾Œæå¤±å€¼è¶¨æ–¼é›¶ã€‚é€™è¡¨ç¤ºæ¨¡å‹çš„ç›®æ¨™æ˜¯æœ€å¤§åŒ–ä¸ç›¸ä¼¼æ¨£æœ¬å°ä¹‹é–“çš„è·é›¢ï¼Œè‡³å°‘è¦å¤§æ–¼ marginï¼Œé€™æ¨£å¯ä»¥ç¢ºä¿æ¨¡å‹å­¸åˆ°çš„è¡¨ç¤ºèƒ½å¤ æœ‰æ•ˆå€åˆ†ä¸ç›¸ä¼¼çš„æ¨£æœ¬å°ã€‚

<br>

___

_END_