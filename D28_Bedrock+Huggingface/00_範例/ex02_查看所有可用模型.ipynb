{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[參考](https://medium.com/@simon3458/aws-workshop-bedrock-llm-note-202404-4016d9ce9dc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看所有的基礎模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "Model ID: amazon.titan-tg1-large, Name: Titan Text Large\n",
      "Model ID: amazon.titan-image-generator-v1:0, Name: Titan Image Generator G1\n",
      "Model ID: amazon.titan-image-generator-v1, Name: Titan Image Generator G1\n",
      "Model ID: amazon.titan-text-premier-v1:0, Name: Titan Text G1 - Premier\n",
      "Model ID: amazon.titan-embed-g1-text-02, Name: Titan Text Embeddings v2\n",
      "Model ID: amazon.titan-text-lite-v1:0:4k, Name: Titan Text G1 - Lite\n",
      "Model ID: amazon.titan-text-lite-v1, Name: Titan Text G1 - Lite\n",
      "Model ID: amazon.titan-text-express-v1:0:8k, Name: Titan Text G1 - Express\n",
      "Model ID: amazon.titan-text-express-v1, Name: Titan Text G1 - Express\n",
      "Model ID: amazon.titan-embed-text-v1:2:8k, Name: Titan Embeddings G1 - Text\n",
      "Model ID: amazon.titan-embed-text-v1, Name: Titan Embeddings G1 - Text\n",
      "Model ID: amazon.titan-embed-text-v2:0:8k, Name: Titan Text Embeddings V2\n",
      "Model ID: amazon.titan-embed-text-v2:0, Name: Titan Text Embeddings V2\n",
      "Model ID: amazon.titan-embed-image-v1:0, Name: Titan Multimodal Embeddings G1\n",
      "Model ID: amazon.titan-embed-image-v1, Name: Titan Multimodal Embeddings G1\n",
      "Model ID: stability.stable-diffusion-xl-v1:0, Name: SDXL 1.0\n",
      "Model ID: stability.stable-diffusion-xl-v1, Name: SDXL 1.0\n",
      "Model ID: ai21.j2-grande-instruct, Name: J2 Grande Instruct\n",
      "Model ID: ai21.j2-jumbo-instruct, Name: J2 Jumbo Instruct\n",
      "Model ID: ai21.j2-mid, Name: Jurassic-2 Mid\n",
      "Model ID: ai21.j2-mid-v1, Name: Jurassic-2 Mid\n",
      "Model ID: ai21.j2-ultra, Name: Jurassic-2 Ultra\n",
      "Model ID: ai21.j2-ultra-v1:0:8k, Name: Jurassic-2 Ultra\n",
      "Model ID: ai21.j2-ultra-v1, Name: Jurassic-2 Ultra\n",
      "Model ID: anthropic.claude-instant-v1:2:100k, Name: Claude Instant\n",
      "Model ID: anthropic.claude-instant-v1, Name: Claude Instant\n",
      "Model ID: anthropic.claude-v2:0:18k, Name: Claude\n",
      "Model ID: anthropic.claude-v2:0:100k, Name: Claude\n",
      "Model ID: anthropic.claude-v2:1:18k, Name: Claude\n",
      "Model ID: anthropic.claude-v2:1:200k, Name: Claude\n",
      "Model ID: anthropic.claude-v2:1, Name: Claude\n",
      "Model ID: anthropic.claude-v2, Name: Claude\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0:28k, Name: Claude 3 Sonnet\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0:200k, Name: Claude 3 Sonnet\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0, Name: Claude 3 Sonnet\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0:48k, Name: Claude 3 Haiku\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0:200k, Name: Claude 3 Haiku\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0, Name: Claude 3 Haiku\n",
      "Model ID: cohere.command-text-v14:7:4k, Name: Command\n",
      "Model ID: cohere.command-text-v14, Name: Command\n",
      "Model ID: cohere.command-r-v1:0, Name: Command R\n",
      "Model ID: cohere.command-r-plus-v1:0, Name: Command R+\n",
      "Model ID: cohere.command-light-text-v14:7:4k, Name: Command Light\n",
      "Model ID: cohere.command-light-text-v14, Name: Command Light\n",
      "Model ID: cohere.embed-english-v3:0:512, Name: Embed English\n",
      "Model ID: cohere.embed-english-v3, Name: Embed English\n",
      "Model ID: cohere.embed-multilingual-v3:0:512, Name: Embed Multilingual\n",
      "Model ID: cohere.embed-multilingual-v3, Name: Embed Multilingual\n",
      "Model ID: meta.llama2-13b-chat-v1:0:4k, Name: Llama 2 Chat 13B\n",
      "Model ID: meta.llama2-13b-chat-v1, Name: Llama 2 Chat 13B\n",
      "Model ID: meta.llama2-70b-chat-v1:0:4k, Name: Llama 2 Chat 70B\n",
      "Model ID: meta.llama2-70b-chat-v1, Name: Llama 2 Chat 70B\n",
      "Model ID: meta.llama2-13b-v1:0:4k, Name: Llama 2 13B\n",
      "Model ID: meta.llama2-13b-v1, Name: Llama 2 13B\n",
      "Model ID: meta.llama2-70b-v1:0:4k, Name: Llama 2 70B\n",
      "Model ID: meta.llama2-70b-v1, Name: Llama 2 70B\n",
      "Model ID: meta.llama3-8b-instruct-v1:0, Name: Llama 3 8B Instruct\n",
      "Model ID: meta.llama3-70b-instruct-v1:0, Name: Llama 3 70B Instruct\n",
      "Model ID: mistral.mistral-7b-instruct-v0:2, Name: Mistral 7B Instruct\n",
      "Model ID: mistral.mixtral-8x7b-instruct-v0:1, Name: Mixtral 8x7B Instruct\n",
      "Model ID: mistral.mistral-large-2402-v1:0, Name: Mistral Large\n",
      "Model ID: mistral.mistral-small-2402-v1:0, Name: Mistral Small\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 環境變數\n",
    "load_dotenv()\n",
    "\n",
    "def list_available_models():\n",
    "    aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    region_name = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "    if not all([aws_access_key_id, aws_secret_access_key, region_name]):\n",
    "        print(\"錯誤：環境變數中並未設置 AWS 憑證或是區域。\")\n",
    "        return\n",
    "\n",
    "    # 建立 AWS Bedrock 客户端物件\n",
    "    client = boto3.client(\n",
    "        \"bedrock\",\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        region_name=region_name,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 列出所有基礎模型\n",
    "        response = client.list_foundation_models()\n",
    "        models = response.get('modelSummaries', [])\n",
    "        if models:\n",
    "            print(\"Available models:\")\n",
    "            for model in models:\n",
    "                print(f\"Model ID: {model['modelId']}, Name: {model['modelName']}\")\n",
    "        else:\n",
    "            print(\"No models available.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"ClientError: {e.response['Error']['Message']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_available_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "格式化輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available advanced models:\n",
      "Model ID: amazon.titan-tg1-large\n",
      "Name: Titan Text Large\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-image-generator-v1:0\n",
      "Name: Titan Image Generator G1\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: IMAGE\n",
      "Customizations Supported: FINE_TUNING\n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-image-generator-v1\n",
      "Name: Titan Image Generator G1\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: IMAGE\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-text-premier-v1:0\n",
      "Name: Titan Text G1 - Premier\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-embed-g1-text-02\n",
      "Name: Titan Text Embeddings v2\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-text-lite-v1:0:4k\n",
      "Name: Titan Text G1 - Lite\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: FINE_TUNING, CONTINUED_PRE_TRAINING\n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-text-lite-v1\n",
      "Name: Titan Text G1 - Lite\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-text-express-v1:0:8k\n",
      "Name: Titan Text G1 - Express\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: FINE_TUNING, CONTINUED_PRE_TRAINING\n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-text-express-v1\n",
      "Name: Titan Text G1 - Express\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-embed-text-v1:2:8k\n",
      "Name: Titan Embeddings G1 - Text\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-embed-text-v1\n",
      "Name: Titan Embeddings G1 - Text\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-embed-text-v2:0:8k\n",
      "Name: Titan Text Embeddings V2\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: \n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-embed-text-v2:0\n",
      "Name: Titan Text Embeddings V2\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-embed-image-v1:0\n",
      "Name: Titan Multimodal Embeddings G1\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: FINE_TUNING\n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: amazon.titan-embed-image-v1\n",
      "Name: Titan Multimodal Embeddings G1\n",
      "Provider: Amazon\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: stability.stable-diffusion-xl-v1:0\n",
      "Name: SDXL 1.0\n",
      "Provider: Stability AI\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: IMAGE\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: stability.stable-diffusion-xl-v1\n",
      "Name: SDXL 1.0\n",
      "Provider: Stability AI\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: IMAGE\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: ai21.j2-grande-instruct\n",
      "Name: J2 Grande Instruct\n",
      "Provider: AI21 Labs\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: ai21.j2-jumbo-instruct\n",
      "Name: J2 Jumbo Instruct\n",
      "Provider: AI21 Labs\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: ai21.j2-mid\n",
      "Name: Jurassic-2 Mid\n",
      "Provider: AI21 Labs\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: ai21.j2-mid-v1\n",
      "Name: Jurassic-2 Mid\n",
      "Provider: AI21 Labs\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: ai21.j2-ultra\n",
      "Name: Jurassic-2 Ultra\n",
      "Provider: AI21 Labs\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: ai21.j2-ultra-v1:0:8k\n",
      "Name: Jurassic-2 Ultra\n",
      "Provider: AI21 Labs\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: ai21.j2-ultra-v1\n",
      "Name: Jurassic-2 Ultra\n",
      "Provider: AI21 Labs\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-instant-v1:2:100k\n",
      "Name: Claude Instant\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-instant-v1\n",
      "Name: Claude Instant\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-v2:0:18k\n",
      "Name: Claude\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-v2:0:100k\n",
      "Name: Claude\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-v2:1:18k\n",
      "Name: Claude\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-v2:1:200k\n",
      "Name: Claude\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-v2:1\n",
      "Name: Claude\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-v2\n",
      "Name: Claude\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0:28k\n",
      "Name: Claude 3 Sonnet\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0:200k\n",
      "Name: Claude 3 Sonnet\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Name: Claude 3 Sonnet\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0:48k\n",
      "Name: Claude 3 Haiku\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0:200k\n",
      "Name: Claude 3 Haiku\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0\n",
      "Name: Claude 3 Haiku\n",
      "Provider: Anthropic\n",
      "Input Modalities: TEXT, IMAGE\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.command-text-v14:7:4k\n",
      "Name: Command\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: FINE_TUNING\n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.command-text-v14\n",
      "Name: Command\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.command-r-v1:0\n",
      "Name: Command R\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.command-r-plus-v1:0\n",
      "Name: Command R+\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.command-light-text-v14:7:4k\n",
      "Name: Command Light\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: FINE_TUNING\n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.command-light-text-v14\n",
      "Name: Command Light\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.embed-english-v3:0:512\n",
      "Name: Embed English\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.embed-english-v3\n",
      "Name: Embed English\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.embed-multilingual-v3:0:512\n",
      "Name: Embed Multilingual\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: cohere.embed-multilingual-v3\n",
      "Name: Embed Multilingual\n",
      "Provider: Cohere\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: EMBEDDING\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: False\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama2-13b-chat-v1:0:4k\n",
      "Name: Llama 2 Chat 13B\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: PROVISIONED\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama2-13b-chat-v1\n",
      "Name: Llama 2 Chat 13B\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama2-70b-chat-v1:0:4k\n",
      "Name: Llama 2 Chat 70B\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: \n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama2-70b-chat-v1\n",
      "Name: Llama 2 Chat 70B\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama2-13b-v1:0:4k\n",
      "Name: Llama 2 13B\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: FINE_TUNING\n",
      "Inference Types Supported: \n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama2-13b-v1\n",
      "Name: Llama 2 13B\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: \n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama2-70b-v1:0:4k\n",
      "Name: Llama 2 70B\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: FINE_TUNING\n",
      "Inference Types Supported: \n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama2-70b-v1\n",
      "Name: Llama 2 70B\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: \n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama3-8b-instruct-v1:0\n",
      "Name: Llama 3 8B Instruct\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: meta.llama3-70b-instruct-v1:0\n",
      "Name: Llama 3 70B Instruct\n",
      "Provider: Meta\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: mistral.mistral-7b-instruct-v0:2\n",
      "Name: Mistral 7B Instruct\n",
      "Provider: Mistral AI\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: mistral.mixtral-8x7b-instruct-v0:1\n",
      "Name: Mixtral 8x7B Instruct\n",
      "Provider: Mistral AI\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: mistral.mistral-large-2402-v1:0\n",
      "Name: Mistral Large\n",
      "Provider: Mistral AI\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n",
      "Model ID: mistral.mistral-small-2402-v1:0\n",
      "Name: Mistral Small\n",
      "Provider: Mistral AI\n",
      "Input Modalities: TEXT\n",
      "Output Modalities: TEXT\n",
      "Customizations Supported: \n",
      "Inference Types Supported: ON_DEMAND\n",
      "Response Streaming Supported: True\n",
      "Model Lifecycle Status: ACTIVE\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "def list_advanced_models():\n",
    "    aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    region_name = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "    if not all([aws_access_key_id, aws_secret_access_key, region_name]):\n",
    "        print(\"錯誤：環境變數尚未設置\")\n",
    "        return\n",
    "\n",
    "    # 建立 AWS Bedrock 客户端物件\n",
    "    client = boto3.client(\n",
    "        \"bedrock\",\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        region_name=region_name,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 列出所有基礎模型\n",
    "        response = client.list_foundation_models()\n",
    "        models = response.get('modelSummaries', [])\n",
    "        if models:\n",
    "            print(\"Available advanced models:\")\n",
    "            for model in models:\n",
    "                model_id = model['modelId']\n",
    "                model_name = model['modelName']\n",
    "                provider_name = model['providerName']\n",
    "                input_modalities = \", \".join(model.get('inputModalities', []))\n",
    "                output_modalities = \", \".join(model.get('outputModalities', []))\n",
    "                customizations_supported = \", \".join(model.get('customizationsSupported', []))\n",
    "                inference_types_supported = \", \".join(model.get('inferenceTypesSupported', []))\n",
    "                response_streaming_supported = model.get('responseStreamingSupported', False)\n",
    "                model_lifecycle_status = model['modelLifecycle']['status']\n",
    "\n",
    "                print(f\"Model ID: {model_id}\")\n",
    "                print(f\"Name: {model_name}\")\n",
    "                print(f\"Provider: {provider_name}\")\n",
    "                print(f\"Input Modalities: {input_modalities}\")\n",
    "                print(f\"Output Modalities: {output_modalities}\")\n",
    "                print(f\"Customizations Supported: {customizations_supported}\")\n",
    "                print(f\"Inference Types Supported: {inference_types_supported}\")\n",
    "                print(f\"Response Streaming Supported: {response_streaming_supported}\")\n",
    "                print(f\"Model Lifecycle Status: {model_lifecycle_status}\")\n",
    "                print(\"-\" * 60)\n",
    "        else:\n",
    "            print(\"No models available.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"ClientError: {e.response['Error']['Message']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_advanced_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envllmChatBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
