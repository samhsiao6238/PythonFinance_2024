### L1 與 L2 正則化

在機器學習中，L1 和 L2 正則化 是兩種常見的正則化技術，它們主要用來防止模型過擬合（overfitting），即模型在訓練資料上表現很好，但在測試資料或新資料上表現較差。這兩種方法透過在損失函數（loss function）中添加一項懲罰項，來限制模型參數的大小或複雜度，從而提高模型的泛化能力。

---

### 1. L1 正則化 (Lasso Regression)

L1 正則化 也稱為 Lasso Regression，它透過在損失函數中加入權重的 L1 範數（L1 norm），即所有權重的絕對值之和。

- L1 正則化的公式：
  \[
  \text{Loss} = \text{原始損失} + \lambda \sum_{i} |w_i|
  \]
  其中，$\lambda$ 是正則化係數，$w_i$ 是模型的參數。

- 效果：L1 正則化會將一些不重要的權重變為 0，從而使模型變得稀疏。這意味著 L1 正則化能夠同時進行特徵選擇，因為一些特徵的權重可能會完全消失。

---

### 2. L2 正則化 (Ridge Regression)

L2 正則化 也稱為 Ridge Regression，它透過在損失函數中加入權重的 L2 範數（L2 norm），即所有權重的平方和。

- L2 正則化的公式：
  \[
  \text{Loss} = \text{原始損失} + \lambda \sum_{i} w_i^2
  \]
  其中，$\lambda$ 是正則化係數，$w_i$ 是模型的參數。

- 效果：L2 正則化會將不重要的權重逼近 0，但不會完全變為 0。L2 會均勻地減少權重，使模型更加穩定，防止過於依賴某些特徵。

---

### 3. L1 與 L2 正則化的比較

| 特性               | L1 正則化 (Lasso)            | L2 正則化 (Ridge)          |
|-------------------|-----------------------------|----------------------------|
| 主要作用      | 使模型參數稀疏，進行特徵選擇 | 平滑權重，避免過大參數    |
| 懲罰方式      | 使用參數的絕對值             | 使用參數的平方             |
| 特徵選擇      | 可以使某些特徵的權重變為 0    | 權重會接近 0 但不會完全為 0 |
| 應用場景      | 特徵選擇、簡單模型            | 避免過擬合、提高穩定性    |

---

### 4. Elastic Net 正則化

Elastic Net 是 L1 和 L2 正則化的結合，它將 L1 和 L2 正則化的優點結合在一起，既能夠進行特徵選擇，又能夠防止模型參數過度縮小。

- Elastic Net 的公式：
  \[
  \text{Loss} = \text{原始損失} + \lambda_1 \sum_{i} |w_i| + \lambda_2 \sum_{i} w_i^2
  \]
  其中，$\lambda_1$ 和 $\lambda_2$ 分別控制 L1 和 L2 的權重。

- 效果：Elastic Net 可以同時進行特徵選擇和參數的平滑調整，適合用在高維度數據且特徵之間存在高度相關性的情況。

---

### 5. 其他類似的正則化技術

除了 L1 和 L2，還有其他的正則化技術常用於機器學習模型中，這些技術也屬於廣義的正則化範疇：

- Dropout 正則化：主要用於神經網路中，隨機移除神經元來防止過擬合。
- Early Stopping：在模型訓練過程中監測驗證集損失，一旦損失開始增加，就停止訓練，防止過擬合。
- Data Augmentation：通過生成更多的數據樣本來增加訓練數據的多樣性，防止模型過擬合。

---

### 6. 正則化的父類名稱

L1 和 L2 正則化，以及其他類似技術，可以統稱為 「正則化技術（Regularization Techniques）」。這個大類別包含了各種用來限制模型複雜度，防止過擬合的技術方法。正則化是機器學習中提高模型泛化能力的關鍵部分。

---

### 7. 範例程式碼：L1 與 L2 正則化在 Python 中的應用

我們可以使用 `scikit-learn` 來進行 L1 和 L2 正則化。以下是用於線性迴歸的範例：

#### 使用 L1 正則化（Lasso Regression）
```python
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression

# 生成一個範例回歸數據集
X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)

# 分割數據集為訓練集和測試集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 初始化 Lasso 模型 (L1 正則化)
lasso = Lasso(alpha=0.1)

# 訓練模型
lasso.fit(X_train, y_train)

# 評估模型
score = lasso.score(X_test, y_test)
print(f"Lasso 模型準確度: {score}")
```

#### 使用 L2 正則化（Ridge Regression）
```python
from sklearn.linear_model import Ridge

# 初始化 Ridge 模型 (L2 正則化)
ridge = Ridge(alpha=0.1)

# 訓練模型
ridge.fit(X_train, y_train)

# 評估模型
score = ridge.score(X_test, y_test)
print(f"Ridge 模型準確度: {score}")
```

---

### 總結

正則化技術 是防止機器學習模型過擬合的重要工具。L1 正則化 和 L2 正則化 是兩種最常見的方式，它們分別適用於不同的應用場景，並且還有其他技術如 Elastic Net、Dropout 等作為補充。通過正則化技術，我們可以提高模型的穩定性和泛化能力，讓其在面對未知數據時表現更佳。