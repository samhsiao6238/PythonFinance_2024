# [æ¯”ç‡ç¶²](https://www.findrate.tw/)

_38å®¶éŠ€è¡Œç‰Œå‘ŠåŒ¯ç‡_

<br>

![](images/img_01.png)

<br>

## ç¶²é å…§å®¹

1. é¦–é ä¸­æœ‰å„å¹£åˆ¥åŒ¯ç‡è³‡è¨Šã€‚

    ![](images/img_02.png)

<br>

2. å¯æŸ¥è©¢æŒ‡å®šéŠ€è¡Œçš„å¤–åŒ¯è³‡è¨Šã€‚

    ![](images/img_03.png)

<br>

## å„ç¨®æŸ¥è©¢

1. ç›´æ¥å–å›å…¨éƒ¨éŠ€è¡ŒæŒ‡å®šåŒ¯ç‡è³‡è¨Šã€‚

    ```python
    import requests
    from bs4 import BeautifulSoup
    import pandas as pd

    url = "https://www.findrate.tw/USD/"
    response = requests.get(url)
    data = response.text

    soup = BeautifulSoup(data, 'html.parser')
    table = soup.find_all('table')[1]

    # æå–è¨Šæ¯
    exchange_rates = []
    # è·³éæ¨™é¡Œè¡Œ
    for row in table.find_all('tr')[1:]:
        cols = row.find_all('td')
        # æª¢æŸ¥è¡Œæ˜¯å¦æœ‰é æœŸçš„åˆ—æ•¸
        if len(cols) == 7:
            exchange_rates.append([col.text.strip() for col in cols])

    # å»ºç«‹ DataFrame
    if exchange_rates:
        df = pd.DataFrame(
            exchange_rates, 
            columns=[
                "éŠ€è¡Œåç¨±", "ç¾éˆ”è²·å…¥", "ç¾éˆ”è³£å‡º", "å³æœŸè²·å…¥", 
                "å³æœŸè³£å‡º", "æ›´æ–°æ™‚é–“", "ç¾éˆ”æ‰‹çºŒè²»"
            ]
        )
        # å„²å­˜åˆ° Excel
        df.to_excel("ç¾é‡‘åŒ¯ç‡_.xlsx", index=False)
    else:
        print("æœªæ‰¾åˆ°è³‡æ–™æˆ–è¡¨æ ¼æ ¼å¼ä¸åŒã€‚")
    ```

    ![](images/img_04.png)

<br>

2. å„ªåŒ–ä»¥ä¸Šä»£ç¢¼ï¼Œæ·»åŠ  `User-Agent`ï¼Œä¸¦ä»¥æ—¥æœŸä½œç‚ºæª”æ¡ˆåç¨±çš„å°¾ç¶´ã€‚

    ```python
    import requests
    from bs4 import BeautifulSoup
    import pandas as pd
    from datetime import datetime

    # ç›®æ¨™ç¶²å€
    url = "https://www.findrate.tw/USD/"

    # æ·»åŠ  User-Agent é¿å…è¢«é˜»æ“‹
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    # ç™¼é€è«‹æ±‚
    response = requests.get(url, headers=headers)

    # ç¢ºä¿è«‹æ±‚æˆåŠŸ
    if response.status_code == 200:
        data = response.text
        soup = BeautifulSoup(data, 'html.parser')

        # æ‰¾åˆ°æ‰€æœ‰è¡¨æ ¼ï¼Œé¿å…ç´¢å¼•éŒ¯èª¤
        tables = soup.find_all('table')
        
        if len(tables) > 1:
            # ç›®æ¨™è¡¨æ ¼
            table = tables[1]

            # æå–è¨Šæ¯
            exchange_rates = []
            # è·³éæ¨™é¡Œè¡Œ
            for row in table.find_all('tr')[1:]:
                cols = row.find_all('td')
                # ç¢ºä¿ç¬¦åˆé æœŸæ ¼å¼
                if len(cols) == 7:
                    exchange_rates.append([col.text.strip() for col in cols])

            # å»ºç«‹ DataFrame
            if exchange_rates:
                df = pd.DataFrame(
                    exchange_rates, 
                    columns=[
                        "éŠ€è¡Œåç¨±", "ç¾éˆ”è²·å…¥", "ç¾éˆ”è³£å‡º", "å³æœŸè²·å…¥", 
                        "å³æœŸè³£å‡º", "æ›´æ–°æ™‚é–“", "ç¾éˆ”æ‰‹çºŒè²»"
                    ]
                )

                # ç”Ÿæˆ Excel æª”æ¡ˆåç¨±
                today_str = datetime.now().strftime('%Y%m%d')
                excel_filename = f"ç¾é‡‘åŒ¯ç‡_{today_str}.xlsx"

                # å„²å­˜åˆ° Excel
                df.to_excel(excel_filename, index=False)

                print(f"âœ… åŒ¯ç‡è³‡æ–™å·²å„²å­˜è‡³ {excel_filename}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„åŒ¯ç‡æ•¸æ“šã€‚")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç›®æ¨™è¡¨æ ¼ï¼Œè«‹ç¢ºèªç¶²é çµæ§‹æ˜¯å¦è®Šæ›´ã€‚")
    else:
        print(f"âŒ ç„¡æ³•å–å¾—æ•¸æ“šï¼ŒHTTP ç‹€æ…‹ç¢¼: {response.status_code}")
    ```

    ![](images/img_05.png)

<br>

3. åœ¨æŸ¥è©¢çµæœä¸­åŠ å…¥æ—¥æœŸã€‚

    ```python
    import requests
    from bs4 import BeautifulSoup
    import pandas as pd
    from datetime import datetime

    def fetch_data():
        """å¾æŒ‡å®šç¶²ç«™æŠ“å–ç¾é‡‘åŒ¯ç‡æ•¸æ“š"""
        url = "https://www.findrate.tw/USD/"

        # æ·»åŠ  `User-Agent` ä»¥æ¨¡æ“¬ç€è¦½å™¨ï¼Œé¿å…è«‹æ±‚è¢«é˜»æ“‹
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        # ç™¼é€è«‹æ±‚
        response = requests.get(url, headers=headers)

        # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
        if response.status_code != 200:
            print(f"âŒ ç„¡æ³•å–å¾—æ•¸æ“šï¼ŒHTTP ç‹€æ…‹ç¢¼: {response.status_code}")
            return []

        # è§£æ HTML å…§å®¹
        soup = BeautifulSoup(response.text, 'html.parser')

        # å˜—è©¦æå–æ—¥æœŸè³‡è¨Š
        date_info = soup.find('span', style="float:right")
        if not date_info:
            print("âš ï¸ ç„¡æ³•æ‰¾åˆ°æ›´æ–°æ—¥æœŸï¼Œè«‹ç¢ºèªç¶²é çµæ§‹æ˜¯å¦è®Šæ›´ã€‚")
            return []

        # è§£ææ—¥æœŸ
        date_text = date_info.text.strip().split('æ™‚é–“ï¼š')[-1]
        # ç§»é™¤ `-` è½‰ç‚º YYYYMMDD æ ¼å¼
        date_str = date_text.replace('-', '')

        # æ‰¾åˆ°æ‰€æœ‰è¡¨æ ¼ï¼Œé¿å…ç´¢å¼•éŒ¯èª¤
        tables = soup.find_all('table')
        if len(tables) < 2:
            print("âš ï¸ æœªæ‰¾åˆ°ç›®æ¨™è¡¨æ ¼ï¼Œè«‹ç¢ºèªç¶²é çµæ§‹æ˜¯å¦è®Šæ›´ã€‚")
            return []

        # ç›®æ¨™è¡¨æ ¼
        table = tables[1]

        # æå–åŒ¯ç‡æ•¸æ“š
        exchange_rates = []
        # è·³éæ¨™é¡Œè¡Œ
        for row in table.find_all('tr')[1:]:
            cols = row.find_all('td')
            # ç¢ºä¿è¡Œæ•¸ç¬¦åˆé æœŸ
            if len(cols) == 7:
                row_data = [col.text.strip() for col in cols]

                # è™•ç†æ›´æ–°æ™‚é–“ï¼Œç§»é™¤ HTML è¨»è§£ä¸¦åˆä½µæ—¥æœŸ
                update_time = BeautifulSoup(row_data[5], "html.parser").text
                # å®Œæ•´æ™‚é–“è³‡è¨Š
                row_data[5] = f"{date_text} {update_time}"
                exchange_rates.append(row_data)
        # å›å‚³æ•¸æ“šèˆ‡æ—¥æœŸå­—ä¸²
        return exchange_rates, date_str

    # æŠ“å–æ•¸æ“š
    exchange_rates, date_str = fetch_data()

    if exchange_rates:
        # å»ºç«‹ DataFrame
        df = pd.DataFrame(
            exchange_rates, 
            columns=[
                "éŠ€è¡Œåç¨±", "ç¾éˆ”è²·å…¥", "ç¾éˆ”è³£å‡º", "å³æœŸè²·å…¥",
                "å³æœŸè³£å‡º", "æ›´æ–°æ™‚é–“", "ç¾éˆ”æ‰‹çºŒè²»"
            ]
        )

        # ç”Ÿæˆ Excel æª”æ¡ˆåç¨±
        excel_filename = f"ç¾é‡‘åŒ¯ç‡_V2_{date_str}.xlsx"

        # å„²å­˜ç‚º Excel æª”æ¡ˆ
        df.to_excel(excel_filename, index=False)

        print(f"âœ… åŒ¯ç‡è³‡æ–™å·²å„²å­˜è‡³ {excel_filename}")
    else:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•¸æ“šã€‚")
    ```

    ![](images/img_06.png)

<br>

4. å¾æŒ‡å®šéŠ€è¡ŒæŸ¥è©¢æŒ‡å®šè³‡è¨Šï¼›å…¶ä¸­å·²åŠ å…¥ User-Agentï¼Œé¿å…è«‹æ±‚è¢«é˜»æ“‹ã€‚

    ```python
    from bs4 import BeautifulSoup
    import requests

    # æŒ‡å®šæŸ¥è©¢çš„éŠ€è¡Œç¶²å€ï¼ˆç¯„ä¾‹ï¼šå°ç£éŠ€è¡Œï¼‰
    url = "https://www.findrate.tw/bank/10/"

    # æ·»åŠ  `User-Agent` ä»¥æ¨¡æ“¬ç€è¦½å™¨ï¼Œé¿å…è«‹æ±‚è¢«é˜»æ“‹
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    # ç™¼é€è«‹æ±‚
    response = requests.get(url, headers=headers)

    # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
    if response.status_code != 200:
        print(f"âŒ ç„¡æ³•å–å¾—æ•¸æ“šï¼ŒHTTP ç‹€æ…‹ç¢¼: {response.status_code}")
    else:
        # è§£æ HTML å…§å®¹
        soup = BeautifulSoup(response.text, 'html.parser')

        # æ‰¾åˆ°åŒ…å«åŒ¯ç‡çš„è¡¨æ ¼
        table = soup.find('table', {'width': '725px'})
        
        # ç¢ºä¿è¡¨æ ¼å­˜åœ¨
        if table:
            # å­˜æ”¾ USD åŒ¯ç‡è³‡è¨Š
            usd_info = None
            
            # éæ­·è¡¨æ ¼ä¸­çš„æ¯ä¸€è¡Œï¼ŒæŸ¥æ‰¾ USD
            for row in table.find_all('tr'):
                cols = row.find_all('td')
                if cols and 'USD' in cols[0].text:
                    usd_info = [col.text.strip() for col in cols]
                    break  # æ‰¾åˆ°å¾Œå³åœæ­¢æœå°‹
            
            # é¡¯ç¤ºçµæœ
            if usd_info:
                print("âœ… æˆåŠŸå–å¾—ç¾é‡‘åŒ¯ç‡è³‡è¨Šï¼š")
                print(usd_info)
            else:
                print("âš ï¸ æœªæ‰¾åˆ° USD åŒ¯ç‡æ•¸æ“šï¼Œè«‹ç¢ºèªç¶²é çµæ§‹æ˜¯å¦è®Šæ›´ã€‚")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç›®æ¨™è¡¨æ ¼ï¼Œè«‹ç¢ºèªç¶²é çµæ§‹æ˜¯å¦è®Šæ›´ã€‚")
    ```

    ![](images/img_07.png)

<br>

5. åœ¨ä»¥ä¸Šçš„æŸ¥è©¢è¼¸å‡ºä¸­åŠ å…¥éŠ€è¡Œè³‡è¨Šã€‚

    ```python
    from bs4 import BeautifulSoup
    import requests

    # æŒ‡å®šéŠ€è¡ŒæŸ¥è©¢ç¶²å€ï¼ˆç¯„ä¾‹ï¼šå°ç£éŠ€è¡Œï¼‰
    url = "https://www.findrate.tw/bank/10/"

    # æ·»åŠ  `User-Agent` ä»¥æ¨¡æ“¬ç€è¦½å™¨ï¼Œé¿å…è«‹æ±‚è¢«é˜»æ“‹
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    # ç™¼é€è«‹æ±‚
    response = requests.get(url, headers=headers)

    # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
    if response.status_code != 200:
        print(f"âŒ ç„¡æ³•å–å¾—æ•¸æ“šï¼ŒHTTP ç‹€æ…‹ç¢¼: {response.status_code}")
    else:
        # è§£æ HTML å…§å®¹
        soup = BeautifulSoup(response.text, 'html.parser')

        # å–å¾—éŠ€è¡Œåç¨±
        bank_title = soup.find('div', {'id': 'Title'})
        bank_name = bank_title.h1.text if bank_title else "æœªæ‰¾åˆ°éŠ€è¡Œåç¨±"

        # å–å¾—è·¨è¡Œè½‰å¸³ç¨‹å¼ç¢¼å’Œ SWIFT éŠ€è¡Œåœ‹éš›ç¨‹å¼ç¢¼
        bank_info_paragraph = bank_title.find_next_sibling('p') if bank_title else None
        if bank_info_paragraph:
            bank_transfer_code = bank_info_paragraph.find_all('b')[0].text if len(bank_info_paragraph.find_all('b')) > 0 else "æœªæ‰¾åˆ°è½‰å¸³ç¨‹å¼ç¢¼"
            swift_code = bank_info_paragraph.find_all('b')[1].text if len(bank_info_paragraph.find_all('b')) > 1 else "æœªæ‰¾åˆ° SWIFT ç¨‹å¼ç¢¼"
        else:
            bank_transfer_code = "æœªæ‰¾åˆ°è½‰å¸³ç¨‹å¼ç¢¼"
            swift_code = "æœªæ‰¾åˆ° SWIFT ç¨‹å¼ç¢¼"

        # æ‰¾åˆ°åŒ…å«åŒ¯ç‡çš„è¡¨æ ¼
        table = soup.find('table', {'width': '725px'})

        # ç¢ºä¿è¡¨æ ¼å­˜åœ¨
        usd_info = None
        if table:
            # éæ­·è¡¨æ ¼ä¸­çš„æ¯ä¸€è¡Œï¼ŒæŸ¥æ‰¾ USD
            for row in table.find_all('tr'):
                cols = row.find_all('td')
                if cols and 'USD' in cols[0].text:
                    usd_info = [col.text.strip() for col in cols]
                    break  # æ‰¾åˆ°å¾Œå³åœæ­¢æœå°‹

        # é¡¯ç¤ºçµæœ
        print(f"ğŸ¦ éŠ€è¡Œåç¨±: {bank_name}")
        print(f"ğŸ§ è·¨è¡Œè½‰å¸³ç¨‹å¼ç¢¼: {bank_transfer_code}")
        print(f"ğŸ’³ SWIFT ç¨‹å¼ç¢¼: {swift_code}")
        
        if usd_info:
            print("âœ… ç¾é‡‘åŒ¯ç‡è³‡è¨Š:", usd_info)
        else:
            print("âš ï¸ æœªæ‰¾åˆ° USD åŒ¯ç‡æ•¸æ“šï¼Œè«‹ç¢ºèªç¶²é çµæ§‹æ˜¯å¦è®Šæ›´ã€‚")
    ```

    ![](images/img_08.png)

<br>

## å°è£

1. æ‰¹æ¬¡æŸ¥è©¢å¤šå®¶éŠ€è¡Œè³‡è¨Šï¼›å¾çµæœå¯çŸ¥ï¼Œè‹¥æœ‰æœªæä¾›è³‡è¨Šçš„ç‹€æ…‹ä¸¦ä¸æœƒå‡ºéŒ¯ã€‚

    ```python
    import requests
    from bs4 import BeautifulSoup

    def fetch_data(bank_index):
        """
        å¾æŒ‡å®šçš„éŠ€è¡Œç´¢å¼•é é¢æŠ“å–éŠ€è¡Œè³‡è¨Šèˆ‡ USD åŒ¯ç‡æ•¸æ“š
        """
        # æŒ‡å®šéŠ€è¡ŒæŸ¥è©¢ç¶²å€
        url = f"https://www.findrate.tw/bank/{bank_index}/"

        # æ·»åŠ  `User-Agent` ä»¥æ¨¡æ“¬ç€è¦½å™¨ï¼Œé¿å…è«‹æ±‚è¢«é˜»æ“‹
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        # ç™¼é€è«‹æ±‚
        response = requests.get(url, headers=headers)

        # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
        if response.status_code != 200:
            print(
                f"âŒ ç„¡æ³•å–å¾—æ•¸æ“š (HTTP ç‹€æ…‹ç¢¼: {response.status_code})"
                f"ï¼Œè·³ééŠ€è¡Œç´¢å¼• {bank_index}"
            )
            return

        # è§£æ HTML å…§å®¹
        soup = BeautifulSoup(response.text, 'html.parser')

        # å–å¾—éŠ€è¡Œåç¨±
        bank_title = soup.find('div', {'id': 'Title'})
        bank_name = bank_title.h1.text.strip() if bank_title else f"æœªæ‰¾åˆ°éŠ€è¡Œåç¨± (ç´¢å¼•: {bank_index})"

        # å–å¾—è·¨è¡Œè½‰å¸³ç¨‹å¼ç¢¼å’Œ SWIFT éŠ€è¡Œåœ‹éš›ç¨‹å¼ç¢¼
        bank_info_paragraph = bank_title.find_next_sibling('p') if bank_title else None
        if bank_info_paragraph:
            bank_transfer_code = bank_info_paragraph.find_all('b')[0].text if len(bank_info_paragraph.find_all('b')) > 0 else "æœªæ‰¾åˆ°è½‰å¸³ç¨‹å¼ç¢¼"
            swift_code = bank_info_paragraph.find_all('b')[1].text if len(bank_info_paragraph.find_all('b')) > 1 else "æœªæ‰¾åˆ° SWIFT ç¨‹å¼ç¢¼"
        else:
            bank_transfer_code = "æœªæ‰¾åˆ°è½‰å¸³ç¨‹å¼ç¢¼"
            swift_code = "æœªæ‰¾åˆ° SWIFT ç¨‹å¼ç¢¼"

        # æ‰¾åˆ°åŒ…å«åŒ¯ç‡çš„è¡¨æ ¼
        table = soup.find('table', {'width': '725px'})

        # ç¢ºä¿è¡¨æ ¼å­˜åœ¨ä¸¦æå– USD åŒ¯ç‡æ•¸æ“š
        usd_info = None
        if table:
            for row in table.find_all('tr'):
                cols = row.find_all('td')
                if cols and 'USD' in cols[0].text:
                    usd_info = [col.text.strip() for col in cols]
                    # æ‰¾åˆ°å¾Œå³åœæ­¢æœå°‹
                    break

        # é¡¯ç¤ºçµæœ
        print("=" * 50)
        print(f"ğŸ¦ éŠ€è¡Œåç¨±: {bank_name}")
        print(f"ğŸ§ è·¨è¡Œè½‰å¸³ç¨‹å¼ç¢¼: {bank_transfer_code}")
        print(f"ğŸ’³ SWIFT ç¨‹å¼ç¢¼: {swift_code}")
        
        if usd_info:
            print("âœ… ç¾é‡‘åŒ¯ç‡è³‡è¨Š:", usd_info)
        else:
            print("âš ï¸ æœªæ‰¾åˆ° USD åŒ¯ç‡æ•¸æ“šï¼Œè«‹ç¢ºèªç¶²é çµæ§‹æ˜¯å¦è®Šæ›´ã€‚")
        print("=" * 50)

    # ä¸»ç¨‹å¼: éæ­·éŠ€è¡Œç´¢å¼• 1~10
    if __name__ == '__main__':
        # 10 å®¶éŠ€è¡Œ
        for i in range(1, 11):
            fetch_data(i)
    ```

    ![](images/img_09.png)

<br>

2. å°‡çµæœå„²å­˜ã€‚

    ```python
    import pandas as pd
    import requests
    from bs4 import BeautifulSoup
    from datetime import datetime

    def fetch_data(bank_index):
        """
        å¾æŒ‡å®šçš„éŠ€è¡Œç´¢å¼•é é¢æŠ“å–éŠ€è¡Œè³‡è¨Šèˆ‡ USD åŒ¯ç‡æ•¸æ“š
        """
        # æŒ‡å®šéŠ€è¡ŒæŸ¥è©¢ç¶²å€
        url = f"https://www.findrate.tw/bank/{bank_index}/"

        # æ·»åŠ  `User-Agent` ä»¥æ¨¡æ“¬ç€è¦½å™¨ï¼Œé¿å…è«‹æ±‚è¢«é˜»æ“‹
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        # ç™¼é€è«‹æ±‚
        response = requests.get(url, headers=headers)

        # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
        if response.status_code != 200:
            print(
                f"âŒ ç„¡æ³•å–å¾—æ•¸æ“š (HTTP ç‹€æ…‹ç¢¼: {response.status_code})ï¼Œ"
                f"è·³ééŠ€è¡Œç´¢å¼• {bank_index}"
            )
            return None

        # è§£æ HTML å…§å®¹
        soup = BeautifulSoup(response.text, 'html.parser')

        # å–å¾—å®Œæ•´éŠ€è¡Œåç¨±
        bank_title = soup.find('div', {'id': 'Title'})
        full_bank_name = bank_title.h1.text.strip() if bank_title else f"æœªæ‰¾åˆ°éŠ€è¡Œåç¨± (ç´¢å¼•: {bank_index})"
        # å»é™¤å¤šé¤˜å­—ä¸²
        bank_name = full_bank_name.split('ç‰Œå‘ŠåŒ¯ç‡')[0].strip()

        # å–å¾—è·¨è¡Œè½‰å¸³ç¨‹å¼ç¢¼å’Œ SWIFT éŠ€è¡Œåœ‹éš›ç¨‹å¼ç¢¼
        bank_info_paragraph = bank_title.find_next_sibling('p') if bank_title else None
        if bank_info_paragraph:
            bank_transfer_code = bank_info_paragraph.find_all('b')[0].text if len(bank_info_paragraph.find_all('b')) > 0 else "æœªæ‰¾åˆ°è½‰å¸³ç¨‹å¼ç¢¼"
            swift_code = bank_info_paragraph.find_all('b')[1].text if len(bank_info_paragraph.find_all('b')) > 1 else "æœªæ‰¾åˆ° SWIFT ç¨‹å¼ç¢¼"
        else:
            bank_transfer_code = "æœªæ‰¾åˆ°è½‰å¸³ç¨‹å¼ç¢¼"
            swift_code = "æœªæ‰¾åˆ° SWIFT ç¨‹å¼ç¢¼"

        # æ‰¾åˆ°åŒ…å«åŒ¯ç‡çš„è¡¨æ ¼
        table = soup.find('table', {'width': '725px'})

        # ç¢ºä¿è¡¨æ ¼å­˜åœ¨ä¸¦æå– USD åŒ¯ç‡æ•¸æ“š
        usd_info = None
        if table:
            for row in table.find_all('tr'):
                cols = row.find_all('td')
                if cols and 'USD' in cols[0].text:
                    # åªæå–åŒ¯ç‡æ•¸æ“š
                    usd_info = [col.text.strip() for col in cols[1:]]
                    # æ‰¾åˆ°å¾Œå³åœæ­¢æœå°‹
                    break

        # æª¢æŸ¥æ˜¯å¦æœ‰å–å¾—åˆ° USD åŒ¯ç‡æ•¸æ“š
        if usd_info is not None:
            return [bank_name, bank_transfer_code, swift_code] + usd_info
        else:
            # ç„¡æ•¸æ“šå‰‡å¡«å…¥ None
            return [bank_name, bank_transfer_code, swift_code] + [None, None, None, None]

    # åˆå§‹åŒ–å„²å­˜æ‰€æœ‰éŠ€è¡Œæ•¸æ“šçš„åˆ—è¡¨
    all_bank_data = []

    # éæ­· 1~10 çš„éŠ€è¡Œç´¢å¼•
    for i in range(1, 11):
        result = fetch_data(i)
        if result:
            all_bank_data.append(result)

    # å»ºç«‹ DataFrame
    df = pd.DataFrame(
        all_bank_data, 
        columns=[
            "éŠ€è¡Œåç¨±", "éŠ€è¡Œç¨‹å¼ç¢¼", "SWIFT Code", 
            "ç¾é‡‘è²·å…¥", "ç¾é‡‘è³£å‡º", "å³æœŸè²·å…¥", "å³æœŸè³£å‡º"
        ]
    )

    # å–å¾—ç•¶å‰æ—¥æœŸï¼Œæ ¼å¼ç‚º YYYYMMDD
    current_date = datetime.now().strftime("%Y%m%d")

    # ç”Ÿæˆ Excel æª”æ¡ˆåç¨±
    file_name = f"ç¾é‡‘åŒ¯ç‡_å½™æ•´_{current_date}.xlsx"

    # å„²å­˜ç‚º Excel æª”æ¡ˆ
    df.to_excel(file_name, index=False)

    print(f"âœ… åŒ¯ç‡æ•¸æ“šå·²å„²å­˜è‡³ {file_name}")
    ```

    ![](images/img_10.png)

<br>

## æ¯”è¼ƒ

1. é¡¯ç¤ºå³æœŸè²·å…¥èˆ‡å³æœŸè³£å‡ºæœ€é«˜èˆ‡æœ€ä½çš„éŠ€è¡Œã€‚

    ```python
    import pandas as pd
    import requests
    from bs4 import BeautifulSoup

    def fetch_data(bank_index):
        """
        å¾æŒ‡å®šçš„éŠ€è¡Œç´¢å¼•é é¢æŠ“å–éŠ€è¡Œè³‡è¨Šèˆ‡ USD åŒ¯ç‡æ•¸æ“š
        """
        url = f"https://www.findrate.tw/bank/{bank_index}/"

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            print(f"âŒ ç„¡æ³•å–å¾—æ•¸æ“š (HTTP ç‹€æ…‹ç¢¼: {response.status_code})ï¼Œè·³ééŠ€è¡Œç´¢å¼• {bank_index}")
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        bank_title = soup.find('div', {'id': 'Title'})
        full_bank_name = bank_title.h1.text.strip() if bank_title else f"æœªæ‰¾åˆ°éŠ€è¡Œåç¨± (ç´¢å¼•: {bank_index})"
        bank_name = full_bank_name.split('ç‰Œå‘ŠåŒ¯ç‡')[0].strip()

        table = soup.find('table', {'width': '725px'})

        usd_info = None
        if table:
            for row in table.find_all('tr'):
                cols = row.find_all('td')
                if cols and 'USD' in cols[0].text:
                    usd_info = [col.text.strip() for col in cols[1:]]  # åªæå–åŒ¯ç‡æ•¸æ“š
                    break  

        if usd_info:
            try:
                return [bank_name, float(usd_info[2]), float(usd_info[3])]  # å³æœŸè²·å…¥ã€å³æœŸè³£å‡º
            except ValueError:
                print(f"âš ï¸ {bank_name} è³‡æ–™æ ¼å¼éŒ¯èª¤ï¼Œè·³é")
                return None
        else:
            return None

    # æŠ“å–æ‰€æœ‰éŠ€è¡Œæ•¸æ“š
    all_bank_data = []
    for i in range(1, 11):
        result = fetch_data(i)
        if result:
            all_bank_data.append(result)

    # å»ºç«‹ DataFrame
    df = pd.DataFrame(all_bank_data, columns=["éŠ€è¡Œåç¨±", "å³æœŸè²·å…¥", "å³æœŸè³£å‡º"])

    # æ‰¾å‡ºå³æœŸè²·å…¥æœ€é«˜èˆ‡æœ€ä½
    if not df.empty:
        max_buy = df.loc[df["å³æœŸè²·å…¥"].idxmax()]
        min_buy = df.loc[df["å³æœŸè²·å…¥"].idxmin()]
        max_sell = df.loc[df["å³æœŸè³£å‡º"].idxmax()]
        min_sell = df.loc[df["å³æœŸè³£å‡º"].idxmin()]

        print("\nğŸ“Š å³æœŸè²·å…¥èˆ‡å³æœŸè³£å‡ºæœ€é«˜èˆ‡æœ€ä½éŠ€è¡Œ")
        print(f"ğŸ† å³æœŸè²·å…¥æœ€é«˜: {max_buy['éŠ€è¡Œåç¨±']} - {max_buy['å³æœŸè²·å…¥']}")
        print(f"ğŸ”» å³æœŸè²·å…¥æœ€ä½: {min_buy['éŠ€è¡Œåç¨±']} - {min_buy['å³æœŸè²·å…¥']}")
        print(f"ğŸ† å³æœŸè³£å‡ºæœ€é«˜: {max_sell['éŠ€è¡Œåç¨±']} - {max_sell['å³æœŸè³£å‡º']}")
        print(f"ğŸ”» å³æœŸè³£å‡ºæœ€ä½: {min_sell['éŠ€è¡Œåç¨±']} - {min_sell['å³æœŸè³£å‡º']}")
    else:
        print("âŒ ç„¡æ³•å–å¾—æœ‰æ•ˆçš„åŒ¯ç‡æ•¸æ“š")
    ```

    ![](images/img_11.png)

<br>

___

_END_