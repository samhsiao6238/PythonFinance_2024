import streamlit as st
# ä½¿ç”¨ langchain_openai çš„ ChatOpenAI
from langchain_openai import ChatOpenAI
# å¼•å…¥ LangChain çš„ PromptTemplate æ¨¡çµ„ï¼Œç”¨æ–¼å»ºç«‹ `æç¤ºæ¨¡æ¿`
from langchain.prompts import (
    PromptTemplate,
)

# å¯†é‘°å’Œæ¨¡å‹åç¨±
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
OPENAI_API_MODEL = st.secrets["OPENAI_API_MODEL"]

# æ¨™é¡Œ
st.title("ğŸ¦œğŸ”— Langchain - Blog Outline Generator App")


# è‡ªè¨‚å‡½æ•¸ï¼šç”Ÿæˆå¤§ç¶±
def generate_outline(topic):
    # å¯¦ä¾‹åŒ–èªè¨€æ¨¡å‹å°è±¡
    llm = ChatOpenAI(
        model_name=OPENAI_API_MODEL,
        openai_api_key=OPENAI_API_KEY
    )
    # å»ºç«‹æç¤ºæ¨¡æ¿
    template = "ä½œç‚ºç¶“é©—è±å¯Œçš„è³‡æ–™ç§‘å­¸å®¶å’Œå­¸è€…ï¼Œè«‹ä½ ç‚ºæœ‰é—œ {topic} çš„ä¸»é¡Œè£½å®šæ¼”è¬›å¤§ç¶±"
    prompt = PromptTemplate(
        # æŒ‡å®šæ¨¡æ¿ä¸­ä½¿ç”¨çš„è®Šæ•¸åç¨±
        input_variables=["topic"],
        # æä¾›æ¨¡æ¿å­—ä¸²
        template=template,
    )
    # æ ¼å¼åŒ–æç¤ºï¼Œå°‡ä¸»é¡Œæ’å…¥åˆ°æ¨¡æ¿ä¸­
    prompt_query = prompt.format(topic=topic)
    # é‹è¡Œèªè¨€æ¨¡å‹ï¼Œç”Ÿæˆå›æ‡‰
    response = llm.invoke(prompt_query)

    # å–å¾—å›æ‡‰ï¼šç›´æ¥è¨ªå• response çš„ content å±¬æ€§
    content = response.content
    # èª¿ç”¨è‡ªè¨‚å‡½æ•¸å°‡å›æ‡‰è§£æä¸¦ç‚ºæ›´æ˜“æ–¼ç†è§£çš„è‡ªç„¶èªè¨€æ ¼å¼
    formatted_response = format_response(content)

    # é¡¯ç¤ºçµæœ
    return st.info(formatted_response)


def format_response(response):
    """
    å°‡æ¨¡å‹çš„å›æ‡‰è§£æä¸¦æ ¼å¼åŒ–ç‚ºæ˜“æ–¼é–±è®€çš„è‡ªç„¶èªè¨€ã€‚
    """
    sections = response.split("\n\n")
    formatted_response = ""

    for section in sections:
        if section.startswith("Blog Title:"):
            formatted_response += f"### {section}\n"
        elif section.startswith("Introduction"):
            formatted_response += f"**{section}**\n"
        elif section.startswith("Section"):
            formatted_response += f"#### {section}\n"
        elif section.startswith("Conclusion"):
            formatted_response += f"**{section}**\n"
        elif section.startswith("Call to Action"):
            formatted_response += f"**{section}**\n"
        else:
            formatted_response += f"{section}\n"

    return formatted_response


# å»ºç«‹åç‚º "myform" çš„è¡¨å–®ï¼Œç”¨æ–¼æ¥æ”¶ç”¨æˆ¶è¼¸å…¥
with st.form("myform"):
    # åœ¨è¡¨å–®å…§æ·»åŠ ä¸€å€‹æ–‡æœ¬è¼¸å…¥æ¡†ï¼Œè®“ç”¨æˆ¶è¼¸å…¥ä¸»é¡Œ
    topic_text = st.text_input("è«‹è¼¸å…¥ä¸»é¡Œé—œéµå­—ï¼š", "")
    # åœ¨è¡¨å–®å…§æ·»åŠ ä¸€å€‹æäº¤æŒ‰éˆ•
    submitted = st.form_submit_button("æäº¤")
    if not OPENAI_API_KEY:
        # å¦‚æœæ²’æœ‰æä¾› API å¯†é‘°ï¼Œé¡¯ç¤ºæç¤ºè¨Šæ¯
        st.info("è«‹æ–°å¢ OpenAI API é‡‘é‘°ä»¥ç¹¼çºŒã€‚")
    elif submitted:
        # èª¿ç”¨è‡ªè¨‚å‡½æ•¸ï¼Œç”Ÿæˆåšå®¢å¤§ç¶±
        generate_outline(topic_text)
