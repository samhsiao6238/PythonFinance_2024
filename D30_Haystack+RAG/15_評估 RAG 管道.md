# è©•ä¼° RAG ç®¡é“

![](images/img_61.png)

## èªªæ˜

1. é€™æ˜¯å®˜æ–¹åœ¨ `2024/05/10` ç™¼ä½ˆçš„ [å®˜æ–¹æ•™ç¨‹](https://haystack.deepset.ai/tutorials/35_evaluating_rag_pipelines)ï¼Œæ“ä½œä¸­éœ€è¦ä½¿ç”¨ `OpenAI API Key`ã€‚

<br>

2. ç¯„ä¾‹çš„ç›®æ¨™æ˜¯ä½¿ç”¨ `Haystack çš„è©•ä¼°å·¥å…·` å° RAG ç®¡é“é€²è¡Œè©•ä¼°ï¼ŒåŒ…æ‹¬åŸºæ–¼ `æ¨¡å‹çš„è©•ä¼°` å’Œ `çµ±è¨ˆè©•ä¼°`ï¼Œå°¤å…¶æ˜¯å°æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) ç®¡é“çš„è©•ä¼°ã€‚

<br>

3. `RAG ç®¡é“`é€šå¸¸è‡³å°‘åŒ…æ‹¬ `æª¢ç´¢èˆ‡ç”Ÿæˆ` å…©å€‹æ­¥é©Ÿï¼Œè¦è©•ä¼°ä¸€å€‹å®Œæ•´çš„ RAG ç®¡é“ï¼Œéœ€è¦åˆ†åˆ¥å°é€™äº›ç®¡é“ä¸­çš„æ­¥é©Ÿé€²è¡Œè©•ä¼°ï¼ŒåŒæ™‚é‚„è¦å°æ•´å€‹å–®å…ƒé€²è¡Œè©•ä¼°ã€‚å„˜ç®¡æœ‰æ™‚å¯ä»¥ä½¿ç”¨éœ€è¦æ¨™ç±¤çš„çµ±è¨ˆæŒ‡æ¨™ä¾†è©•ä¼°æª¢ç´¢æ­¥é©Ÿï¼Œä½†å°ç”Ÿæˆæ­¥é©Ÿé€²è¡Œç›¸åŒçš„è©•ä¼°ä¸¦ä¸å®¹æ˜“ã€‚å› æ­¤ï¼Œé€šå¸¸ä¾é åŸºæ–¼æ¨¡å‹çš„æŒ‡æ¨™ä¾†è©•ä¼°ç”Ÿæˆæ­¥é©Ÿï¼Œä¸¦ä½¿ç”¨ `LLM` ä½œç‚º `è©•ä¼°è€…`ã€‚

<br>

## ä½¿ç”¨çµ„ä»¶

1. `InMemoryDocumentStore`ï¼š

<br>

2. `InMemoryEmbeddingRetriever`ï¼š

<br>

3. `PromptBuilder`ï¼š

<br>

4. `OpenAIGenerator`ï¼š

<br>

5. `DocumentMRREvaluator`ï¼š

<br>

6. `FaithfulnessEvaluator`ï¼š

<br>

7. `SASEvaluator`ï¼š

<br>

## æµç¨‹èªªæ˜

1. å»ºç«‹ä¸€å€‹åŸºæ–¼ PubMed æ•¸æ“šå›ç­”é†«å­¸å•é¡Œçš„ç®¡é“ã€‚

<br>

2. æ§‹å»ºä¸€å€‹è©•ä¼°ç®¡é“ï¼Œä½¿ç”¨ä¸€äº›æŒ‡æ¨™å¦‚æ–‡ä»¶ MRR å’Œç­”æ¡ˆå¿ å¯¦æ€§é€²è¡Œè©•ä¼°ã€‚

<br>

3. é‹è¡Œä½ çš„ RAG ç®¡é“ä¸¦ç”¨è©•ä¼°ç®¡é“å°å…¶è¼¸å‡ºé€²è¡Œè©•ä¼°ã€‚

<br>

## é–‹å§‹

1. å®‰è£ä¾è³´ã€‚

```bash
pip install haystack-ai "datasets>=2.6.1" sentence-transformers>=2.2.0
```

<br>

## ä¸‹è¼‰æ•¸æ“š

_å»ºç«‹ç®¡é“ä¸¦è©•ä¼°ä¹‹å‰ï¼Œå°‡ä½¿ç”¨ä¸€å€‹å¸¶æœ‰å•é¡Œã€ä¸Šä¸‹æ–‡å’Œç­”æ¡ˆæ¨™è¨»çš„ `PubMed` æ•¸æ“šé›†ã€‚_

1. ä¸‹è¼‰æ•¸æ“šã€‚

```python
# è¼‰å…¥æ•¸æ“šé›†
from datasets import load_dataset
from haystack import Document

# åŠ è¼‰ PubMedQA æ•¸æ“šé›†
dataset = load_dataset(
    "vblagoje/PubMedQA_instruction",
    split="train"
)
# åƒ…å–å‰ 1000 æ¢æ•¸æ“š
dataset = dataset.select(range(1000))

# æå–å…¶ä¸­çš„ `context` ä½œç‚ºæ–‡ä»¶
all_documents = [
    Document(content=doc["context"])
    for doc in dataset
]
# æå– `instruction` ä½œç‚ºå•é¡Œ
all_questions = [
    doc["instruction"]
    for doc in dataset
]
# æå– `response` ä½œç‚ºçœŸå¯¦ç­”æ¡ˆ
all_ground_truth_answers = [
    doc["response"]
    for doc in dataset
]
```

<br>

2. æœƒé€²è¡Œä¸‹è¼‰ã€‚

    ![](images/img_62.png)

<br>

## æ§‹å»ºç®¡é“

_å»ºç«‹ç´¢å¼•ç®¡é“ï¼Œä¸¦ä½¿ç”¨ `InMemoryDocumentStore` å°‡æ–‡ä»¶å¯«å…¥ `DocumentStore`ï¼Œé€™è£¡å¯«å…¥çš„æ˜¯ç·©å­˜ã€‚_

<br>

1. å°å…¥çµ„ä»¶ã€‚

```python
from typing import List
from haystack import Pipeline
from haystack.components.embedders import SentenceTransformersDocumentEmbedder
from haystack.components.writers import DocumentWriter
from haystack.document_stores.in_memory import InMemoryDocumentStore
from haystack.document_stores.types import DuplicatePolicy
```

<br>

2. å»ºç«‹ç´¢å¼•ç®¡é“å°è±¡ `indexing`ã€‚

```python
# å»ºç«‹ç´¢å¼•ç®¡é“
indexing = Pipeline()
```

<br>

3. å»ºç«‹æ–‡ä»¶å„²å­˜ã€åµŒå…¥å™¨ã€å¯«å…¥å™¨ã€‚

```python
# å»ºç«‹ `æ–‡ä»¶åµŒå…¥å™¨`
document_embedder = SentenceTransformersDocumentEmbedder(
    model="sentence-transformers/all-MiniLM-L6-v2"
)

# å»ºç«‹ `å…§å­˜æ–‡ä»¶å„²å­˜` å°è±¡
document_store = InMemoryDocumentStore()

# ä½¿ç”¨å„²å­˜å°è±¡å»ºç«‹ `æ–‡ä»¶å¯«å…¥å™¨`
document_writer = DocumentWriter(
    document_store=document_store,
    # é‡è¤‡æ™‚è·³é
    policy=DuplicatePolicy.SKIP
)
```

<br>

3. ç‚ºç®¡é“æ·»åŠ ç®¡é“å…ƒä»¶ã€‚

```python
# æ·»åŠ ç®¡é“å…ƒä»¶
indexing.add_component(
    instance=document_embedder,
    name="document_embedder"
)
indexing.add_component(
    instance=document_writer,
    name="document_writer"
)
```

<br>

4. å°‡å·²æ·»åŠ çš„ç®¡é“å…ƒä»¶é€²è¡Œé€£æ¥ã€‚

```python
# é€£æ¥ç®¡é“å…ƒä»¶ï¼šé€£æ¥åµŒå…¥å™¨å’Œå¯«å…¥å™¨
indexing.connect(
    "document_embedder.documents",
    "document_writer.documents"
)
```

<br>

5. å¯è§€å¯Ÿè¼¸å‡ºã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x33a517550>
    ğŸš… Components
    - document_embedder: SentenceTransformersDocumentEmbedder
    - document_writer: DocumentWriter
    ğŸ›¤ï¸ Connections
    - document_embedder.documents -> document_writer.documents (List[Document])
    ```

<br>

6. é‹è¡Œç´¢å¼•ç®¡é“ `indexing`ã€‚

```python
# åŸ·è¡Œç´¢å¼•ç®¡é“
indexing.run(
    {"document_embedder": {"documents": all_documents}}
)
```

<br>

7. è¼¸å‡ºå¦‚ä¸‹ã€‚

    ![](images/img_63.png)

<br>

8. å‡ºåœ–æŸ¥çœ‹ç®¡é“ã€‚

```python
indexing.draw('ex15-1.png')
```

<br>

9. åœ–å½¢å¦‚ä¸‹ã€‚

    ![](images/img_64.png)

<br>

## å»ºç«‹ RAG ç®¡é“

_å°‡ä½¿ç”¨ `InMemoryEmbeddingRetriever` ä¾†æª¢ç´¢èˆ‡æŸ¥è©¢ç›¸é—œçš„æ–‡ä»¶ï¼Œä¸¦é€é `OpenAIGenerator` ç”ŸæˆæŸ¥è©¢çš„ç­”æ¡ˆã€‚_

<br>

1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸ã€‚

```python
import os
from getpass import getpass
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
# è¨­ç½® OpenAI API é‡‘é‘°
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass("Enter OpenAI API key:")
```

<br>

2. å°å…¥ç¯„ä¾‹æ‰€éœ€ä¾è³´åº«ã€‚

```python
from haystack.components.builders import AnswerBuilder, PromptBuilder
from haystack.components.embedders import SentenceTransformersTextEmbedder
from haystack.components.generators import OpenAIGenerator
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
```

<br>

3. å®šç¾©æ¨¡æ¿ã€‚

```python
# å®šç¾©ç”Ÿæˆç­”æ¡ˆçš„æ¨¡æ¿
template = """
        æ‚¨å¿…é ˆåƒ…æ ¹æ“šçµ¦å®šçš„ä¸Šä¸‹æ–‡è³‡è¨Šå›ç­”ä»¥ä¸‹å•é¡Œã€‚

        ä¸Šä¸‹æ–‡:
        {% for document in documents %}
            {{ document.content }}
        {% endfor %}

        å•é¡Œ: {{question}}
        ç­”æ¡ˆ:
        """
```

<br>

4. å»ºç«‹ RAG ç®¡é“ã€‚

```python
# å»ºç«‹ RAG ç®¡é“
rag_pipeline = Pipeline()
```

<br>

5. æ·»åŠ ç®¡é“çµ„ä»¶ã€‚

```python
rag_pipeline.add_component(
    "query_embedder", 
    SentenceTransformersTextEmbedder(
        model="sentence-transformers/all-MiniLM-L6-v2"
    )
)
rag_pipeline.add_component(
    "retriever",
    InMemoryEmbeddingRetriever(document_store, top_k=3)
)
rag_pipeline.add_component(
    "prompt_builder",
    PromptBuilder(template=template)
)
rag_pipeline.add_component(
    "generator",
    OpenAIGenerator(model="gpt-4-turbo")
)
rag_pipeline.add_component(
    "answer_builder",
    AnswerBuilder()
)
```

<br>

6. é€£æ¥çµ„ä»¶ã€‚

```python
# é€£æ¥ç®¡é“çš„çµ„ä»¶
rag_pipeline.connect(
    "query_embedder", "retriever.query_embedding"
)
rag_pipeline.connect(
    "retriever", "prompt_builder.documents"
)
rag_pipeline.connect(
    "prompt_builder", "generator"
)
rag_pipeline.connect(
    "generator.replies", "answer_builder.replies"
)
rag_pipeline.connect(
    "generator.meta", "answer_builder.meta"
)
rag_pipeline.connect(
    "retriever", "answer_builder.documents"
)
```

<br>

## æå•

1. ä½¿ç”¨ç®¡é“çš„ `run()` æ–¹æ³•å¯é€²è¡Œ `æå•`ï¼Œè¦ç¢ºä¿å°‡å•é¡Œæä¾›çµ¦æ‰€æœ‰éœ€è¦å®ƒçš„çµ„ä»¶ä½œç‚ºè¼¸å…¥ï¼Œé€™äº›çµ„ä»¶åŒ…æ‹¬ `query_embedder`ã€`prompt_builder` å’Œ `answer_builder`ã€‚

```python
# å•é¡Œ
question = "å°å…’è‚ç§»æ¤è¡“å¾Œæ—©æœŸé™éˆ£ç´ åŸé«˜æ˜¯å¦è¡¨ç¤ºè¡“å¾Œæ•ˆæœä¸ä½³ï¼Ÿ"
# é‹è¡Œç®¡é“
response = rag_pipeline.run(
    {
        "query_embedder": {"text": question},
        "prompt_builder": {"question": question},
        "answer_builder": {"query": question}
    }
)
# è¼¸å‡º
print(response["answer_builder"]["answers"][0].data)
```

<br>

## è©•ä¼°ç®¡é“èªªæ˜

_ä½¿ç”¨ä»¥ä¸‹æŒ‡æ¨™ä¾†è©•ä¼°ç®¡é“_

<br>

1. æ–‡ä»¶å¹³å‡äº’æƒ æ’å (Document MRR): ä½¿ç”¨çœŸå¯¦æ¨™ç±¤è©•ä¼°æª¢ç´¢åˆ°çš„æ–‡ä»¶ï¼Œæª¢æŸ¥çœŸå¯¦æ–‡ä»¶åœ¨æª¢ç´¢åˆ°çš„æ–‡ä»¶åˆ—è¡¨ä¸­çš„æ’åã€‚

<br>

2. èªç¾©ç­”æ¡ˆç›¸ä¼¼æ€§ (Semantic Answer Similarity): ä½¿ç”¨çœŸå¯¦æ¨™ç±¤è©•ä¼°é æ¸¬çš„ç­”æ¡ˆï¼Œæª¢æŸ¥é æ¸¬ç­”æ¡ˆå’ŒçœŸå¯¦ç­”æ¡ˆçš„èªç¾©ç›¸ä¼¼æ€§ã€‚

<br>

3. å¿ å¯¦æ€§ (Faithfulness): ä½¿ç”¨ LLM è©•ä¼°ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å¯ä»¥å¾æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ¨æ–·å‡ºä¾†ï¼Œä¸éœ€è¦çœŸå¯¦æ¨™ç±¤ã€‚

<br>

## é€²è¡Œè©•ä¼°

1. é‹è¡Œ RAG ç®¡é“ä¸¦ç¢ºä¿æœ‰é€™äº›å•é¡Œçš„çœŸå¯¦æ¨™ç±¤ï¼ŒåŒ…æ‹¬ç­”æ¡ˆå’Œæ–‡ä»¶ï¼Œä»¥ä¸‹æ“ä½œå°‡å¾ `25` å€‹éš¨æ©Ÿå•é¡Œå’Œæ¨™ç±¤é–‹å§‹ã€‚

```python
import random

# éš¨æ©ŸæŠ½å– 25 å€‹å•é¡Œå’Œæ¨™ç±¤
# å•é¡Œã€å…·é«”ç­”æ¡ˆã€å…·é«”æ–‡ä»¶
questions, ground_truth_answers, ground_truth_docs = zip(
    *random.sample(
        list(zip(
            all_questions,
            all_ground_truth_answers,
            all_documents
        )),
        25
    )
)
```

<br>

2. é‹è¡Œç®¡é“ä¸¦è¨˜éŒ„å…¶è¿”å›çš„ç­”æ¡ˆå’Œæª¢ç´¢åˆ°çš„æ–‡ä»¶ã€‚

```python
# è¿”å›çš„ç­”æ¡ˆ
rag_answers = []
# ç´¢å¼•åˆ°çš„æ–‡ä»¶
retrieved_docs = []
# éæ­·å•é¡Œ
for question in list(questions):
    # é‹è¡Œç®¡é“
    response = rag_pipeline.run({
        "query_embedder": {"text": question},
        "prompt_builder": {"question": question},
        "answer_builder": {"query": question}
    })
    # è¼¸å‡º
    print(f"Question: {question}")
    print("Answer from pipeline:")
    print(response["answer_builder"]["answers"][0].data)
    print("\n-----------------------------------\n")

    # è¨˜éŒ„ç­”æ¡ˆ
    rag_answers.append(
        response["answer_builder"]["answers"][0].data
    )
    # ç´€éŒ„æª¢ç´¢åˆ°çš„æ–‡ä»¶
    retrieved_docs.append(
        response["answer_builder"]["answers"][0].documents
    )
```

<br>

## å»ºç«‹è©•ä¼°ç®¡é“

_é›–ç„¶æ¯å€‹è©•ä¼°å™¨éƒ½æ˜¯ `Haystack` ä¸­å¯ä»¥å–®ç¨é‹è¡Œçš„ `çµ„ä»¶`ï¼Œä½†å®ƒå€‘ä¹Ÿå¯ä»¥æ·»åŠ åˆ°ç®¡é“ä¸­ï¼Œé€™æ¨£å¯ä»¥æ§‹å»ºä¸€å€‹åŒ…å«æ‰€æœ‰ `è©•ä¼°æŒ‡æ¨™` çš„ `è©•ä¼°ç®¡é“`ã€‚_

<br>

1. å°å…¥åº«ã€‚

```python
from haystack.components.evaluators.document_mrr import DocumentMRREvaluator
from haystack.components.evaluators.faithfulness import FaithfulnessEvaluator
from haystack.components.evaluators.sas_evaluator import SASEvaluator
```

<br>

2. å»ºç«‹è©•ä¼°ç®¡é“ã€‚

```python
# å»ºç«‹è©•ä¼°ç®¡é“
eval_pipeline = Pipeline()
```

<br>

3. å»ºç«‹ç®¡é“çµ„ä»¶ï¼šçµ„ä»¶çš„ç¬¬ä¸€å€‹åƒæ•¸æ˜¯ `åç¨±`ï¼Œå°‡åœ¨ç®¡é“ä¸­ä½œç‚º `è­˜åˆ¥ç¬¦`ã€‚

```python
eval_pipeline.add_component(
    "doc_mrr_evaluator",
    DocumentMRREvaluator()
)
eval_pipeline.add_component(
    "faithfulness",
    FaithfulnessEvaluator()
)
eval_pipeline.add_component(
    "sas_evaluator",
    SASEvaluator(model="sentence-transformers/all-MiniLM-L6-v2")
)
```

<br>

4. é‹è¡Œè©•ä¼°ç®¡é“ã€‚

```python
# é‹è¡Œè©•ä¼°ç®¡é“
results = eval_pipeline.run({
    "doc_mrr_evaluator": {
        "ground_truth_documents": list([d] for d in ground_truth_docs),
        "retrieved_documents": retrieved_docs
    },
    "faithfulness": {
        "questions": list(questions),
        "contexts": list([d.content] for d in ground_truth_docs),
        "predicted_answers": rag_answers
    },
    "sas_evaluator": {
        "predicted_answers": rag_answers,
        "ground_truth_answers": list(ground_truth_answers)
    }
})
```

<br>

## æ§‹å»ºè©•ä¼°å ±å‘Š

1. é‹è¡Œè©•ä¼°ç®¡é“å¾Œå¯ç”Ÿæˆå®Œæ•´çš„è©•ä¼°å ±å‘Šï¼Œ`Haystack` æä¾›äº†ä¸€å€‹ `EvaluationRunResult` ä¾†é¡¯ç¤ºåˆ†æ•¸å ±å‘Šã€‚

```python
from haystack.evaluation.eval_run_result import EvaluationRunResult

inputs = {
    "question": list(questions),
    "contexts": list([d.content] for d in ground_truth_docs),
    "answer": list(ground_truth_answers),
    "predicted_answer": rag_answers,
}

evaluation_result = EvaluationRunResult(
    run_name="pubmed_rag_pipeline",
    inputs=inputs,
    results=results
)
evaluation_result.score_report()
```

<br>

## è½‰æ›å ±å‘Šæ ¼å¼

1. å¯å°‡å ±å‘Šè½‰æ›ç‚º Pandas DataFrameã€‚

```python
import pandas as pd

# å°‡è©•ä¼°çµæœè½‰æ›ç‚º DataFrame
results_df = evaluation_result.to_pandas()
results_df

# ä½¿ç”¨ Pandas éæ¿¾çµæœï¼Œé¡¯ç¤ºèªç¾©ç­”æ¡ˆç›¸ä¼¼æ€§æœ€é«˜çš„ 3 å€‹å’Œæœ€ä½çš„ 3 å€‹
top_3 = results_df.nlargest(3, 'sas_evaluator')
bottom_3 = results_df.nsmallest(3, 'sas_evaluator')
pd.concat([top_3, bottom_3])
```

<br>

___

_END_