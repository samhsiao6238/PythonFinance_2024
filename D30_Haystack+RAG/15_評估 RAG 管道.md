# è©•ä¼° RAG ç®¡é“

![](images/img_61.png)

## èªªæ˜

1. é€™æ˜¯å®˜æ–¹åœ¨ `2024/05/10` ç™¼ä½ˆçš„ [å®˜æ–¹æ•™ç¨‹](https://haystack.deepset.ai/tutorials/35_evaluating_rag_pipelines)ï¼Œæ“ä½œä¸­éœ€è¦ä½¿ç”¨ `OpenAI API Key`ã€‚

<br>

2. ç¯„ä¾‹çš„ç›®æ¨™æ˜¯ä½¿ç”¨ `Haystack çš„è©•ä¼°å·¥å…·` å° RAG ç®¡é“é€²è¡Œè©•ä¼°ï¼ŒåŒ…æ‹¬åŸºæ–¼ `æ¨¡å‹çš„è©•ä¼°` å’Œ `çµ±è¨ˆè©•ä¼°`ï¼Œå°¤å…¶æ˜¯å°æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) ç®¡é“çš„è©•ä¼°ã€‚

<br>

3. `RAG ç®¡é“`é€šå¸¸è‡³å°‘åŒ…æ‹¬ `æª¢ç´¢èˆ‡ç”Ÿæˆ` å…©å€‹æ­¥é©Ÿï¼Œè¦è©•ä¼°ä¸€å€‹å®Œæ•´çš„ RAG ç®¡é“ï¼Œéœ€è¦åˆ†åˆ¥å°é€™äº›ç®¡é“ä¸­çš„æ­¥é©Ÿé€²è¡Œè©•ä¼°ï¼ŒåŒæ™‚é‚„è¦å°æ•´å€‹å–®å…ƒé€²è¡Œè©•ä¼°ã€‚å„˜ç®¡æœ‰æ™‚å¯ä»¥ä½¿ç”¨éœ€è¦æ¨™ç±¤çš„çµ±è¨ˆæŒ‡æ¨™ä¾†è©•ä¼°æª¢ç´¢æ­¥é©Ÿï¼Œä½†å°ç”Ÿæˆæ­¥é©Ÿé€²è¡Œç›¸åŒçš„è©•ä¼°ä¸¦ä¸å®¹æ˜“ã€‚å› æ­¤ï¼Œé€šå¸¸ä¾é åŸºæ–¼æ¨¡å‹çš„æŒ‡æ¨™ä¾†è©•ä¼°ç”Ÿæˆæ­¥é©Ÿï¼Œä¸¦ä½¿ç”¨ `LLM` ä½œç‚º `è©•ä¼°è€…`ã€‚

<br>

## ä½¿ç”¨çµ„ä»¶

1. `InMemoryDocumentStore`ï¼š

<br>

2. `InMemoryEmbeddingRetriever`ï¼š

<br>

3. `PromptBuilder`ï¼š

<br>

4. `OpenAIGenerator`ï¼š

<br>

5. `DocumentMRREvaluator`ï¼š

<br>

6. `FaithfulnessEvaluator`ï¼š

<br>

7. `SASEvaluator`ï¼š

<br>

## æµç¨‹èªªæ˜

1. å»ºç«‹ä¸€å€‹åŸºæ–¼ PubMed æ•¸æ“šå›ç­”é†«å­¸å•é¡Œçš„ç®¡é“ã€‚

<br>

2. å»ºç«‹ä¸€å€‹è©•ä¼°ç®¡é“ï¼Œä½¿ç”¨ä¸€äº›æŒ‡æ¨™å¦‚æ–‡ä»¶ MRR å’Œç­”æ¡ˆå¿ å¯¦æ€§é€²è¡Œè©•ä¼°ã€‚

<br>

3. é‹è¡Œä½ çš„ RAG ç®¡é“ä¸¦ç”¨è©•ä¼°ç®¡é“å°å…¶è¼¸å‡ºé€²è¡Œè©•ä¼°ã€‚

<br>

## é–‹å§‹

1. å®‰è£ä¾è³´ã€‚

    ```bash
    pip install haystack-ai "datasets>=2.6.1" sentence-transformers>=2.2.0
    ```

<br>

## ä¸‹è¼‰æ•¸æ“š

_å»ºç«‹ç®¡é“ä¸¦è©•ä¼°ä¹‹å‰ï¼Œå°‡ä½¿ç”¨ä¸€å€‹å¸¶æœ‰å•é¡Œã€ä¸Šä¸‹æ–‡å’Œç­”æ¡ˆæ¨™è¨»çš„ `PubMed` æ•¸æ“šé›†ã€‚_

1. ä¸‹è¼‰æ•¸æ“šã€‚

    ```python
    # è¼‰å…¥æ•¸æ“šé›†
    from datasets import load_dataset
    from haystack import Document

    # åŠ è¼‰ PubMedQA æ•¸æ“šé›†
    dataset = load_dataset(
        "vblagoje/PubMedQA_instruction",
        split="train"
    )
    # åƒ…å–å‰ 1000 æ¢æ•¸æ“š
    dataset = dataset.select(range(1000))

    # æå–å…¶ä¸­çš„ `context` ä½œç‚ºæ–‡ä»¶
    all_documents = [
        Document(content=doc["context"])
        for doc in dataset
    ]
    # æå– `instruction` ä½œç‚ºå•é¡Œ
    all_questions = [
        doc["instruction"]
        for doc in dataset
    ]
    # æå– `response` ä½œç‚ºçœŸå¯¦ç­”æ¡ˆ
    all_ground_truth_answers = [
        doc["response"]
        for doc in dataset
    ]
    ```

<br>

2. æœƒé€²è¡Œä¸‹è¼‰ã€‚

    ![](images/img_62.png)

<br>

## å»ºç«‹ç®¡é“

_å»ºç«‹ç´¢å¼•ç®¡é“ï¼Œä¸¦ä½¿ç”¨ `InMemoryDocumentStore` å°‡æ–‡ä»¶å¯«å…¥ `DocumentStore`ï¼Œé€™è£¡å¯«å…¥çš„æ˜¯ç·©å­˜ã€‚_

<br>

1. å°å…¥çµ„ä»¶ã€‚

    ```python
    from typing import List
    from haystack import Pipeline
    from haystack.components.embedders import SentenceTransformersDocumentEmbedder
    from haystack.components.writers import DocumentWriter
    from haystack.document_stores.in_memory import InMemoryDocumentStore
    from haystack.document_stores.types import DuplicatePolicy
    ```

<br>

2. å»ºç«‹ç´¢å¼•ç®¡é“å°è±¡ `indexing`ã€‚

    ```python
    # å»ºç«‹ç´¢å¼•ç®¡é“
    indexing = Pipeline()
    ```

<br>

3. å»ºç«‹æ–‡ä»¶å„²å­˜ã€åµŒå…¥å™¨ã€å¯«å…¥å™¨ã€‚

    ```python
    # å»ºç«‹ `æ–‡ä»¶åµŒå…¥å™¨`
    document_embedder = SentenceTransformersDocumentEmbedder(
        model="sentence-transformers/all-MiniLM-L6-v2"
    )

    # å»ºç«‹ `å…§å­˜æ–‡ä»¶å„²å­˜` å°è±¡
    document_store = InMemoryDocumentStore()

    # ä½¿ç”¨å„²å­˜å°è±¡å»ºç«‹ `æ–‡ä»¶å¯«å…¥å™¨`
    document_writer = DocumentWriter(
        document_store=document_store,
        # é‡è¤‡æ™‚è·³é
        policy=DuplicatePolicy.SKIP
    )
    ```

<br>

4. ç‚ºç®¡é“æ·»åŠ ç®¡é“å…ƒä»¶ã€‚

    ```python
    # æ·»åŠ ç®¡é“å…ƒä»¶
    indexing.add_component(
        instance=document_embedder,
        name="document_embedder"
    )
    indexing.add_component(
        instance=document_writer,
        name="document_writer"
    )
    ```

<br>

5. å°‡å·²æ·»åŠ çš„ç®¡é“å…ƒä»¶é€²è¡Œé€£æ¥ã€‚

    ```python
    # é€£æ¥ç®¡é“å…ƒä»¶ï¼šé€£æ¥åµŒå…¥å™¨å’Œå¯«å…¥å™¨
    indexing.connect(
        "document_embedder.documents",
        "document_writer.documents"
    )
    ```

<br>

6. å¯è§€å¯Ÿè¼¸å‡ºã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x33a517550>
    ğŸš… Components
    - document_embedder: SentenceTransformersDocumentEmbedder
    - document_writer: DocumentWriter
    ğŸ›¤ï¸ Connections
    - document_embedder.documents -> document_writer.documents (List[Document])
    ```

<br>

7. é‹è¡Œç´¢å¼•ç®¡é“ `indexing`ã€‚

    ```python
    # åŸ·è¡Œç´¢å¼•ç®¡é“
    indexing.run(
        {"document_embedder": {"documents": all_documents}}
    )
    ```

<br>

8. è¼¸å‡ºå¦‚ä¸‹ã€‚

    ![](images/img_63.png)

<br>

9. å‡ºåœ–æŸ¥çœ‹ç®¡é“ã€‚

    ```python
    indexing.draw('ex15-1.png')
    ```

<br>

10. åœ–å½¢å¦‚ä¸‹ã€‚

    ![](images/img_64.png)

<br>

## å»ºç«‹ RAG ç®¡é“

_å°‡ä½¿ç”¨ `InMemoryEmbeddingRetriever` ä¾†æª¢ç´¢èˆ‡æŸ¥è©¢ç›¸é—œçš„æ–‡ä»¶ï¼Œä¸¦é€é `OpenAIGenerator` ç”ŸæˆæŸ¥è©¢çš„ç­”æ¡ˆã€‚_

<br>

1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸ã€‚

    ```python
    import os
    from getpass import getpass
    from dotenv import load_dotenv

    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    load_dotenv()
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
    # è¨­ç½® OpenAI API é‡‘é‘°
    if "OPENAI_API_KEY" not in os.environ:
        os.environ["OPENAI_API_KEY"] = getpass("Enter OpenAI API key:")
    ```

<br>

2. å°å…¥ç¯„ä¾‹æ‰€éœ€ä¾è³´åº«ã€‚

    ```python
    from haystack.components.builders import AnswerBuilder, PromptBuilder
    from haystack.components.embedders import SentenceTransformersTextEmbedder
    from haystack.components.generators import OpenAIGenerator
    from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
    ```

<br>

3. å®šç¾©æ¨¡æ¿ã€‚

    ```python
    # å®šç¾©ç”Ÿæˆç­”æ¡ˆçš„æ¨¡æ¿
    template = """
            æ‚¨å¿…é ˆåƒ…æ ¹æ“šçµ¦å®šçš„ä¸Šä¸‹æ–‡è³‡è¨Šå›ç­”ä»¥ä¸‹å•é¡Œã€‚

            ä¸Šä¸‹æ–‡:
            {% for document in documents %}
                {{ document.content }}
            {% endfor %}

            å•é¡Œ: {{question}}
            ç­”æ¡ˆ:
            """
    ```

<br>

4. å»ºç«‹ RAG ç®¡é“ã€‚

    ```python
    # å»ºç«‹ RAG ç®¡é“
    rag_pipeline = Pipeline()
    ```

<br>

5. æ·»åŠ ç®¡é“çµ„ä»¶ã€‚

    ```python
    rag_pipeline.add_component(
        "query_embedder", 
        SentenceTransformersTextEmbedder(
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
    )
    rag_pipeline.add_component(
        "retriever",
        InMemoryEmbeddingRetriever(document_store, top_k=3)
    )
    rag_pipeline.add_component(
        "prompt_builder",
        PromptBuilder(template=template)
    )
    rag_pipeline.add_component(
        "generator",
        OpenAIGenerator(model="gpt-4-turbo")
    )
    rag_pipeline.add_component(
        "answer_builder",
        AnswerBuilder()
    )
    ```

<br>

6. é€£æ¥çµ„ä»¶ã€‚

    ```python
    # é€£æ¥ç®¡é“çš„çµ„ä»¶
    rag_pipeline.connect(
        "query_embedder", "retriever.query_embedding"
    )
    rag_pipeline.connect(
        "retriever", "prompt_builder.documents"
    )
    rag_pipeline.connect(
        "prompt_builder", "generator"
    )
    rag_pipeline.connect(
        "generator.replies", "answer_builder.replies"
    )
    rag_pipeline.connect(
        "generator.meta", "answer_builder.meta"
    )
    rag_pipeline.connect(
        "retriever", "answer_builder.documents"
    )
    ```

<br>

7. è¼¸å‡ºå¦‚ä¸‹ã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x38bad2230>
    
    ğŸš… Components
        - query_embedder: SentenceTransformersTextEmbedder
        - retriever: InMemoryEmbeddingRetriever
        - prompt_builder: PromptBuilder
        - generator: OpenAIGenerator
        - answer_builder: AnswerBuilder
    
    ğŸ›¤ï¸ Connections
        - query_embedder.embedding -> retriever.query_embedding (List[float])
        - retriever.documents -> prompt_builder.documents (List[Document])
        - retriever.documents -> answer_builder.documents (List[Document])
        - prompt_builder.prompt -> generator.prompt (str)
        - generator.replies -> answer_builder.replies (List[str])
        - generator.meta -> answer_builder.meta (List[Dict[str, Any]])
    ```

<br>

## æå•

1. ä½¿ç”¨ç®¡é“çš„ `run()` æ–¹æ³•å¯é€²è¡Œ `æå•`ï¼Œè¦ç¢ºä¿å°‡å•é¡Œæä¾›çµ¦æ‰€æœ‰éœ€è¦å®ƒçš„çµ„ä»¶ä½œç‚ºè¼¸å…¥ï¼Œé€™äº›çµ„ä»¶åŒ…æ‹¬ `query_embedder`ã€`prompt_builder` å’Œ `answer_builder`ã€‚

    ```python
    # å•é¡Œï¼šå°å…’è‚ç§»æ¤è¡“å¾Œæ—©æœŸé™éˆ£ç´ åŸé«˜æ˜¯å¦è¡¨ç¤ºè¡“å¾Œæ•ˆæœä¸ä½³ï¼Ÿ
    question = "Do high levels of procalcitonin in the early phase after?"

    # é‹è¡Œç®¡é“
    response = rag_pipeline.run(
        {
            "query_embedder": {"text": question},
            "prompt_builder": {"question": question},
            "answer_builder": {"query": question}
        }
    )
    # è¼¸å‡º
    print(response["answer_builder"]["answers"][0].data)
    ```

<br>

2. çµæœã€‚

    ```bash
    Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.75it/s]
    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
    To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
    Yes, high levels of procalcitonin in the early phase after pediatric liver transplantation indicate a poor postoperative outcome. Patients with high procalcitonin levels on postoperative day 2 were observed to have higher International Normalized Ratio values on postoperative day 5 and suffered more often from primary graft non-function. Additionally, these patients experienced longer stays in the pediatric intensive care unit and required prolonged mechanical ventilation. These indications collectively suggest a correlation between early postoperative elevations in procalcitonin and compromised postoperative recovery.
    ```

<br>

## ä½¿ç”¨ä¸­æ–‡æå•

_åœ¨ Haystack çš„æœ€æ–°å®˜æ–¹æ–‡ä»¶ä¸­ä¸¦ç„¡åˆªé™¤æˆ–æ–·é–‹çµ„ä»¶é€£æ¥å¾—æ–¹æ³•ï¼Œè‹¥è¦ä½¿ç”¨ä¸­æ–‡ï¼Œå°±å¿…é ˆé‡å»ºç®¡é“ã€‚_

<br>

1. åŸæœ¬ä½¿ç”¨çš„æ¨¡å‹æ˜¯ `sentence-transformers/all-MiniLM-L6-v2`ï¼Œé€™æ¨¡å‹æ˜¯é‡å°è‹±æ–‡æ–‡æœ¬é€²è¡Œè¨“ç·´çš„ï¼Œå°æ–¼ä¸­æ–‡æ–‡æœ¬çš„æ”¯æŒå¯èƒ½æœ‰é™ï¼Œå› æ­¤åœ¨è™•ç†ä¸­æ–‡æŸ¥è©¢æ™‚ç„¡æ³•ç”Ÿæˆé«˜è³ªé‡çš„åµŒå…¥ï¼Œå°è‡´æª¢ç´¢å’Œç”Ÿæˆçš„çµæœä¸ç†æƒ³ã€‚

<br>

2. æ”¹ç”¨æ”¯æŒå¤šèªè¨€çš„åµŒå…¥æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹ `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ï¼Œé€™å€‹æ¨¡å‹èƒ½å¤ æ›´å¥½åœ°è™•ç†å¤šèªè¨€æ–‡æœ¬ï¼Œç‰¹åˆ¥æ³¨æ„ï¼Œé›–ç„¶çµ„ä»¶åç¨±åœ¨ç®¡é“ä¸­å¿…é ˆæ˜¯å”¯ä¸€çš„ï¼Œä½†é€™è£¡å› ç‚ºæœƒå»ºç«‹æ–°çš„ç®¡é“ï¼Œæ‰€ä»¥åç¨±æ²¿ç”¨ç„¡å¦¨ï¼Œå”¯ç¨åµŒå…¥æ¨¡å‹éƒ¨åˆ†é‡æ–°å‘½åç‚º `multi_language_embedder`ã€‚

<br>

3. å»ºç«‹æ–°çš„ç®¡é“ã€‚

    ```python
    # å»ºç«‹ RAG ç®¡é“
    new_rag_pipeline = Pipeline()
    ```

<br>

4. æ”¹ç”¨æ–°çš„åµŒå…¥æ¨¡å‹ã€‚

    ```python
    # æ”¹ç”¨æ”¯æŒå¤šèªè¨€çš„åµŒå…¥æ¨¡å‹
    new_rag_pipeline.add_component(
        # é€™æ˜¯æ–°çš„åµŒå…¥æ¨¡å‹åç¨±
        "multi_language_embedder",
        SentenceTransformersTextEmbedder(
            # ä½¿ç”¨æ–°çš„åµŒå…¥æ¨¡å‹
            model="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
    )
    # å…¶é¤˜çµ„ä»¶è¨­å®šç¶­æŒä¸ä¾¿
    new_rag_pipeline.add_component(
        "retriever",
        InMemoryEmbeddingRetriever(document_store, top_k=3)
    )
    new_rag_pipeline.add_component(
        "prompt_builder",
        PromptBuilder(template=template)
    )
    new_rag_pipeline.add_component(
        "generator",
        OpenAIGenerator(model="gpt-4-turbo")
    )
    new_rag_pipeline.add_component(
        "answer_builder",
        AnswerBuilder()
    )
    ```

<br>

5. é€£æ¥çµ„ä»¶ã€‚

    ```python
    # é€£æ¥ç®¡é“çš„çµ„ä»¶
    new_rag_pipeline.connect(
        "multi_language_embedder", "retriever.query_embedding"
    )
    new_rag_pipeline.connect(
        "retriever", "prompt_builder.documents"
    )
    new_rag_pipeline.connect(
        "prompt_builder", "generator"
    )
    new_rag_pipeline.connect(
        "generator.replies", "answer_builder.replies"
    )
    new_rag_pipeline.connect(
        "generator.meta", "answer_builder.meta"
    )
    new_rag_pipeline.connect(
        "retriever", "answer_builder.documents"
    )
    ```

<br>

6. è¼¸å‡ºå¦‚ä¸‹ã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x391b3a470>

    ğŸš… Components
        - multi_language_embedder: SentenceTransformersTextEmbedder
        - retriever: InMemoryEmbeddingRetriever
        - prompt_builder: PromptBuilder
        - generator: OpenAIGenerator
        - answer_builder: AnswerBuilder

    ğŸ›¤ï¸ Connections
        - multi_language_embedder.embedding -> retriever.query_embedding (List[float])
        - retriever.documents -> prompt_builder.documents (List[Document])
        - retriever.documents -> answer_builder.documents (List[Document])
        - prompt_builder.prompt -> generator.prompt (str)
        - generator.replies -> answer_builder.replies (List[str])
        - generator.meta -> answer_builder.meta (List[Dict[str, Any]])
    ```

7. æŸ¥çœ‹ç®¡é“ã€‚

    ```python
    new_rag_pipeline.draw('new_rag_pipeline.png')
    ```

    ![](images/img_65.png)

<br>

## è‡ªå®šç¾©è¼¸å‡ºåœ–ç‰‡å‡½æ•¸

1. å»ºç«‹ä¸€å€‹è³‡æ–™å¤¾ `utils`ï¼Œæ·»åŠ ä¸€å€‹æ¨¡çµ„ `draw_pipeline.py`ï¼Œç·¨è¼¯å…§å®¹å¦‚ä¸‹ã€‚

    ```python
    # å°å…¥éœ€è¦çš„å‡½æ•¸å’Œæ¨¡çµ„
    from IPython.display import Image, display  # ç”¨æ–¼é¡¯ç¤ºåœ–ç‰‡çš„ IPython å‡½æ•¸


    # å®šç¾©æ“´å±•çš„ draw å‡½æ•¸
    def draw_and_display(pipeline, image_path):
        """
        æ“´å±• draw å‡½æ•¸ï¼Œç”Ÿæˆåœ–ç‰‡å¾Œç›´æ¥åœ¨ Jupyter Notebook ä¸­é¡¯ç¤ºã€‚

        :param pipeline: è¦ç¹ªè£½çš„ç®¡é“å°è±¡
        :param image_path: ä¿å­˜åœ–ç‰‡çš„è·¯å¾‘
        """
        # ç”Ÿæˆä¸¦ä¿å­˜ç®¡é“åœ–ç‰‡
        pipeline.draw(image_path)

        # è®€å–ä¸¦é¡¯ç¤ºåœ–ç‰‡
        display(Image(filename=image_path))
    ```

<br>

2. åœ¨ `JupyterNotebook` ä¸­èª¿ç”¨ï¼Œå„²å­˜åœ–ç‰‡å¾ŒåŒæ™‚æœƒé¡¯ç¤ºåœ¨å„²å­˜æ ¼ä¸­ã€‚

    ```python
    from utils.draw_pipeline import draw_and_display

    draw_and_display(new_rag_pipeline, 'new_rag_pipeline.png')
    ```

<br>

## ä½¿ç”¨ä¸­æ–‡é€²è¡Œæå•

1. ä½¿ç”¨ä¸­æ–‡é€²è¡Œæå•ã€‚

    ```python
    # å•é¡Œ
    question = "å°å…’è‚ç§»æ¤è¡“å¾Œæ—©æœŸé™éˆ£ç´ åŸé«˜æ˜¯å¦è¡¨ç¤ºè¡“å¾Œæ•ˆæœä¸ä½³ï¼Ÿ"

    # é‹è¡Œç®¡é“
    response = new_rag_pipeline.run(
        {
            # ä½¿ç”¨æ–°çš„åµŒå…¥æ¨¡å‹ `multi_language_embedder`
            "multi_language_embedder": {"text": question},
            "prompt_builder": {"question": question},
            "answer_builder": {"query": question}
        }
    )
    # è¼¸å‡º
    print(response["answer_builder"]["answers"][0].data)
    ```

<br>

2. çµæœã€‚

    ```bash
    æ˜¯çš„ï¼Œå°å…’è‚ç§»æ¤è¡“å¾Œæ—©æœŸé™éˆ£ç´ åŸ(PCT)æ°´å¹³çš„å‡é«˜èˆ‡è¡“å¾Œæ•ˆæœä¸ä½³æœ‰é—œã€‚åœ¨ä¸Šè¿°çš„ç ”ç©¶ä¸­ï¼Œé¡¯ç¤ºè¡“å¾Œç¬¬äºŒå¤©PCTæ°´å¹³é«˜çš„æ‚£è€…åœ¨è¡“å¾Œç¬¬äº”å¤©æœ‰æ›´é«˜çš„åœ‹éš›æ¨™æº–åŒ–æ¯”ç‡å€¼ï¼Œæ›´å¸¸å‡ºç¾åŸç™¼æ€§ç§»æ¤ç‰©ç„¡åŠŸèƒ½çš„æƒ…æ³ï¼Œä¸¦ä¸”åœ¨å…’ç§‘é‡ç—‡ç›£è­·ç—…æˆ¿åœç•™æ™‚é–“æ›´é•·ï¼Œä»¥åŠéœ€è¦æ›´é•·æ™‚é–“çš„æ©Ÿæ¢°é€šæ°£ã€‚è¿™äº›çµæœèªªæ˜PCTæ°´å¹³çš„å‡é«˜èˆ‡è¡“å¾Œæ•ˆæœä¸ä½³æœ‰é—œï¼Œå°¤å…¶æ˜¯åœ¨è‚åŠŸèƒ½åŠç—…äººæ¢å¾©æ–¹é¢ã€‚
    ```

<br>

## è©•ä¼°ç®¡é“èªªæ˜

_ä½¿ç”¨ä»¥ä¸‹æŒ‡æ¨™ä¾†è©•ä¼°ç®¡é“_

<br>

1. æ–‡ä»¶å¹³å‡äº’æƒ æ’å (Document MRR)ï¼šä½¿ç”¨çœŸå¯¦æ¨™ç±¤è©•ä¼°æª¢ç´¢åˆ°çš„æ–‡ä»¶ï¼Œæª¢æŸ¥çœŸå¯¦æ–‡ä»¶åœ¨æª¢ç´¢åˆ°çš„æ–‡ä»¶åˆ—è¡¨ä¸­çš„æ’åã€‚

<br>

2. èªç¾©ç­”æ¡ˆç›¸ä¼¼æ€§ (Semantic Answer Similarity)ï¼šä½¿ç”¨çœŸå¯¦æ¨™ç±¤è©•ä¼°é æ¸¬çš„ç­”æ¡ˆï¼Œæª¢æŸ¥é æ¸¬ç­”æ¡ˆå’ŒçœŸå¯¦ç­”æ¡ˆçš„èªç¾©ç›¸ä¼¼æ€§ã€‚

<br>

3. å¿ å¯¦æ€§ (Faithfulness): ä½¿ç”¨ `LLM` è©•ä¼°ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å¯ä»¥å¾æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ¨æ–·å‡ºä¾†ï¼Œä¸éœ€è¦çœŸå¯¦æ¨™ç±¤ã€‚

<br>

## é€²è¡Œè©•ä¼°

1. é‹è¡Œ `RAG` ç®¡é“ä¸¦ç¢ºä¿æœ‰é€™äº›å•é¡Œçš„çœŸå¯¦æ¨™ç±¤ï¼ŒåŒ…æ‹¬ç­”æ¡ˆå’Œæ–‡ä»¶ï¼Œä»¥ä¸‹æ“ä½œå°‡å¾ `25` å€‹éš¨æ©Ÿå•é¡Œå’Œæ¨™ç±¤é–‹å§‹ã€‚

    ```python
    import random

    # éš¨æ©ŸæŠ½å– 25 å€‹å•é¡Œå’Œæ¨™ç±¤
    # å•é¡Œã€å…·é«”ç­”æ¡ˆã€å…·é«”æ–‡ä»¶
    questions, ground_truth_answers, ground_truth_docs = zip(
        *random.sample(
            list(zip(
                all_questions,
                all_ground_truth_answers,
                all_documents
            )),
            25
        )
    )
    ```

<br>

2. é‹è¡Œç®¡é“ä¸¦è¨˜éŒ„å…¶è¿”å›çš„ç­”æ¡ˆå’Œæª¢ç´¢åˆ°çš„æ–‡ä»¶ã€‚

    ```python
    # è¿”å›çš„ç­”æ¡ˆ
    rag_answers = []
    # ç´¢å¼•åˆ°çš„æ–‡ä»¶
    retrieved_docs = []
    # éæ­·å•é¡Œ
    for question in list(questions):
        # é‹è¡Œç®¡é“
        response = new_rag_pipeline.run({
            "multi_language_embedder": {"text": question},
            "prompt_builder": {"question": question},
            "answer_builder": {"query": question}
        })
        # è¼¸å‡º
        print(f"Question: {question}")
        print("Answer from pipeline:")
        print(response["answer_builder"]["answers"][0].data)
        print("\n-----------------------------------\n")

        # è¨˜éŒ„ç­”æ¡ˆ
        rag_answers.append(
            response["answer_builder"]["answers"][0].data
        )
        # ç´€éŒ„æª¢ç´¢åˆ°çš„æ–‡ä»¶
        retrieved_docs.append(
            response["answer_builder"]["answers"][0].documents
        )
    ```

<br>

3. ä»¥ä¸Šä»£ç¢¼æ˜¯ç”¨ä¾†æ¸¬è©¦å’Œé©—è­‰å¤šèªè¨€æ”¯æŒçš„ RAG ç®¡é“ï¼Œéš¨æ©ŸæŠ½å–äº†ä¸€äº›å•é¡Œï¼Œä¸¦ä½¿ç”¨ç®¡é“ç”Ÿæˆç­”æ¡ˆï¼Œç„¶å¾Œæª¢æŸ¥é€™äº›ç­”æ¡ˆçš„æ­£ç¢ºæ€§ï¼Œä»¥ä¸‹ä»¥è¼¸å‡ºçµæœçš„ç¬¬ä¸€å€‹ç‚ºä¾‹ã€‚å…·é«”èªªï¼Œæ¯å€‹å•é¡Œçš„ç­”æ¡ˆéƒ½æ‡‰è©²æ˜¯åŸºæ–¼ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯ç”Ÿæˆçš„ï¼Œä¸¦ä¸”èƒ½å¤ å›ç­”å…·é«”çš„å•é¡Œã€‚

    ```bash
    Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.13it/s]
    # é€™æ˜¯è¼¸å…¥çµ¦ç®¡é“çš„å•é¡Œ
    Question: Do [ EuroSCORE underestimate the mortality risk in cardiac valve surgery of Mexican population ]?
    # è¡¨ç¤ºç®¡é“èƒ½å¤ æ­£ç¢ºåœ°å¾æ–‡æœ¬ä¸­æå–ç›¸é—œä¿¡æ¯ä¸¦ç”Ÿæˆè©³ç´°çš„å›ç­”
    Answer from pipeline:
    Yes, the EuroSCORE does underestimate the mortality risk in cardiac valve surgery of the Mexican population. The data from the study conducted at the Instituto Nacional de CardiologÃ­a Ignacio ChÃ¡vez (INCICh) in MÃ©xico showed that the actual total mortality rate was 9.68%, which was significantly higher than the mortality predicted by the additive (5%) and logistic (5.6%) EuroSCORE models. In addition, the Hosmer-Lemeshow test results had a P<.001 for both models, suggesting that the models did not fit the data well, indicating poor calibration in predicting mortality in this particular population.
    -----------------------------------
    ...çœç•¥
    ```

<br>

## å»ºç«‹è©•ä¼°ç®¡é“

_é›–ç„¶æ¯å€‹è©•ä¼°å™¨éƒ½æ˜¯ `Haystack` ä¸­å¯ä»¥å–®ç¨é‹è¡Œçš„ `çµ„ä»¶`ï¼Œä½†å®ƒå€‘ä¹Ÿå¯ä»¥æ·»åŠ åˆ°ç®¡é“ä¸­ï¼Œé€™æ¨£å¯ä»¥å»ºç«‹ä¸€å€‹åŒ…å«æ‰€æœ‰ `è©•ä¼°æŒ‡æ¨™` çš„ `è©•ä¼°ç®¡é“`ã€‚_

<br>

1. å°å…¥åº«ã€‚

    ```python
    from haystack.components.evaluators.document_mrr import DocumentMRREvaluator
    from haystack.components.evaluators.faithfulness import FaithfulnessEvaluator
    from haystack.components.evaluators.sas_evaluator import SASEvaluator
    ```

<br>

2. å»ºç«‹è©•ä¼°ç®¡é“ã€‚

    ```python
    # å»ºç«‹è©•ä¼°ç®¡é“
    eval_pipeline = Pipeline()
    ```

<br>

3. å»ºç«‹ç®¡é“çµ„ä»¶ï¼šçµ„ä»¶çš„ç¬¬ä¸€å€‹åƒæ•¸æ˜¯ `åç¨±`ï¼Œå°‡åœ¨ç®¡é“ä¸­ä½œç‚º `è­˜åˆ¥ç¬¦`ã€‚

    ```python
    eval_pipeline.add_component(
        "doc_mrr_evaluator",
        DocumentMRREvaluator()
    )
    eval_pipeline.add_component(
        "faithfulness",
        FaithfulnessEvaluator()
    )
    eval_pipeline.add_component(
        "sas_evaluator",
        SASEvaluator(model="sentence-transformers/all-MiniLM-L6-v2")
    )
    ```

<br>

4. é‹è¡Œè©•ä¼°ç®¡é“ã€‚

    ```python
    # é‹è¡Œè©•ä¼°ç®¡é“
    results = eval_pipeline.run({
        "doc_mrr_evaluator": {
            "ground_truth_documents": list([d] for d in ground_truth_docs),
            "retrieved_documents": retrieved_docs
        },
        "faithfulness": {
            "questions": list(questions),
            "contexts": list([d.content] for d in ground_truth_docs),
            "predicted_answers": rag_answers
        },
        "sas_evaluator": {
            "predicted_answers": rag_answers,
            "ground_truth_answers": list(ground_truth_answers)
        }
    })
    ```

<br>

5. è¼¸å‡ºåœ–ç‰‡ã€‚

    ```python
    draw_and_display(eval_pipeline, "eval_pipeline.png")
    ```

    ![](images/img_66.png)

<br>

## å»ºç«‹è©•ä¼°å ±å‘Š

1. é‹è¡Œè©•ä¼°ç®¡é“å¾Œå¯ç”Ÿæˆå®Œæ•´çš„è©•ä¼°å ±å‘Šï¼Œ`Haystack` æä¾›äº†ä¸€å€‹ `EvaluationRunResult` ä¾†é¡¯ç¤ºåˆ†æ•¸å ±å‘Šã€‚

    ```python
    from haystack.evaluation.eval_run_result import EvaluationRunResult

    inputs = {
        "question": list(questions),
        "contexts": list([d.content] for d in ground_truth_docs),
        "answer": list(ground_truth_answers),
        "predicted_answer": rag_answers,
    }

    evaluation_result = EvaluationRunResult(
        run_name="pubmed_rag_pipeline",
        inputs=inputs,
        results=results
    )
    evaluation_result.score_report()
    ```

<br>

2. è¼¸å‡ºåˆ†æ•¸ã€‚

    ```bash
    metrics	score
    0	doc_mrr_evaluator	0.713333
    1	faithfulness	0.880000
    2	sas_evaluator	0.658183
    ```

<br>

## è½‰æ›å ±å‘Šæ ¼å¼

_è½‰æ›ç‚º Pandas DataFrame ä¸¦å„²å­˜ç‚º CSV æ–‡ä»¶_

<br>

1. å¯å°‡å ±å‘Šè½‰æ›ç‚º Pandas DataFrameã€‚

    ```python
    import pandas as pd

    # å°‡è©•ä¼°çµæœè½‰æ›ç‚º DataFrame
    results_df = evaluation_result.to_pandas()
    print(results_df)

    # ä¿å­˜ DataFrame ç‚º CSV æ–‡ä»¶
    results_df.to_csv("evaluation_results.csv", index=False)
    ```

<br>

2. é™¤äº†å„²å­˜ç‚º `.csv` æ–‡ä»¶å¤–ï¼Œä¹Ÿæœƒè¼¸å‡ºçµæœã€‚

    ![](images/img_67.png)

<br>

3. éæ¿¾çµæœã€‚

    ```python
    # ä½¿ç”¨ Pandas éæ¿¾çµæœï¼Œé¡¯ç¤ºèªç¾©ç­”æ¡ˆç›¸ä¼¼æ€§æœ€é«˜çš„ 3 å€‹å’Œæœ€ä½çš„ 3 å€‹
    top_3 = results_df.nlargest(3, 'sas_evaluator')
    bottom_3 = results_df.nsmallest(3, 'sas_evaluator')
    combined_results = pd.concat([top_3, bottom_3])

    # ä¿å­˜éæ¿¾å¾Œçš„çµæœç‚ºå¦ä¸€å€‹ CSV æ–‡ä»¶
    combined_results.to_csv("top_and_bottom_results.csv", index=False)

    # é¡¯ç¤ºéæ¿¾å¾Œçš„çµæœ
    combined_results
    ```

<br>

4. å„²å­˜ä¸¦è¼¸å‡ºçµæœã€‚

    ![](images/img_68.png)

<br>

___

_END_