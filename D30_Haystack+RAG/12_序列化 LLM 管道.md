# åºåˆ—åŒ– LLM ç®¡é“

![](images/img_49.png)

<br>

## èªªæ˜ ğŸ“š

1. é€™æ˜¯å®˜æ–¹åœ¨ `2024/04/25` ç™¼ä½ˆçš„ [å®˜æ–¹ç¯„ä¾‹](https://haystack.deepset.ai/tutorials/29_serializing_pipelines)ã€‚

<br>

2. ä½¿ç”¨çš„çµ„ä»¶åŒ…å« `HuggingFaceLocalGenerator`ã€`PromptBuilder`ã€‚

<br>

3. æœ¬ç¯„ä¾‹ç›®æ¨™æ˜¯åœ¨ `YAML` å’Œ `Python ç¨‹å¼ç¢¼` ä¹‹é–“é€²è¡Œ `åºåˆ—åŒ–` å’Œ `ååºåˆ—åŒ–`ã€‚

<br>

4. `åºåˆ—åŒ–` æ˜¯å°‡ `ç®¡é“` è½‰æ›ç‚ºå¯ä»¥ä¿å­˜åˆ°ç£ç¢Ÿæˆ–å…¶ä»–å„²å­˜ä»‹è³ªçš„æ ¼å¼ï¼Œä»¥ä¾¿ä¹‹å¾Œå¯ä»¥å†æ¬¡è¼‰å…¥å’Œä½¿ç”¨ã€‚

<br>

5. `Haystack` æ”¯æŒå°‡ç®¡é“ç›´æ¥åºåˆ—åŒ–ç‚º `YAML` æ ¼å¼ï¼Œå¦‚æ­¤ä¾¿å¯è¼•é¬†åœ°é€²è¡Œæ›´æ”¹ï¼Œé€™å€‹ç¯„ä¾‹å°‡å»ºç«‹ä¸€å€‹ç°¡å–®çš„ Python ç®¡é“ï¼Œå°‡å…¶åºåˆ—åŒ–ç‚º `YAML`ï¼Œé€²è¡Œä¿®æ”¹å¾Œå†ååºåˆ—åŒ–å› `Haystack` ç®¡é“ã€‚

<br>

## é–‹å§‹

1. å®‰è£å¥—ä»¶ã€‚

    ```bash
    pip install haystack-ai
    ```

<br>

## å»ºç«‹æ—¥èªŒ

1. é€éæ—¥èªŒå¯è§€å¯Ÿç´°éƒ¨éç¨‹èˆ‡è³‡è¨Šã€‚

    ```python
    # æ·»åŠ æ—¥èªŒ
    import logging

    # è¨­ç½®æ—¥èªŒè¼¸å‡ºç´šåˆ¥
    logging.basicConfig(level=logging.INFO)

    # åœ¨å¿…è¦è™•æ·»åŠ å…¶ä»–æ—¥èªŒ
    logging.info("å…¶ä»–è¨Šæ¯ã€‚")
    ```

<br>

## å»ºç«‹ç°¡å–®çš„ç®¡é“

1. å»ºç«‹ä¸€å€‹ç°¡å–®çš„ç®¡é“ï¼Œå…ˆè®“ç”¨æˆ¶è¼¸å…¥ä¸€å€‹ä¸»é¡Œï¼Œç„¶å¾Œç”Ÿæˆé—œæ–¼è©²ä¸»é¡Œçš„æ‘˜è¦ï¼Œé€™è£¡ä½¿ç”¨çš„æ˜¯ `google/flan-t5-large` æ¨¡å‹ï¼Œä¸¦éš¨æ™‚å¯ä¿®æ”¹ç®¡é“ã€‚æ³¨æ„ï¼Œåœ¨é€™å€‹ç®¡é“ä¸­ä½¿ç”¨çš„æ˜¯å¾ `Hugging Face` ç²å–çš„ `æœ¬åœ°æ¨¡å‹`ï¼Œæœƒæ˜¯ä¸€å€‹ç›¸å°è¼ƒå°çš„é–‹æº LLMã€‚

    ```python
    from haystack import Pipeline
    from haystack.components.builders import PromptBuilder
    from haystack.components.generators import HuggingFaceLocalGenerator

    # æ·»åŠ æ—¥èªŒ
    import logging

    # è¨­ç½®æ—¥èªŒè¼¸å‡ºç´šåˆ¥
    logging.basicConfig(level=logging.INFO)

    # å®šç¾©æ¨¡æ¿ï¼Œå°‡ç”¨æˆ¶è¼¸å…¥çš„ä¸»é¡Œæ’å…¥å…¶ä¸­
    # è‹±æ–‡
    template = """
    Please create a summary about the following topic:
    {{ topic }}
    """

    # å»ºç«‹ PromptBuilderï¼Œä½¿ç”¨å®šç¾©çš„æ¨¡æ¿
    builder = PromptBuilder(template=template)
    logging.info("PromptBuilder å·²å»ºç«‹ã€‚")

    # å»ºç«‹ HuggingFaceLocalGeneratorï¼ŒæŒ‡å®šä½¿ç”¨çš„æ¨¡å‹å’Œç”Ÿæˆçš„åƒæ•¸
    logging.info("é–‹å§‹è¼‰å…¥æ¨¡å‹ï¼Œé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ã€‚")
    llm = HuggingFaceLocalGenerator(
        model="google/flan-t5-large",
        task="text2text-generation",
        generation_kwargs={"max_new_tokens": 300, "temperature": 0.7},
    )
    logging.info("HuggingFaceLocalGenerator å·²å»ºç«‹ä¸¦åˆå§‹åŒ–ã€‚")

    # å»ºç«‹ç®¡é“
    pipeline = Pipeline()
    # æ·»åŠ çµ„ä»¶åˆ°ç®¡é“
    pipeline.add_component(name="builder", instance=builder)
    pipeline.add_component(name="llm", instance=llm)

    # é€£æ¥ç®¡é“ä¸­çš„çµ„ä»¶
    pipeline.connect("builder", "llm")
    logging.info("ç®¡é“çµ„ä»¶å·²é€£æ¥ã€‚")

    # å®šç¾©ç”¨æˆ¶è¼¸å…¥çš„ä¸»é¡Œ
    topic = "Climate change"

    # è¼¸å‡ºç”Ÿæˆçš„æ‘˜è¦
    logging.info("æ­£åœ¨é‹è¡Œç®¡é“...")
    # é‹è¡Œç®¡é“ï¼Œç”Ÿæˆé—œæ–¼ä¸»é¡Œçš„æ‘˜è¦
    result = pipeline.run(data={"builder": {"topic": topic}})
    summary = result["llm"]["replies"][0]
    print(f"æŸ¥çœ‹æ•´å€‹ resultï¼š{result}")
    # å¢åŠ æª¢æŸ¥ç”Ÿæˆçš„çµæœ
    if summary.strip() == "":
        logging.warning("ç”Ÿæˆçš„æ‘˜è¦ç‚ºç©ºã€‚")
    else:
        print("ç”Ÿæˆçš„æ‘˜è¦ï¼š", summary)
    ```

<br>

2. åˆæ¬¡ä½¿ç”¨æ™‚ç›¸ç•¶è€—æ™‚ï¼Œå› ç‚ºæ¨¡å‹è¦å¾ Hugging Face çš„ä¼ºæœå™¨ä¸‹è¼‰ï¼Œè¼‰å…¥å®Œæ¨¡å‹å¾Œï¼Œé‚„éœ€è¦ä¸€äº›æ™‚é–“åˆå§‹åŒ–æ¨¡å‹ï¼Œä¸¦å°‡å…¶è¼‰å…¥è¨˜æ†¶é«”ä¸¦é€²è¡Œé…ç½®ã€‚

    ![](images/img_50.png)

<br>

3. æœ€çµ‚å¾—åˆ°çš„çµæœã€‚

    ```bash
    æŸ¥çœ‹æ•´å€‹ resultï¼š{
        'llm': {'replies': [
            'Climate change is a major threat to the planet.'
        ]}
    }
    ç”Ÿæˆçš„æ‘˜è¦ï¼š Climate change is a major threat to the planet.
    ```

<br>

## å°‡ç®¡é“åºåˆ—åŒ–ç‚º YAML

1. `YAML` æ˜¯ä¸€ç¨®äººé¡å¯è®€çš„ `æ•¸æ“šåºåˆ—åŒ–èªè¨€`ï¼Œé€šå¸¸ç”¨æ–¼ `é…ç½®æ–‡ä»¶` å’Œ `æ•¸æ“šäº¤æ›`ï¼Œè€Œ `åºåˆ—åŒ–ç®¡é“ YAML` æ˜¯æŒ‡å°‡ä¸€å€‹è¤‡é›œçš„å°è±¡æˆ–æ•¸æ“šçµæ§‹è½‰æ›ç‚ºä¸€ç¨®å¯ä»¥ä¿å­˜åˆ°æ–‡ä»¶ã€å‚³è¼¸æˆ–å„²å­˜çš„ `æ–‡æœ¬æ ¼å¼`ï¼Œé€™ç¨® `æ–‡æœ¬æ ¼å¼` å¯ä»¥æ˜¯ `YAML`ã€‚

<br>

2. `Haystack` å…§ç½®æ”¯æŒ `YAML`ï¼Œä¸¦ä½¿ç”¨ `dumps()` æ–¹æ³•å°‡ `ç®¡é“` è½‰æ›ç‚º `YAML`ã€‚

    ```python
    # å°‡ç®¡é“åºåˆ—åŒ–ç‚º YAML
    yaml_pipeline = pipeline.dumps()
    print("ç®¡é“çš„ YAML è¡¨ç¤ºï¼š\n", yaml_pipeline)
    ```

<br>

3. æœƒå¾—åˆ°å¦‚ä¸‹æ‰€ç¤ºçš„åºåˆ—åŒ–å¾Œçš„ç®¡é“ `YAML` æ–‡ä»¶ã€‚

    ```yaml
    ç®¡é“çš„ YAML è¡¨ç¤ºï¼š
    components:
        builder:
            init_parameters:
            required_variables: null
            template: '

                Please create a summary about the following topic:

                {{ topic }}

                '
            variables: null
            type: haystack.components.builders.prompt_builder.PromptBuilder
        llm:
            init_parameters:
            generation_kwargs:
                max_new_tokens: 300
                temperature: 0.7
            huggingface_pipeline_kwargs:
                device: mps
                model: google/flan-t5-large
                task: text2text-generation
            stop_words: null
            streaming_callback: null
            token:
                env_vars:
                - HF_API_TOKEN
                strict: false
                type: env_var
            type: haystack.components.generators.hugging_face_local.HuggingFaceLocalGenerator
    connections:
    - receiver: llm.prompt
        sender: builder.prompt
    max_loops_allowed: 100
    metadata: {}
    ```

<br>

## ç·¨è¼¯ YAML ä¸­çš„ç®¡é“

1. ç‰¹åˆ¥æ³¨æ„ï¼Œ `YAML` æ–‡ä»¶å°æ–¼ç¸®æ’æ ¼å¼è¦æ±‚éå¸¸åš´è¬¹ï¼Œä»»ä½•çš„æ ¼å¼éŒ¯èª¤éƒ½å°‡å°è‡´è§£æå¤±æ•—ã€‚

<br>

2. ä¿®æ”¹åºåˆ—åŒ–çš„ç®¡é“æ–‡ä»¶ YAMLï¼Œä¾‹å¦‚ä¿®æ”¹å…¶ä¸­ `promptbuilder` çš„æ¨¡æ¿ï¼Œå°‡æä¾›çš„å¥å­ç¿»è­¯ç‚ºæ³•èªã€‚

    ```yaml
    yaml_pipeline = """
    components:
      builder:
        init_parameters:
          template: "\nPlease translate the following to French: \n{{ sentence }}\n"
        type: haystack.components.builders.prompt_builder.PromptBuilder
      llm:
        init_parameters:
          generation_kwargs:
            max_new_tokens: 150
          huggingface_pipeline_kwargs:
            device: cpu
            model: google/flan-t5-large
            task: text2text-generation
            token: null
          stop_words: null
        type: haystack.components.generators.hugging_face_local.HuggingFaceLocalGenerator
    connections:
    - receiver: llm.prompt
      sender: builder.prompt
    max_loops_allowed: 100
    metadata: {}
    """
    ```

<br>

## å°‡ YAML ç®¡é“ååºåˆ—åŒ–å› Python

1. é€šéèª¿ç”¨ `loads()` æ–¹æ³•å°‡ç®¡é“ååºåˆ—åŒ–ã€‚ä»¥ä¸‹å°‡ååºåˆ—åŒ–ç·¨è¼¯éçš„ `yaml_pipeline`ã€‚

    ```python
    # å°‡ YAML ç®¡é“è¼‰å…¥ç‚º Python ç®¡é“
    logging.info("æ­£åœ¨å¾ YAML è¼‰å…¥æ–°çš„ç®¡é“...")
    new_pipeline = Pipeline.loads(yaml_pipeline)

    # é‹è¡Œæ–°ç®¡é“ï¼Œå°‡å¥å­ç¿»è­¯ç‚ºæ³•èª
    logging.info("æ­£åœ¨é‹è¡Œæ–°çš„ç®¡é“...")
    new_result = new_pipeline.run(data={"builder": {"sentence": "ä½ å¥½ï¼Œè«‹å•ä½ çš„åå­—ï¼Ÿ"}})
    translation = new_result["llm"]["replies"][0]
    print("ç¿»è­¯çµæœï¼š", translation)
    ```

<br>

2. æˆåŠŸç¿»è­¯æœƒå¾—åˆ°ä»¥ä¸‹çµæœã€‚

    ![](images/img_51.png)

<br>

## å¯¦å‹™æ‡‰ç”¨

1. è‡ªå‹•åŒ–å…§å®¹ç”Ÿæˆï¼šåœ¨å¯¦éš›æ¥­å‹™ä¸­ï¼Œé€™ç¨®ç®¡é“å¯ä»¥ç”¨æ–¼è‡ªå‹•ç”Ÿæˆå…§å®¹ï¼Œå¦‚æ–°èæ‘˜è¦ã€ç”¢å“æè¿°ç­‰ã€‚

<br>

2. ç¿»è­¯æœå‹™ï¼šå¯ä»¥é…ç½®ä¸åŒçš„æ¨¡å‹å’Œæ¨¡æ¿ï¼Œå¯¦ç¾å¤šèªç¨®ç¿»è­¯å’Œèªè¨€è™•ç†æœå‹™ã€‚

<br>

3. çŸ¥è­˜ç®¡ç†ï¼šåœ¨ä¼æ¥­çŸ¥è­˜åº«ä¸­ï¼Œåºåˆ—åŒ–çš„ç®¡é“å¯ä»¥ç”¨æ–¼æ–‡ä»¶åˆ†é¡ã€ä¿¡æ¯æŠ½å–ç­‰ä»»å‹™ï¼Œæé«˜çŸ¥è­˜ç®¡ç†æ•ˆç‡ã€‚

<br>

## æ‹“å±•èªªæ˜

1. ä½¿ç”¨æ›´å¼·å¤§çš„æ¨¡å‹ï¼šå¯ä»¥æ›¿æ› `google/flan-t5-large` ç‚ºå…¶ä»–æ›´å¼·å¤§çš„æ¨¡å‹ï¼Œæ ¹æ“šæ¥­å‹™éœ€æ±‚é€²è¡Œèª¿æ•´ã€‚

<br>

2. æ“´å±•åˆ°å¤šæ¨¡æ…‹æ•¸æ“šï¼šä¸åƒ…é™æ–¼æ–‡æœ¬æ•¸æ“šï¼Œå¯ä»¥åŠ å…¥åœ–åƒã€èªéŸ³ç­‰å¤šæ¨¡æ…‹æ•¸æ“šçš„è™•ç†ã€‚

<br>

3. é›†æˆå…¶ä»–æœå‹™ï¼šå¯ä»¥èˆ‡å…¶ä»–æ©Ÿå™¨å­¸ç¿’æˆ–æ·±åº¦å­¸ç¿’æ¡†æ¶é›†æˆï¼Œå¯¦ç¾æ›´è¤‡é›œçš„æ•¸æ“šè™•ç†å’Œåˆ†æã€‚

<br>

## HuggingFaceLocalGenerator

_[å®˜æ–¹æ–‡ä»¶](https://docs.haystack.deepset.ai/docs/huggingfacelocalgenerator)_

<br>

1. å¸¸è¦‹æ–¼ `PromptBuilder` ä¹‹å¾Œï¼Œè¼¸å…¥ `"prompt"` ä½œç‚ºå° LLM çš„æç¤ºå­—ä¸²ï¼Œè¼¸å‡ºçš„ `"replies"` æ˜¯ç”± LLM ç”Ÿæˆçš„å­—ä¸²åˆ—è¡¨ã€‚

<br>

2. `HuggingFaceLocalGenerator` æ˜¯ä¸€å€‹æ¥å£ï¼Œå…è¨±ä½¿ç”¨æœ¬åœ°é‹è¡Œçš„ `Hugging Face` æ¨¡å‹ä¾†ç”Ÿæˆæ–‡æœ¬ï¼Œç•¶ `LLM` åœ¨æœ¬åœ°é‹è¡Œæ™‚ï¼Œå¯èƒ½éœ€è¦å¼·å¤§çš„æ©Ÿå™¨ï¼Œé€™å–æ±ºæ–¼æ‰€é¸æ¨¡å‹åŠå…¶åƒæ•¸æ•¸é‡ã€‚

<br>

3. æ­¤çµ„ä»¶è¨­è¨ˆç”¨æ–¼ `æ–‡æœ¬ç”Ÿæˆ`ï¼Œè€Œä¸æ˜¯ `èŠå¤©`ï¼Œå¦‚æœéœ€è¦ä½¿ç”¨ `Hugging Face LLM` é€²è¡ŒèŠå¤©ï¼Œè«‹ä½¿ç”¨ `HuggingFaceLocalChatGenerator`ã€‚

<br>

4. å°æ–¼é ç¨‹æ–‡ä»¶æˆæ¬Šï¼Œæ­¤çµ„ä»¶é»˜èªä½¿ç”¨ `HF_API_TOKEN` ç’°å¢ƒè®Šæ•¸ã€‚æˆ–è€…ï¼Œå¯ä»¥åœ¨åˆå§‹åŒ–æ™‚å‚³é Hugging Face API ä»¤ç‰Œã€‚

    ```python
    local_generator = HuggingFaceLocalGenerator(
        token=Secret.from_token("<è¼¸å…¥è‡ªå·±çš„ä»¤ç‰Œ>")
    )
    ```

<br>

5. ç¨ç«‹ä½¿ç”¨ã€‚

    ```python
    from haystack.components.generators import HuggingFaceLocalGenerator

    # åˆå§‹åŒ–æœ¬åœ°ç”Ÿæˆå™¨ï¼Œè¨­ç½®æ¨¡å‹å’Œç”Ÿæˆåƒæ•¸
    generator = HuggingFaceLocalGenerator(
        model="google/flan-t5-large",
        task="text2text-generation",
        generation_kwargs={
            "max_new_tokens": 100,
            "temperature": 0.9,
        }
    )

    # é ç†±ç”Ÿæˆå™¨
    generator.warm_up()

    # é‹è¡Œç”Ÿæˆå™¨ï¼Œè¼¸å‡ºçµæœ
    # ç¾åœ‹ç¾ä»»ç¸½çµ±æ˜¯èª°
    _answer= generator.run(
        "Who is the current president of the United States?"
    )
    print(_answer)
    ```

<br>

6. é€™ç­”æ¡ˆä¹Ÿå·®å¤ªå¤šäº†é»ã€‚

    ```python
    {'replies': ['dwight eisenhower']}
    ```

<br>

7. åœ¨ç®¡é“ä¸­ä½¿ç”¨ã€‚

    ```python
    from haystack import Pipeline
    from haystack.components.retrievers.in_memory import InMemoryBM25Retriever
    from haystack.components.builders.prompt_builder import PromptBuilder
    from haystack.components.generators import HuggingFaceLocalGenerator
    from haystack.document_stores.in_memory import InMemoryDocumentStore
    from haystack import Document

    # åˆå§‹åŒ–å…§å­˜æ–‡ä»¶å„²å­˜ï¼Œä¸¦å¯«å…¥æ–‡ä»¶
    docstore = InMemoryDocumentStore()
    docstore.write_documents([
        Document(content="Rome is the capital of Italy"),
        Document(content="Paris is the capital of France")
    ])

    # åˆå§‹åŒ–æœ¬åœ°ç”Ÿæˆå™¨
    generator = HuggingFaceLocalGenerator(
        model="google/flan-t5-large",
        task="text2text-generation",
        generation_kwargs={
            "max_new_tokens": 100,
            "temperature": 0.9,
        }
    )

    # æŸ¥è©¢å•é¡Œ
    query = "What is the capital of France?"

    # å®šç¾©æ¨¡æ¿
    template = """
    Given the following information, answer the question.

    Context:
    {% for document in documents %}
        {{ document.content }}
    {% endfor %}

    Question: {{ query }}?
    """

    # åˆå§‹åŒ–ç®¡é“ï¼Œæ·»åŠ çµ„ä»¶
    pipe = Pipeline()
    pipe.add_component("retriever", InMemoryBM25Retriever(document_store=docstore))
    pipe.add_component("prompt_builder", PromptBuilder(template=template))
    pipe.add_component("llm", generator)

    # é€£æ¥ç®¡é“ä¸­çš„çµ„ä»¶
    pipe.connect("retriever", "prompt_builder.documents")
    pipe.connect("prompt_builder", "llm")

    # é‹è¡Œç®¡é“ï¼Œè¼¸å‡ºçµæœ
    res = pipe.run({
        "prompt_builder": {
            "query": query
        },
        "retriever": {
            "query": query
        }
    })

    print(res)
    ```

<br>

8. é€™ç­”æ¡ˆé‚„å·®ä¸å¤šã€‚

```python
{'llm': {'replies': ['Paris']}}
```

<br>

___

_END_