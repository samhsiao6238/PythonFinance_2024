{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"anakin87/medrag-pubmed-chunk\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for doc in dataset:\n",
    "    docs.append(\n",
    "        Document(\n",
    "            content=doc[\"contents\"],\n",
    "            meta={\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"abstract\": doc[\"content\"],\n",
    "                \"pmid\": doc[\"id\"]\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:174: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v3 of SentenceTransformers.\n",
      "  warnings.warn(\n",
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 481/481 [15:37<00:00,  1.95s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'document_writer': {'documents_written': 15380}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.preprocessors.document_splitter import DocumentSplitter\n",
    "from haystack import Pipeline\n",
    "from haystack.utils import ComponentDevice\n",
    "\n",
    "# 將文件分割為 512 個詞的塊\n",
    "document_splitter = DocumentSplitter(\n",
    "    split_by=\"word\",\n",
    "    split_length=512,\n",
    "    split_overlap=32\n",
    ")\n",
    "\n",
    "# 建立用於密集檢索的文件嵌入\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(\n",
    "    model=\"BAAI/bge-small-en-v1.5\",\n",
    "    # MacOS 不使用 CUDA\n",
    "    # device=ComponentDevice.from_str(\"cuda:0\")\n",
    "    device=ComponentDevice.from_str(\"cpu\")\n",
    ")\n",
    "\n",
    "# 將文件寫入文件儲存\n",
    "document_writer = DocumentWriter(document_store)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"document_splitter\", document_splitter)\n",
    "indexing_pipeline.add_component(\"document_embedder\", document_embedder)\n",
    "indexing_pipeline.add_component(\"document_writer\", document_writer)\n",
    "\n",
    "indexing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
    "indexing_pipeline.connect(\"document_embedder\", \"document_writer\")\n",
    "\n",
    "indexing_pipeline.run({\"document_splitter\": {\"documents\": docs}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(\n",
    "    model=\"BAAI/bge-small-en-v1.5\",\n",
    "    # device=ComponentDevice.from_str(\"cuda:0\")\n",
    "    device=ComponentDevice.from_str(\"cpu\")\n",
    ")\n",
    "embedding_retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "document_joiner = DocumentJoiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.rankers import TransformersSimilarityRanker\n",
    "\n",
    "\n",
    "ranker = TransformersSimilarityRanker(\n",
    "    model=\"BAAI/bge-reranker-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "hybrid_retrieval = Pipeline()\n",
    "hybrid_retrieval.add_component(\"text_embedder\", text_embedder)\n",
    "hybrid_retrieval.add_component(\"embedding_retriever\", embedding_retriever)\n",
    "hybrid_retrieval.add_component(\"bm25_retriever\", bm25_retriever)\n",
    "hybrid_retrieval.add_component(\"document_joiner\", document_joiner)\n",
    "hybrid_retrieval.add_component(\"ranker\", ranker)\n",
    "\n",
    "hybrid_retrieval.connect(\"text_embedder\", \"embedding_retriever\")\n",
    "hybrid_retrieval.connect(\"bm25_retriever\", \"document_joiner\")\n",
    "hybrid_retrieval.connect(\"embedding_retriever\", \"document_joiner\")\n",
    "hybrid_retrieval.connect(\"document_joiner\", \"ranker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retrieval.draw(\"hybrid-retrieval.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"嬰兒呼吸暫停該怎辦？\"\n",
    "query = \"apnea in infants\"\n",
    "\n",
    "result = hybrid_retrieval.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": query},\n",
    "        \"bm25_retriever\": {\"query\": query},\n",
    "        \"ranker\": {\"query\": query},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_results(prediction):\n",
    "    for doc in prediction[\"documents\"]:\n",
    "        print(doc.meta[\"title\"], \"\\t\", doc.score)\n",
    "        print(doc.meta[\"abstract\"])\n",
    "        print(\"\\n\", \"\\n\")\n",
    "        \n",
    "pretty_print_results(result[\"ranker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_results(prediction):\n",
    "    # 檢查是否有文件被檢索到\n",
    "    if not prediction[\"ranker\"][\"documents\"]:\n",
    "        print(\"沒有檢索到相關文件。\")\n",
    "        return\n",
    "\n",
    "    # 美化輸出\n",
    "    for idx, doc in enumerate(prediction[\"ranker\"][\"documents\"]):\n",
    "        print(f\"\\n文件 {idx + 1}:\")\n",
    "        print(f\"標題: {doc.meta.get('title', '無標題')}\")\n",
    "        print(f\"PMID: {doc.meta.get('pmid', '無PMID')}\")\n",
    "        print(f\"分數: {doc.score:.4f}\")\n",
    "        print(f\"摘要: {doc.meta.get('abstract', '無摘要')}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# 美化並輸出結果\n",
    "pretty_print_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文檢索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# 定義分詞函數\n",
    "def tokenize(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "# 對查詢進行分詞\n",
    "query = \"嬰兒呼吸暫停該怎辦？\"\n",
    "tokenized_query = tokenize(query)\n",
    "# 查看分詞後的問題\n",
    "print(tokenized_query)\n",
    "\n",
    "result = hybrid_retrieval.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": tokenized_query},\n",
    "        \"bm25_retriever\": {\"query\": tokenized_query},\n",
    "        \"ranker\": {\"query\": tokenized_query}\n",
    "    }\n",
    ")\n",
    "\n",
    "# 輸出美化版結果\n",
    "pretty_print_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用翻譯庫 `translate` 進行自動翻譯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看一下當前的內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# 打印檢索結果的詳細內容\n",
    "pprint.pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator\n",
    "import jieba\n",
    "import pprint\n",
    "\n",
    "# 初始化翻譯器\n",
    "translator = Translator(to_lang=\"zh\")\n",
    "\n",
    "# 定義分詞函數\n",
    "def tokenize(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "# 對查詢進行分詞\n",
    "query = \"嬰兒呼吸暫停該怎辦？\"\n",
    "tokenized_query = tokenize(query)\n",
    "\n",
    "# 查看分詞後的查詢\n",
    "print(f\"分詞後的查詢: {tokenized_query}\")\n",
    "\n",
    "# 進行檢索\n",
    "result = hybrid_retrieval.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": tokenized_query},\n",
    "        \"bm25_retriever\": {\"query\": tokenized_query},\n",
    "        \"ranker\": {\"query\": tokenized_query}\n",
    "    }\n",
    ")\n",
    "\n",
    "# 檢查結果是否有內容\n",
    "print(\"檢索結果:\")\n",
    "pprint.pprint(result)\n",
    "\n",
    "# 定義翻譯函數\n",
    "def translate_to_chinese(text):\n",
    "    if text:  # 檢查文本是否為空\n",
    "        return translator.translate(text)\n",
    "    else:\n",
    "        return \"無內容\"\n",
    "\n",
    "# 美化輸出結果並進行翻譯\n",
    "def pretty_print_and_translate_results(prediction):\n",
    "    if \"ranker\" not in prediction or not prediction[\"ranker\"]:\n",
    "        print(\"沒有找到相關文件。\")\n",
    "        return\n",
    "\n",
    "    for idx, doc in enumerate(prediction[\"ranker\"][\"documents\"], start=1):\n",
    "        if not doc.content:\n",
    "            print(f\"文件 {idx}: 沒有內容\")\n",
    "            continue\n",
    "\n",
    "        translated_title = translate_to_chinese(doc.meta['title'])\n",
    "        translated_abstract = translate_to_chinese(doc.meta['abstract'])\n",
    "        translated_content = translate_to_chinese(doc.content)\n",
    "\n",
    "        print(f\"文件 {idx}:\")\n",
    "        print(f\"標題: {translated_title}\")\n",
    "        print(f\"摘要: {translated_abstract}\")\n",
    "        print(f\"內容: {translated_content}\")\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 輸出並翻譯結果\n",
    "pretty_print_and_translate_results(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "縮短"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator\n",
    "import jieba\n",
    "\n",
    "# 初始化翻譯器\n",
    "translator = Translator(to_lang=\"zh-tw\")\n",
    "\n",
    "# 定義分詞函數\n",
    "def tokenize(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "# 定義文本截斷函數\n",
    "def truncate_text(text, max_length=400):\n",
    "    if len(text) > max_length:\n",
    "        return text[:max_length] + \"...\"\n",
    "    return text\n",
    "\n",
    "# 定義翻譯函數\n",
    "def translate_to_chinese(text):\n",
    "    return translator.translate(text)\n",
    "\n",
    "# 對查詢進行分詞\n",
    "query = \"嬰兒呼吸暫停急救措施\"\n",
    "tokenized_query = tokenize(query)\n",
    "\n",
    "# 查看分詞後的查詢\n",
    "print(f\"分詞後的查詢: {tokenized_query}\")\n",
    "\n",
    "# 進行檢索\n",
    "result = hybrid_retrieval.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": tokenized_query},\n",
    "        \"bm25_retriever\": {\"query\": tokenized_query},\n",
    "        \"ranker\": {\"query\": tokenized_query}\n",
    "    }\n",
    ")\n",
    "\n",
    "# 檢查結果是否有內容\n",
    "print(\"檢索結果:\")\n",
    "pprint.pprint(result)\n",
    "\n",
    "# 美化輸出結果並進行翻譯\n",
    "def pretty_print_and_translate_results(prediction):\n",
    "    if \"ranker\" not in prediction or not prediction[\"ranker\"][\"documents\"]:\n",
    "        print(\"沒有找到相關文件。\")\n",
    "        return\n",
    "\n",
    "    for idx, doc in enumerate(prediction[\"ranker\"][\"documents\"], start=1):\n",
    "        truncated_title = truncate_text(doc.meta['title'])\n",
    "        truncated_abstract = truncate_text(doc.meta['abstract'])\n",
    "        truncated_content = truncate_text(doc.content)\n",
    "\n",
    "        translated_title = translate_to_chinese(truncated_title)\n",
    "        translated_abstract = translate_to_chinese(truncated_abstract)\n",
    "        translated_content = translate_to_chinese(truncated_content)\n",
    "\n",
    "        print(f\"文件 {idx}:\")\n",
    "        print(f\"標題: {translated_title}\")\n",
    "        print(f\"摘要: {translated_abstract}\")\n",
    "        print(f\"內容: {translated_content}\")\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 輸出並翻譯結果\n",
    "pretty_print_and_translate_results(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envHaystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
