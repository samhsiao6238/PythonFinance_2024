{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¿å­˜ OpenAI API Key ç‚ºç’°å¢ƒè®Šé‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
    "load_dotenv()\n",
    "# å…©å€‹ API çš„å¯†é‘°\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'replies': [ChatMessage(content='è‡ªç„¶èªè¨€è™•ç†ï¼ˆNatural Language Processingï¼Œç¸®å¯«ç‚ºNLPï¼‰æ˜¯ä¸€é–€è·¨é ˜åŸŸçš„ç§‘å­¸æŠ€è¡“ï¼Œä¸»è¦ç ”ç©¶å¦‚ä½•è®“è¨ˆç®—æ©Ÿè‡ªå‹•åœ°ç†è§£å’Œè™•ç†äººé¡èªè¨€çš„æ–¹æ³•å’ŒæŠ€è¡“ã€‚é€™åŒ…æ‹¬å¾èªè¨€å­¸åŸºç¤”åˆ°è¤‡é›œçš„æ¼”ç®—æ³•å’Œæ¨¡å‹ï¼Œç›®çš„åœ¨æ–¼å¯¦ç¾èˆ‡äººé¡èªè¨€ç›¸é—œçš„å„ç¨‹è‡ªå‹•åŒ–ä»»å‹™ï¼Œä¾‹å¦‚èªéŸ³è­˜åˆ¥ã€æ©Ÿå™¨ç¿»è­¯ã€æƒ…æ„Ÿåˆ†æä»¥åŠè‡ªå‹•æ‘˜è¦ç­‰ã€‚', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-4-turbo-2024-04-09', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 174, 'prompt_tokens': 71, 'total_tokens': 245}})]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "# å»ºç«‹ç³»çµ±æ¶ˆæ¯å’Œç”¨æˆ¶æ¶ˆæ¯çš„ ChatMessage å°è±¡\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"å³ä½¿æŸäº›è¼¸å…¥è³‡æ–™æ¡ç”¨å…¶ä»–èªè¨€ï¼Œä¹Ÿå§‹çµ‚ä»¥ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\n",
    "        \"ä»€éº¼æ˜¯è‡ªç„¶èªè¨€è™•ç†ï¼Ÿè¦ç°¡æ½”ã€‚\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# åˆå§‹åŒ– OpenAIChatGenerator\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-4-turbo\")\n",
    "# å‚³å…¥æ¶ˆæ¯ä¸¦é‹è¡Œ\n",
    "chat_generator.run(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è‡ªç„¶èªè¨€è™ ç†ï¼ˆNatural Language Processing, NLPï¼‰æ˜¯è¨ˆç®—æ©Ÿç§‘å­¸ã€äººå·¥æ™ºèƒ½èˆ‡èªè¨€å­¸é ˜åŸŸçš„äº¤å‰å­¸ç§‘ï¼Œæ—¨åœ¨è®“è¨ˆç®—æ©Ÿèƒ½å¤ ç†è§£ã€è§£é‡‹å’Œç”Ÿæˆäººé¡èªè¨€ã€‚é€éNLPï¼Œæ©Ÿå™¨å¯ä»¥åŸ·è¡ŒèªéŸ³è­˜åˆ¥ã€æ©Ÿå™¨ç¿»è­¯ã€æƒ…æ„Ÿåˆ†æç­‰ä»»å‹™ã€‚"
     ]
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators.utils import print_streaming_chunk\n",
    "\n",
    "# ä½¿ç”¨æµå¼å›èª¿å‡½æ•¸åˆå§‹åŒ– OpenAIChatGenerator\n",
    "chat_generator = OpenAIChatGenerator(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    streaming_callback=print_streaming_chunk\n",
    ")\n",
    "# å‚³å…¥æ¶ˆæ¯ä¸¦é‹è¡Œ\n",
    "response = chat_generator.run(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:174: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v3 of SentenceTransformers.\n",
      "  warnings.warn(\n",
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'doc_writer': {'documents_written': 5}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline, Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "\n",
    "# å»ºç«‹æ–‡ä»¶\n",
    "# documents = [\n",
    "#     Document(content=\"æˆ‘çš„åå­—æ˜¯ Jeanï¼Œæˆ‘ä½åœ¨ Parisã€‚\"),\n",
    "#     Document(content=\"æˆ‘çš„åå­—æ˜¯ Markï¼Œæˆ‘ä½åœ¨ Berlinã€‚\"),\n",
    "#     Document(content=\"æˆ‘çš„åå­—æ˜¯ Giorgioï¼Œæˆ‘ä½åœ¨ Romeã€‚\"),\n",
    "#     Document(content=\"æˆ‘çš„åå­—æ˜¯ Martaï¼Œæˆ‘ä½åœ¨ Madridã€‚\"),\n",
    "#     Document(content=\"æˆ‘çš„åå­—æ˜¯ Harryï¼Œæˆ‘ä½åœ¨ Londonã€‚\"),\n",
    "# ]\n",
    "documents = [\n",
    "    Document(content=\"My name is Jean and I live in Paris.\"),\n",
    "    Document(content=\"My name is Mark and I live in Berlin.\"),\n",
    "    Document(content=\"My name is Giorgio and I live in Rome.\"),\n",
    "    Document(content=\"My name is Marta and I live in Madrid.\"),\n",
    "    Document(content=\"My name is Harry and I live in London.\"),\n",
    "]\n",
    "# åˆå§‹åŒ–å…§å­˜æ–‡ä»¶å„²å­˜\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# å»ºç«‹ç´¢å¼•ç®¡é“\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\n",
    "    instance=SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"), name=\"doc_embedder\"\n",
    ")\n",
    "indexing_pipeline.add_component(\n",
    "    instance=DocumentWriter(document_store=document_store),\n",
    "    name=\"doc_writer\"\n",
    ")\n",
    "\n",
    "# é€£æ¥åµŒå…¥å™¨å’Œæ–‡ä»¶å¯«å…¥å™¨\n",
    "indexing_pipeline.connect(\n",
    "    \"doc_embedder.documents\",\n",
    "    \"doc_writer.documents\"\n",
    ")\n",
    "\n",
    "# é‹è¡Œç®¡é“\n",
    "indexing_pipeline.run({\n",
    "    \"doc_embedder\": {\"documents\": documents}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x158c079d0>\n",
       "ğŸš… Components\n",
       "  - embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ğŸ›¤ï¸ Connections\n",
       "  - embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "# å®šç¾©æç¤ºæ¨¡æ¿\n",
    "template = \"\"\"\n",
    "æ ¹æ“šçµ¦å®šçš„ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚\n",
    "\n",
    "ä¸Šä¸‹æ–‡:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "å•é¡Œ: {{ question }}\n",
    "ç­”æ¡ˆ:\n",
    "\"\"\"\n",
    "\n",
    "# å»ºç«‹ RAG ç®¡é“\n",
    "rag_pipe = Pipeline()\n",
    "rag_pipe.add_component(\n",
    "    \"embedder\",\n",
    "    SentenceTransformersTextEmbedder(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    ")\n",
    "rag_pipe.add_component(\n",
    "    \"retriever\",\n",
    "    InMemoryEmbeddingRetriever(\n",
    "        document_store=document_store\n",
    "    )\n",
    ")\n",
    "rag_pipe.add_component(\n",
    "    \"prompt_builder\",\n",
    "    PromptBuilder(\n",
    "        template=template\n",
    "    )\n",
    ")\n",
    "rag_pipe.add_component(\n",
    "    \"llm\",\n",
    "    OpenAIGenerator(model=\"gpt-4-turbo\")\n",
    ")\n",
    "\n",
    "# é€£æ¥çµ„ä»¶\n",
    "rag_pipe.connect(\n",
    "    \"embedder.embedding\",\n",
    "    \"retriever.query_embedding\"\n",
    ")\n",
    "rag_pipe.connect(\n",
    "    \"retriever\",\n",
    "    \"prompt_builder.documents\"\n",
    ")\n",
    "rag_pipe.connect(\n",
    "    \"prompt_builder\",\n",
    "    \"llm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'replies': ['Mark ä½åœ¨ Berlin.'],\n",
       "  'meta': [{'model': 'gpt-4-turbo-2024-04-09',\n",
       "    'index': 0,\n",
       "    'finish_reason': 'stop',\n",
       "    'usage': {'completion_tokens': 7,\n",
       "     'prompt_tokens': 107,\n",
       "     'total_tokens': 114}}]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Mark ä½åœ¨å“ªè£¡ï¼Ÿ\"\n",
    "rag_pipe.run({\n",
    "    \"embedder\": {\"text\": query},\n",
    "    \"prompt_builder\": {\"question\": query}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_func(query: str):\n",
    "    result = rag_pipe.run({\n",
    "        \"embedder\": {\"text\": query},\n",
    "        \"prompt_builder\": {\"question\": query}\n",
    "    })\n",
    "    return {\"reply\": result[\"llm\"][\"replies\"][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_INFO = {\n",
    "    \"Berlin\": {\n",
    "        \"weather\": \"mostly sunny\", \"temperature\": 7, \"unit\": \"celsius\"\n",
    "    },\n",
    "    \"Paris\": {\n",
    "        \"weather\": \"mostly cloudy\", \"temperature\": 8, \"unit\": \"celsius\"\n",
    "    },\n",
    "    \"Rome\": {\n",
    "        \"weather\": \"sunny\", \"temperature\": 14, \"unit\": \"celsius\"\n",
    "    },\n",
    "    \"Madrid\": {\n",
    "        \"weather\": \"sunny\", \"temperature\": 10, \"unit\": \"celsius\"\n",
    "    },\n",
    "    \"London\": {\n",
    "        \"weather\": \"cloudy\", \"temperature\": 9, \"unit\": \"celsius\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_current_weather(location: str):\n",
    "    if location in WEATHER_INFO:\n",
    "        return WEATHER_INFO[location]\n",
    "    else:\n",
    "        # å›é€€æ•¸æ“š\n",
    "        return {\n",
    "            \"weather\": \"ç‹‚é¢¨æš´é›¨\",\n",
    "            \"temperature\": 99.9,\n",
    "            \"unit\": \"fahrenheit\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag_pipeline_func\",\n",
    "            \"description\": \"ç²å–æœ‰é—œäººå€‘å±…ä½åœ°é»çš„ä¿¡æ¯\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"æœå°‹ä¸­ä½¿ç”¨çš„æŸ¥è©¢ã€‚å¾ç”¨æˆ¶çš„æ¶ˆæ¯ä¸­æ¨æ–·å‡ºé€™ä¸€é»ã€‚å®ƒæ‡‰è©²æ˜¯ä¸€å€‹å•é¡Œæˆ–ä¸€å€‹é™³è¿°ã€‚\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"å–å¾—ç•¶å‰å¤©æ°£\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚åŠ å·èˆŠé‡‘å±±\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'replies': [ChatMessage(content='[{\"index\": 0, \"id\": \"call_PK1299vodTJ4uFSyAUVXp741\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\":\\\\\"Where does Mark live?\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-4-turbo-2024-04-09', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {}})]}\n"
     ]
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators.utils import print_streaming_chunk\n",
    "\n",
    "# å»ºç«‹æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç³»çµ±æ¶ˆæ¯å’Œç”¨æˆ¶æŸ¥è©¢\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"ä¸è¦å‡è¨­å°‡å“ªäº›å€¼æ’å…¥å‡½æ•¸ä¸­ã€‚å¦‚æœç”¨æˆ¶è¦æ±‚ä¸æ˜ç¢ºï¼Œè«‹è¦æ±‚æ¾„æ¸…ã€‚\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\"ä½ èƒ½å‘Šè¨´æˆ‘ Mark ä½åœ¨å“ªè£¡å—ï¼Ÿ\"),\n",
    "]\n",
    "\n",
    "# åˆå§‹åŒ– OpenAIChatGenerator\n",
    "chat_generator = OpenAIChatGenerator(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    streaming_callback=print_streaming_chunk\n",
    ")\n",
    "# å‚³å…¥æ¶ˆæ¯å’Œå·¥å…·åˆ—è¡¨ä¸¦é‹è¡Œ\n",
    "response = chat_generator.run(\n",
    "    messages=messages,\n",
    "    generation_kwargs={\"tools\": tools}\n",
    ")\n",
    "# è¼¸å‡ºæŸ¥çœ‹\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Name: rag_pipeline_func\n",
      "Function Arguments: {'query': 'Where does Mark live?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Response: {'reply': 'Berlin'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# è§£æå‡½æ•¸èª¿ç”¨ä¿¡æ¯\n",
    "# æå–ç¬¬ä¸€å€‹å›æ‡‰ä¸­çš„ content\n",
    "content = response['replies'][0].content\n",
    "\n",
    "# å°‡ content è§£æç‚º JSON\n",
    "# content æ˜¯ä¸€å€‹ JSON å­—ä¸²ï¼Œéœ€è¦è½‰æ›ç‚º Python å­—å…¸\n",
    "function_calls = json.loads(content)\n",
    "\n",
    "# æå–ç¬¬ä¸€å€‹å‡½æ•¸èª¿ç”¨ä¿¡æ¯\n",
    "# æå–å‡½æ•¸èª¿ç”¨åˆ—è¡¨ä¸­çš„ç¬¬ä¸€å€‹å…ƒç´ \n",
    "function_call = function_calls[0]\n",
    "\n",
    "# ç²å–å‡½æ•¸åç¨±\n",
    "# ç²å–å‡½æ•¸åç¨±ï¼Œé€™æ˜¯æˆ‘å€‘éœ€è¦èª¿ç”¨çš„å‡½æ•¸\n",
    "function_name = function_call['function']['name']\n",
    "\n",
    "# è§£æå‡½æ•¸åƒæ•¸\n",
    "# å°‡åƒæ•¸è§£æç‚ºå­—å…¸æ ¼å¼\n",
    "function_args = json.loads(function_call['function']['arguments'])\n",
    "\n",
    "# æ‰“å°å‡½æ•¸åç¨±å’Œåƒæ•¸\n",
    "print(\"Function Name:\", function_name)\n",
    "print(\"Function Arguments:\", function_args)\n",
    "\n",
    "\n",
    "# å¯ç”¨å‡½æ•¸å­—å…¸\n",
    "available_functions = {\n",
    "    \"rag_pipeline_func\": rag_pipeline_func,\n",
    "    \"get_current_weather\": get_current_weather\n",
    "}\n",
    "\n",
    "# æœå°‹ç›¸æ‡‰çš„å‡½æ•¸ä¸¦ä½¿ç”¨çµ¦å®šçš„åƒæ•¸èª¿ç”¨å®ƒ\n",
    "if function_name in available_functions:\n",
    "    # æ ¹æ“šå‡½æ•¸åç¨±æ‰¾åˆ°å°æ‡‰çš„å‡½æ•¸\n",
    "    function_to_call = available_functions[function_name]\n",
    "    # ä½¿ç”¨è§£åŒ…æ“ä½œå°‡åƒæ•¸å‚³éçµ¦å‡½æ•¸\n",
    "    function_response = function_to_call(**function_args)\n",
    "    # æ‰“å°å‡½æ•¸çš„è¿”å›å€¼\n",
    "    print(\"Function Response:\", function_response)\n",
    "else:\n",
    "    # å¦‚æœå‡½æ•¸åç¨±æœªæ‰¾åˆ°ï¼Œæ‰“å°éŒ¯èª¤è¨Šæ¯\n",
    "    print(f\"Function {function_name} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark ä½åœ¨æŸæ—ã€‚"
     ]
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "# å»ºç«‹å‡½æ•¸å›æ‡‰æ¶ˆæ¯\n",
    "function_message = ChatMessage.from_function(\n",
    "    content=json.dumps(function_response),\n",
    "    name=function_name\n",
    ")\n",
    "# å°‡å‡½æ•¸å›æ‡‰æ¶ˆæ¯æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨\n",
    "messages.append(function_message)\n",
    "\n",
    "# å†æ¬¡é‹è¡Œ OpenAIChatGenerator\n",
    "response = chat_generator.run(\n",
    "    messages=messages,\n",
    "    generation_kwargs={\"tools\": tools}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-4-turbo\")\n",
    "response = None\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"ä¸è¦å‡è¨­å°‡å“ªäº›å€¼æ’å…¥å‡½æ•¸ä¸­ã€‚\"\n",
    "        \"å¦‚æœç”¨æˆ¶è¦æ±‚ä¸æ˜ç¢ºï¼Œè«‹è¦æ±‚æ¾„æ¸…ã€‚\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# å®šç¾©èŠå¤©æ©Ÿå™¨äººå‡½æ•¸\n",
    "def chatbot_with_fc(message, history):\n",
    "    messages.append(ChatMessage.from_user(message))\n",
    "    response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "    while True:\n",
    "        # å¦‚æœ OpenAI å›æ‡‰æ˜¯ä¸€å€‹å·¥å…·èª¿ç”¨\n",
    "        if response and response[\"replies\"][0].meta[\"finish_reason\"] == \"tool_calls\":\n",
    "            function_calls = json.loads(response[\"replies\"][0].content)\n",
    "            print(response[\"replies\"][0])\n",
    "            for function_call in function_calls:\n",
    "                # è§£æå‡½æ•¸èª¿ç”¨ä¿¡æ¯\n",
    "                function_name = function_call[\"function\"][\"name\"]\n",
    "                function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
    "\n",
    "                # æœå°‹ç›¸æ‡‰çš„å‡½æ•¸ä¸¦ä½¿ç”¨çµ¦å®šçš„åƒæ•¸èª¿ç”¨å®ƒ\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_response = function_to_call(function_args)\n",
    "\n",
    "                # ä½¿ç”¨ `ChatMessage.from_function` å°‡å‡½æ•¸å›æ‡‰æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨\n",
    "                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n",
    "                response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "        # å¸¸è¦å°è©±\n",
    "        else:\n",
    "            messages.append(response[\"replies\"][0])\n",
    "            break\n",
    "    return response[\"replies\"][0].content\n",
    "\n",
    "# å»ºç«‹èŠå¤©ç•Œé¢\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot_with_fc,\n",
    "    # é¡¯ç¤ºåœ¨ä¸‹æ–¹çš„ç¯„ä¾‹æ¬„ä½\n",
    "    examples=[\n",
    "        \"ä½ èƒ½å‘Šè¨´æˆ‘ Giorgio ä½åœ¨å“ªè£¡å—ï¼Ÿ\",\n",
    "        \"Madrid çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ\",\n",
    "        \"èª°ä½åœ¨ London?\",\n",
    "        \"Mark ä½çš„åœ°æ–¹çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ\",\n",
    "    ],\n",
    "    title=\"è«‹è©¢å•æœ‰é—œå¤©æ°£æˆ–äººå€‘å±…ä½çš„åœ°æ–¹ã€‚\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å•Ÿå‹•èŠå¤©æ‡‰ç”¨ç¨‹åº\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envHaystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
