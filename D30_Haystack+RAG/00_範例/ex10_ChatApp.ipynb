{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¿å­˜ OpenAI API Key ç‚ºç’°å¢ƒè®Šé‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
    "load_dotenv()\n",
    "# å…©å€‹ API çš„å¯†é‘°\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'replies': [ChatMessage(content='è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯ä¸€å€‘è³‡è¨Šç§‘æŠ€é ˜åŸŸï¼Œè‡´åŠ›æ–¼è®“è¨ˆç®—æ©Ÿèƒ½å¤ ç†è§£ã€è§£é‡‹ã€æ“ä½œå’Œç”Ÿæˆäººé¡èªè¨€ã€‚è¿™é¡¹æŠ€æœ¯æ¶‰åŠèªè¨€å­¸ã€è¨ˆç®—æ©Ÿç§‘å­¸å’Œäººå·¥æ™ºæ…§çš„äº¤å‰ç™¼å±•ã€‚', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-4-turbo-2024-04-09', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 96, 'prompt_tokens': 67, 'total_tokens': 163}})]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "# å‰µå»ºç³»çµ±æ¶ˆæ¯å’Œç”¨æˆ¶æ¶ˆæ¯çš„ ChatMessage å°è±¡\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"å³ä½¿æŸäº›è¼¸å…¥è³‡æ–™æ¡ç”¨å…¶ä»–èªè¨€ï¼Œä¹Ÿå§‹çµ‚ä»¥ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\n",
    "        \"ä»€éº¼æ˜¯è‡ªç„¶èªè¨€è™•ç†ï¼Ÿè¦ç°¡æ½”ã€‚\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# åˆå§‹åŒ– OpenAIChatGenerator\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-4-turbo\")\n",
    "# å‚³å…¥æ¶ˆæ¯ä¸¦é‹è¡Œ\n",
    "chat_generator.run(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è‡ªç„¶èªè¨€è™•ç†æ˜¯äººå·¥æ™®éå’Œèªè¨€å­¸é ˜åŸŸçš„ä¸€éƒ¨åˆ†ï¼Œå®ƒå¹«åŠ©è¨ˆç®—æ©Ÿç†è§£ã€è§£é‡‹ã€æ“ä½œå’Œç”Ÿæˆäººé¡èªè¨€ã€‚"
     ]
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators.utils import print_streaming_chunk\n",
    "\n",
    "# ä½¿ç”¨æµå¼å›èª¿å‡½æ•¸åˆå§‹åŒ– OpenAIChatGenerator\n",
    "chat_generator = OpenAIChatGenerator(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    streaming_callback=print_streaming_chunk\n",
    ")\n",
    "# å‚³å…¥æ¶ˆæ¯ä¸¦é‹è¡Œ\n",
    "response = chat_generator.run(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:174: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v3 of SentenceTransformers.\n",
      "  warnings.warn(\n",
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  2.00s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'doc_writer': {'documents_written': 5}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline, Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "\n",
    "# å‰µå»ºæ–‡ä»¶\n",
    "documents = [\n",
    "    Document(content=\"æˆ‘çš„åå­—æ˜¯ Jeanï¼Œæˆ‘ä½åœ¨ Parisã€‚\"),\n",
    "    Document(content=\"æˆ‘çš„åå­—æ˜¯ Markï¼Œæˆ‘ä½åœ¨ Berlinã€‚\"),\n",
    "    Document(content=\"æˆ‘çš„åå­—æ˜¯ Giorgioï¼Œæˆ‘ä½åœ¨ Romeã€‚\"),\n",
    "    Document(content=\"æˆ‘çš„åå­—æ˜¯ Martaï¼Œæˆ‘ä½åœ¨ Madridã€‚\"),\n",
    "    Document(content=\"æˆ‘çš„åå­—æ˜¯ Harryï¼Œæˆ‘ä½åœ¨ Londonã€‚\"),\n",
    "]\n",
    "\n",
    "# åˆå§‹åŒ–å…§å­˜æ–‡ä»¶å„²å­˜\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# å‰µå»ºç´¢å¼•ç®¡é“\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\n",
    "    instance=SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"), name=\"doc_embedder\"\n",
    ")\n",
    "indexing_pipeline.add_component(\n",
    "    instance=DocumentWriter(document_store=document_store),\n",
    "    name=\"doc_writer\"\n",
    ")\n",
    "\n",
    "# é€£æ¥åµŒå…¥å™¨å’Œæ–‡ä»¶å¯«å…¥å™¨\n",
    "indexing_pipeline.connect(\n",
    "    \"doc_embedder.documents\",\n",
    "    \"doc_writer.documents\"\n",
    ")\n",
    "\n",
    "# é‹è¡Œç®¡é“\n",
    "indexing_pipeline.run({\n",
    "    \"doc_embedder\": {\"documents\": documents}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x325298a30>\n",
       "ğŸš… Components\n",
       "  - embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ğŸ›¤ï¸ Connections\n",
       "  - embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "# å®šç¾©æç¤ºæ¨¡æ¿\n",
    "template = \"\"\"\n",
    "æ ¹æ“šçµ¦å®šçš„ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚\n",
    "\n",
    "ä¸Šä¸‹æ–‡:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "å•é¡Œ: {{ question }}\n",
    "ç­”æ¡ˆ:\n",
    "\"\"\"\n",
    "\n",
    "# å‰µå»º RAG ç®¡é“\n",
    "rag_pipe = Pipeline()\n",
    "rag_pipe.add_component(\n",
    "    \"embedder\",\n",
    "    SentenceTransformersTextEmbedder(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    ")\n",
    "rag_pipe.add_component(\n",
    "    \"retriever\",\n",
    "    InMemoryEmbeddingRetriever(\n",
    "        document_store=document_store\n",
    "    )\n",
    ")\n",
    "rag_pipe.add_component(\n",
    "    \"prompt_builder\",\n",
    "    PromptBuilder(\n",
    "        template=template\n",
    "    )\n",
    ")\n",
    "rag_pipe.add_component(\n",
    "    \"llm\",\n",
    "    OpenAIGenerator(model=\"gpt-4-turbo\")\n",
    ")\n",
    "\n",
    "# é€£æ¥çµ„ä»¶\n",
    "rag_pipe.connect(\n",
    "    \"embedder.embedding\",\n",
    "    \"retriever.query_embedding\"\n",
    ")\n",
    "rag_pipe.connect(\n",
    "    \"retriever\",\n",
    "    \"prompt_builder.documents\"\n",
    ")\n",
    "rag_pipe.connect(\n",
    "    \"prompt_builder\",\n",
    "    \"llm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'replies': ['Mark ä½åœ¨ Berlinã€‚'],\n",
       "  'meta': [{'model': 'gpt-4-turbo-2024-04-09',\n",
       "    'index': 0,\n",
       "    'finish_reason': 'stop',\n",
       "    'usage': {'completion_tokens': 7,\n",
       "     'prompt_tokens': 127,\n",
       "     'total_tokens': 134}}]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Mark ä½åœ¨å“ªè£¡ï¼Ÿ\"\n",
    "rag_pipe.run({\n",
    "    \"embedder\": {\"text\": query},\n",
    "    \"prompt_builder\": {\"question\": query}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_func(query: str):\n",
    "    result = rag_pipe.run({\n",
    "        \"embedder\": {\"text\": query},\n",
    "        \"prompt_builder\": {\"question\": query}\n",
    "    })\n",
    "    return {\"reply\": result[\"llm\"][\"replies\"][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_INFO = {\n",
    "    \"Berlin\": {\n",
    "        \"weather\": \"mostly sunny\", \"temperature\": 7, \"unit\": \"celsius\"\n",
    "    },\n",
    "    \"Paris\": {\n",
    "        \"weather\": \"mostly cloudy\", \"temperature\": 8, \"unit\": \"celsius\"\n",
    "    },\n",
    "    \"Rome\": {\n",
    "        \"weather\": \"sunny\", \"temperature\": 14, \"unit\": \"celsius\"\n",
    "    },\n",
    "    \"Madrid\": {\n",
    "        \"weather\": \"sunny\", \"temperature\": 10, \"unit\": \"celsius\"\n",
    "    },\n",
    "    \"London\": {\n",
    "        \"weather\": \"cloudy\", \"temperature\": 9, \"unit\": \"celsius\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_current_weather(location: str):\n",
    "    if location in WEATHER_INFO:\n",
    "        return WEATHER_INFO[location]\n",
    "    else:\n",
    "        # å›é€€æ•¸æ“š\n",
    "        return {\n",
    "            \"weather\": \"sunny\",\n",
    "            \"temperature\": 21.8,\n",
    "            \"unit\": \"fahrenheit\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag_pipeline_func\",\n",
    "            \"description\": \"ç²å–æœ‰é—œäººå€‘å±…ä½åœ°é»çš„ä¿¡æ¯\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"æœå°‹ä¸­ä½¿ç”¨çš„æŸ¥è©¢ã€‚å¾ç”¨æˆ¶çš„æ¶ˆæ¯ä¸­æ¨æ–·å‡ºé€™ä¸€é»ã€‚å®ƒæ‡‰è©²æ˜¯ä¸€å€‹å•é¡Œæˆ–ä¸€å€‹é™³è¿°ã€‚\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"å–å¾—ç•¶å‰å¤©æ°£\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚åŠ å·èˆŠé‡‘å±±\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•æä¾›é—œæ–¼ç‰¹å®šäººç‰©å¦‚ Tom çš„ç§äººä½å€ä¿¡æ¯ï¼Œé™¤éæ‚¨èƒ½æä¾›æ›´å¤šçš„èƒŒæ™¯è³‡æ–™æˆ–æ˜ç¢ºçš„å…¬é–‹è³‡è¨Šéœ€æ±‚ã€‚å¦‚æœæœ‰å…¶ä»–å•é¡Œæˆ–éœ€è¦å¹«åŠ©ï¼Œè«‹å‘Šè¨´æˆ‘ï¼{'replies': [ChatMessage(content='æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•æä¾›é—œæ–¼ç‰¹å®šäººç‰©å¦‚ Tom çš„ç§äººä½å€ä¿¡æ¯ï¼Œé™¤éæ‚¨èƒ½æä¾›æ›´å¤šçš„èƒŒæ™¯è³‡æ–™æˆ–æ˜ç¢ºçš„å…¬é–‹è³‡è¨Šéœ€æ±‚ã€‚å¦‚æœæœ‰å…¶ä»–å•é¡Œæˆ–éœ€è¦å¹«åŠ©ï¼Œè«‹å‘Šè¨´æˆ‘ï¼', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-4-turbo-2024-04-09', 'index': 0, 'finish_reason': 'stop', 'usage': {}})]}\n"
     ]
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators.utils import print_streaming_chunk\n",
    "\n",
    "# å‰µå»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç³»çµ±æ¶ˆæ¯å’Œç”¨æˆ¶æŸ¥è©¢\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"ä¸è¦å‡è¨­å°‡å“ªäº›å€¼æ’å…¥å‡½æ•¸ä¸­ã€‚å¦‚æœç”¨æˆ¶è¦æ±‚ä¸æ˜ç¢ºï¼Œè«‹è¦æ±‚æ¾„æ¸…ã€‚\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\"ä½ èƒ½å‘Šè¨´æˆ‘ Tom ä½åœ¨å“ªè£¡å—ï¼Ÿ\"),\n",
    "]\n",
    "\n",
    "# åˆå§‹åŒ– OpenAIChatGenerator\n",
    "chat_generator = OpenAIChatGenerator(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    streaming_callback=print_streaming_chunk\n",
    ")\n",
    "# å‚³å…¥æ¶ˆæ¯å’Œå·¥å…·åˆ—è¡¨ä¸¦é‹è¡Œ\n",
    "response = chat_generator.run(\n",
    "    messages=messages,\n",
    "    generation_kwargs={\"tools\": tools}\n",
    ")\n",
    "# è¼¸å‡ºæŸ¥çœ‹\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m content \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplies\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# å°‡ content è§£æç‚º JSON\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# content æ˜¯ä¸€å€‹ JSON å­—ä¸²ï¼Œéœ€è¦è½‰æ›ç‚º Python å­—å…¸\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m function_calls \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# æå–ç¬¬ä¸€å€‹å‡½æ•¸èª¿ç”¨ä¿¡æ¯\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# æå–å‡½æ•¸èª¿ç”¨åˆ—è¡¨ä¸­çš„ç¬¬ä¸€å€‹å…ƒç´ \u001b[39;00m\n\u001b[1;32m     13\u001b[0m function_call \u001b[38;5;241m=\u001b[39m function_calls[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# è§£æå‡½æ•¸èª¿ç”¨ä¿¡æ¯\n",
    "# æå–ç¬¬ä¸€å€‹å›æ‡‰ä¸­çš„ content\n",
    "content = response['replies'][0].content\n",
    "\n",
    "# å°‡ content è§£æç‚º JSON\n",
    "# content æ˜¯ä¸€å€‹ JSON å­—ä¸²ï¼Œéœ€è¦è½‰æ›ç‚º Python å­—å…¸\n",
    "function_calls = json.loads(content)\n",
    "\n",
    "# æå–ç¬¬ä¸€å€‹å‡½æ•¸èª¿ç”¨ä¿¡æ¯\n",
    "# æå–å‡½æ•¸èª¿ç”¨åˆ—è¡¨ä¸­çš„ç¬¬ä¸€å€‹å…ƒç´ \n",
    "function_call = function_calls[0]\n",
    "\n",
    "# ç²å–å‡½æ•¸åç¨±\n",
    "# ç²å–å‡½æ•¸åç¨±ï¼Œé€™æ˜¯æˆ‘å€‘éœ€è¦èª¿ç”¨çš„å‡½æ•¸\n",
    "function_name = function_call['function']['name']\n",
    "\n",
    "# è§£æå‡½æ•¸åƒæ•¸\n",
    "# å°‡åƒæ•¸è§£æç‚ºå­—å…¸æ ¼å¼\n",
    "function_args = json.loads(function_call['function']['arguments'])\n",
    "\n",
    "# æ‰“å°å‡½æ•¸åç¨±å’Œåƒæ•¸\n",
    "print(\"Function Name:\", function_name)\n",
    "print(\"Function Arguments:\", function_args)\n",
    "\n",
    "# å®šç¾©å¯ç”¨çš„å‡½æ•¸\n",
    "def rag_pipeline_func(query: str):\n",
    "    # é€™è£¡å‡è¨­ä½ çš„ `rag_pipeline_func` å‡½æ•¸å®šç¾©\n",
    "    return {\"reply\": f\"Mark lives in Berlin, query was: {query}\"}\n",
    "\n",
    "def get_current_weather(location: str):\n",
    "    # é€™è£¡å‡è¨­ä½ çš„ `get_current_weather` å‡½æ•¸å®šç¾©\n",
    "    return {\"weather\": \"sunny\", \"temperature\": 20, \"location\": location}\n",
    "\n",
    "# å¯ç”¨å‡½æ•¸å­—å…¸\n",
    "available_functions = {\n",
    "    \"rag_pipeline_func\": rag_pipeline_func,\n",
    "    \"get_current_weather\": get_current_weather\n",
    "}\n",
    "\n",
    "# æŸ¥æ‰¾ç›¸æ‡‰çš„å‡½æ•¸ä¸¦ä½¿ç”¨çµ¦å®šçš„åƒæ•¸èª¿ç”¨å®ƒ\n",
    "if function_name in available_functions:\n",
    "    # æ ¹æ“šå‡½æ•¸åç¨±æ‰¾åˆ°å°æ‡‰çš„å‡½æ•¸\n",
    "    function_to_call = available_functions[function_name]\n",
    "    # ä½¿ç”¨è§£åŒ…æ“ä½œå°‡åƒæ•¸å‚³éçµ¦å‡½æ•¸\n",
    "    function_response = function_to_call(**function_args)\n",
    "    # æ‰“å°å‡½æ•¸çš„è¿”å›å€¼\n",
    "    print(\"Function Response:\", function_response)\n",
    "else:\n",
    "    # å¦‚æœå‡½æ•¸åç¨±æœªæ‰¾åˆ°ï¼Œæ‰“å°éŒ¯èª¤è¨Šæ¯\n",
    "    print(f\"Function {function_name} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°ä¸èµ·ï¼Œæˆ‘æ²’è¾¦æ³•æä¾›æœ‰é—œ Tom çš„å…·é«”å±…ä½åœ°é»ã€‚æ‚¨éœ€è¦è©³ç´°ä¿¡æ¯æˆ–æœ‰å…¶ä»–å•é¡Œå—ï¼Ÿ"
     ]
    }
   ],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "# å‰µå»ºå‡½æ•¸å›æ‡‰æ¶ˆæ¯\n",
    "function_message = ChatMessage.from_function(\n",
    "    content=json.dumps(function_response),\n",
    "    name=function_name\n",
    ")\n",
    "# å°‡å‡½æ•¸å›æ‡‰æ¶ˆæ¯æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨\n",
    "messages.append(function_message)\n",
    "\n",
    "# å†æ¬¡é‹è¡Œ OpenAIChatGenerator\n",
    "response = chat_generator.run(\n",
    "    messages=messages,\n",
    "    generation_kwargs={\"tools\": tools}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\")\n",
    "response = None\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"ä¸è¦å‡è¨­å°‡å“ªäº›å€¼æ’å…¥å‡½æ•¸ä¸­ã€‚\"\n",
    "        \"å¦‚æœç”¨æˆ¶è¦æ±‚ä¸æ˜ç¢ºï¼Œè«‹è¦æ±‚æ¾„æ¸…ã€‚\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# å®šç¾©èŠå¤©æ©Ÿå™¨äººå‡½æ•¸\n",
    "def chatbot_with_fc(message, history):\n",
    "    messages.append(ChatMessage.from_user(message))\n",
    "    response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "    while True:\n",
    "        # å¦‚æœ OpenAI å›æ‡‰æ˜¯ä¸€å€‹å·¥å…·èª¿ç”¨\n",
    "        if response and response[\"replies\"][0].meta[\"finish_reason\"] == \"tool_calls\":\n",
    "            function_calls = json.loads(response[\"replies\"][0].content)\n",
    "            print(response[\"replies\"][0])\n",
    "            for function_call in function_calls:\n",
    "                # è§£æå‡½æ•¸èª¿ç”¨ä¿¡æ¯\n",
    "                function_name = function_call[\"function\"][\"name\"]\n",
    "                function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
    "\n",
    "                # æŸ¥æ‰¾ç›¸æ‡‰çš„å‡½æ•¸ä¸¦ä½¿ç”¨çµ¦å®šçš„åƒæ•¸èª¿ç”¨å®ƒ\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_response = function_to_call(function_args)\n",
    "\n",
    "                # ä½¿ç”¨ `ChatMessage.from_function` å°‡å‡½æ•¸å›æ‡‰æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨\n",
    "                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n",
    "                response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "        # å¸¸è¦å°è©±\n",
    "        else:\n",
    "            messages.append(response[\"replies\"][0])\n",
    "            break\n",
    "    return response[\"replies\"][0].content\n",
    "\n",
    "# å‰µå»ºèŠå¤©ç•Œé¢\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot_with_fc,\n",
    "    # é¡¯ç¤ºåœ¨ä¸‹æ–¹çš„ç¯„ä¾‹æ¬„ä½\n",
    "    examples=[\n",
    "        \"ä½ èƒ½å‘Šè¨´æˆ‘ Giorgio ä½åœ¨å“ªè£¡å—ï¼Ÿ\",\n",
    "        \"Madrid çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ\",\n",
    "        \"èª°ä½åœ¨ London?\",\n",
    "        \"Mark ä½çš„åœ°æ–¹çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ\",\n",
    "    ],\n",
    "    title=\"è«‹è©¢å•æœ‰é—œå¤©æ°£æˆ–äººå€‘å±…ä½çš„åœ°æ–¹ã€‚\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessage(content='[{\"id\": \"call_iHmsDkbsiYIoVVRZMYeGE6Pb\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\":\\\\\"Tom\\\\u4f4f\\\\u54ea\\\\u88e1\\\\uff1f\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 23, 'prompt_tokens': 217, 'total_tokens': 240}})\n",
      "ChatMessage(content='[{\"id\": \"call_PbEX28ukt89LbvIzbUsqYrvG\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\":\\\\\"Who lives in what city?\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 21, 'prompt_tokens': 288, 'total_tokens': 309}})\n",
      "ChatMessage(content='[{\"id\": \"call_F2k4ZZxC1gHRv6bdNiRn4m4a\", \"function\": {\"arguments\": \"{\\\\\"location\\\\\":\\\\\"\\\\u67cf\\\\u6797\\\\\"}\", \"name\": \"get_current_weather\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 18, 'prompt_tokens': 352, 'total_tokens': 370}})\n",
      "ChatMessage(content='[{\"id\": \"call_q4TuerI9kI9Xk4HCOFxnm4J5\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\": \\\\\"Who lives where?\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}, {\"id\": \"call_jZI0SvzZsdVIZbgAmTygpxUR\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\": \\\\\"Where do people live?\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 54, 'prompt_tokens': 485, 'total_tokens': 539}})\n",
      "ChatMessage(content='[{\"id\": \"call_7RFOUYr4xCM7SCmHtSQbWP7Z\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\": \\\\\"Who lives where?\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}, {\"id\": \"call_HTA665QyeN1T4hiD0EYmtiG4\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\": \\\\\"Where do people live?\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 54, 'prompt_tokens': 542, 'total_tokens': 596}})\n",
      "ChatMessage(content='[{\"id\": \"call_ii2BpqMUDjJnyyGdxhb0glYD\", \"function\": {\"arguments\": \"{\\\\\"location\\\\\":\\\\\"Madrid\\\\\"}\", \"name\": \"get_current_weather\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 43, 'prompt_tokens': 634, 'total_tokens': 677}})\n",
      "ChatMessage(content='[{\"id\": \"call_K2orNqf5wWoapMcJhXXVEXK6\", \"function\": {\"arguments\": \"{\\\\\"location\\\\\":\\\\\"Berlin\\\\\"}\", \"name\": \"get_current_weather\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 15, 'prompt_tokens': 756, 'total_tokens': 771}})\n"
     ]
    }
   ],
   "source": [
    "# å•Ÿå‹•èŠå¤©æ‡‰ç”¨ç¨‹åº\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envHaystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
