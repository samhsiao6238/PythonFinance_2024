# é è™•ç†ä¸åŒæ–‡ä»¶é¡å‹

_ä½¿ç”¨ `FileTypeRouter`_

<br>

## èªªæ˜

1. ä½¿ç”¨ [Haystack 2.0](https://haystack.deepset.ai/overview/quick-start)ï¼Œå¯æŸ¥è©¢å®˜æ–¹ [Haystack 2.0 æ–‡ä»¶](https://haystack.deepset.ai/docs/latest).

<br>

2. å»ºç«‹ç´¢å¼•ç®¡é“å¾Œï¼Œé‚„å¯æ­é… `Hugging Face API` ä¾†å½™æ•´æ–‡ä»¶å„²å­˜æ§‹å»º RAG ç®¡é“ã€‚

<br>

3. é€™å€‹ç¯„ä¾‹çš„ç›®æ¨™æ˜¯æ§‹å»ºä¸€å€‹ç´¢å¼•ç®¡é“ï¼Œè©²ç®¡é“å¯ä»¥é è™•ç†ä¸åŒé¡å‹çš„æ–‡ä»¶ï¼ŒåŒ…æ‹¬ Markdownã€TXT å’Œ PDF æ–‡ä»¶ã€‚æ¯ç¨®æ–‡ä»¶é¡å‹éƒ½éœ€è¦ä½¿ç”¨ç‰¹å®šçš„æ–‡ä»¶è½‰æ›å™¨ä¾†é€²è¡Œè™•ç†ã€‚é€™äº›è½‰æ›å™¨å°‡å„è‡ªçš„æ–‡ä»¶æ ¼å¼è½‰æ›ç‚ºæ¨™æº–çš„ Haystack æ–‡ä»¶æ ¼å¼ã€‚

<br>

## æ–‡ä»¶è½‰æ›å™¨

1. MarkdownToDocumentï¼šå°‡ Markdown æ–‡ä»¶è½‰æ›ç‚º Haystack æ–‡ä»¶ã€‚

<br>

2. TextFileToDocumentï¼šå°‡æ–‡æœ¬æ–‡ä»¶ï¼ˆå¦‚ TXTï¼‰è½‰æ›ç‚º Haystack æ–‡ä»¶ã€‚

<br>

3. PyPDFToDocumentï¼šå°‡ PDF æ–‡ä»¶è½‰æ›ç‚º Haystack æ–‡ä»¶ã€‚

<br>

## ç´¢å¼•ç®¡é“çš„å…¶ä»–æ­¥é©Ÿ

_ä¸€æ—¦æ‰€æœ‰æ–‡ä»¶éƒ½è¢«è½‰æ›ç‚º Haystack æ–‡ä»¶æ ¼å¼ï¼Œç´¢å¼•ç®¡é“çš„å…¶é¤˜éƒ¨åˆ†ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å¹¾å€‹æ¨™æº–æ­¥é©Ÿ_

<br>

1. æ–‡ä»¶æ¸…ç†ï¼ˆDocumentCleanerï¼‰ï¼šå»é™¤æ–‡ä»¶ä¸­çš„å¤šé¤˜ç©ºç™½å’Œå…¶ä»–ä¸éœ€è¦çš„å­—ç¬¦ï¼Œä½¿æ–‡ä»¶æ›´åŠ æ•´æ½”ã€‚

<br>

2. æ–‡ä»¶åˆ†å¡Šï¼ˆDocumentSplitterï¼‰ï¼šå°‡æ–‡ä»¶åˆ†æˆå°å¡Šï¼ˆä¾‹å¦‚æ¯å¡Š 150 å€‹å–®è©ï¼‰ï¼Œé€™æ¨£å¯ä»¥æ›´æ–¹ä¾¿åœ°é€²è¡Œæª¢ç´¢å’Œåˆ†æï¼Œä¸¦ä¸”å¯ä»¥é¿å…ä¸Ÿå¤±ä¸Šä¸‹æ–‡ã€‚

<br>

3. å‰µå»ºåµŒå…¥ï¼ˆSentenceTransformersDocumentEmbedderï¼‰ï¼šä½¿ç”¨åµŒå…¥æ¨¡å‹ï¼ˆä¾‹å¦‚ `sentence-transformers/all-MiniLM-L6-v2`ï¼‰ç‚ºæ–‡ä»¶å‰µå»ºåµŒå…¥å‘é‡ï¼Œé€™äº›å‘é‡ç”¨æ–¼å¾ŒçºŒçš„æ–‡æœ¬æª¢ç´¢å’ŒæŸ¥è©¢ã€‚

<br>

4. å¯«å…¥æ–‡ä»¶å„²å­˜ï¼ˆDocumentWriterï¼‰ï¼šæœ€å¾Œï¼Œå°‡è™•ç†å¥½çš„æ–‡ä»¶å¯«å…¥åˆ°æ–‡ä»¶å„²å­˜ï¼ˆä¾‹å¦‚ `InMemoryDocumentStore`ï¼‰ä¸­ï¼Œé€™æ¨£å¯ä»¥åœ¨å¾ŒçºŒçš„æŸ¥è©¢ä¸­é€²è¡Œé«˜æ•ˆæª¢ç´¢ã€‚

<br>

## å…¶ä»–é‡è¦æ¨¡çµ„

1. FileTypeRouterï¼šæ˜¯ä¸€å€‹è·¯ç”±å™¨ï¼Œç”¨æ–¼æ ¹æ“šæ–‡ä»¶çš„ MIME é¡å‹å°‡æ–‡ä»¶è·¯ç”±åˆ°ä¸åŒçš„æ¨¡çµ„æˆ–è½‰æ›å™¨ã€‚ç•¶æ–‡ä»¶ä¾†è‡ªä¸åŒçš„æ•¸æ“šæºï¼Œä¸”æ¯å€‹æ–‡ä»¶çš„é¡å‹ä¸åŒï¼ˆå¦‚ PDFã€Markdown æˆ– TXT æ–‡ä»¶ï¼‰æ™‚ï¼ŒFileTypeRouter å¯ä»¥æ ¹æ“šæ–‡ä»¶çš„ MIME é¡å‹å°‡å…¶è·¯ç”±åˆ°ç›¸æ‡‰çš„æ–‡ä»¶è½‰æ›å™¨é€²è¡Œè™•ç†ï¼Œé€™æ¨£å¯ä»¥ç¢ºä¿æ¯å€‹æ–‡ä»¶éƒ½èƒ½è¢«æ­£ç¢ºåœ°è½‰æ›æˆ Haystack æ–‡ä»¶æ ¼å¼ã€‚

<br>

2. DocumentJoinerï¼šç”¨æ–¼å°‡ä¾†è‡ªä¸åŒç®¡é“åˆ†æ”¯çš„æ–‡ä»¶åˆä½µæˆä¸€å€‹çµ±ä¸€çš„æ–‡ä»¶åˆ—è¡¨ï¼Œç•¶ä¸åŒçš„æ–‡ä»¶é¡å‹è¢«ä¸åŒçš„è½‰æ›å™¨è™•ç†ä¸¦ç”Ÿæˆ Haystack æ–‡ä»¶å¾Œï¼ŒDocumentJoiner å°‡é€™äº›åˆ†æ•£çš„æ–‡ä»¶åˆä½µæˆä¸€å€‹çµ±ä¸€çš„æ–‡ä»¶åˆ—è¡¨ï¼Œä»¥ä¾¿å¾ŒçºŒçš„æ¸…ç†ã€åˆ†å¡Šå’ŒåµŒå…¥è™•ç†ã€‚

<br>

## é–‹å§‹

1. å®‰è£ä¾è³´ã€‚

    ```bash
    pip install haystack-ai
    pip install "sentence-transformers>=2.2.0" "huggingface_hub>=0.22.0"
    pip install markdown-it-py mdit_plain pypdf
    # ä¸‹è¼‰æ–‡ä»¶
    pip install gdown
    ```

<br>

2. ä¸‹è¼‰æ‰€æœ‰æ–‡ä»¶ã€‚

    ```python
    # å°å…¥åº«
    import gdown
    # ä¸‹è¼‰ç¶²å€
    url = "https://drive.google.com/drive/folders/1n9yqq5Gl_HWfND5bTlrCwAOycMDt5EMj"
    # å„²å­˜çš„è³‡æ–™å¤¾
    output_dir = "recipe_files"
    # ä»¥éœé»˜æ–¹å¼ä¸‹è¼‰åˆ°æŒ‡å®šè³‡æ–™å¤¾ä¸­
    gdown.download_folder(
        url,
        quiet=True,
        output=output_dir
    )
    ```

<br>

3. ä¸‹è¼‰çš„è³‡æ–™æœ‰ä¸‰å€‹ï¼Œæ ¼å¼åˆ†åˆ¥ç‚º `Markdown`ã€`PDF`ã€`TXT`ã€‚

    ![](images/img_19.png)

<br>

4. å‰µå»º `ç´¢å¼•æ–‡ä»¶` çš„ `ç®¡é“`ï¼Œé¦–å…ˆæè¿°ä¸€ä¸‹é€™å€‹æµç¨‹ï¼šé¦–å…ˆè¦ä½¿ç”¨ `InMemoryDocumentStore` è™•ç† `æ–‡ä»¶å„²å­˜`ï¼Œéœ€è¦ç‚ºæ•¸æ“šä¸­çš„æ¯ç¨®é¡å‹çš„æ–‡ä»¶ä½¿ç”¨ä¸åŒçš„ `æ–‡ä»¶è½‰æ›å™¨é¡`ï¼Œè€Œ `FileTypeRouter` å¯ç‚ºæ¯ç¨®é¡å‹çš„æ–‡ä»¶é€£æ¥åˆ°é©ç•¶çš„è½‰æ›å™¨ã€‚è½‰æ›å¾Œçš„ Haystack æ–‡ä»¶ä¾¿å¯ä½¿ç”¨ `DocumentJoiner` æ¨¡çµ„å°‡é€™äº›æ–‡ä»¶åˆä½µæˆä¸€å€‹ `æ–‡ä»¶åˆ—è¡¨`ï¼Œç„¶å¾Œä¸€èµ·å‚³éçµ¦ç´¢å¼•ç®¡é“çš„å…¶é¤˜éƒ¨åˆ†ã€‚

    ```python
    from haystack.components.writers import DocumentWriter
    from haystack.components.converters import MarkdownToDocument, PyPDFToDocument, TextFileToDocument
    from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner
    from haystack.components.routers import FileTypeRouter
    from haystack.components.joiners import DocumentJoiner
    from haystack.components.embedders import SentenceTransformersDocumentEmbedder
    from haystack import Pipeline
    from haystack.document_stores.in_memory import InMemoryDocumentStore

    # å»ºç«‹ä¸€å€‹æ–‡ä»¶å„²å­˜å¯¦é«”
    document_store = InMemoryDocumentStore()
    # çµ„ä»¶ï¼šå»ºç«‹æ–‡ä»¶è½‰æ›è·¯ç”±å™¨
    file_type_router = FileTypeRouter(
        mime_types=[
            "text/plain",
            "application/pdf",
            "text/markdown"
        ]
    )
    # åˆ†åˆ¥é‡å°æ–‡ä»¶é¡å‹æŒ‡å®šè½‰æ›å™¨çµ„ä»¶
    # çµ„ä»¶ï¼šTXT
    text_file_converter = TextFileToDocument()
    # çµ„ä»¶ï¼šMarkdown
    markdown_converter = MarkdownToDocument()
    # çµ„ä»¶ï¼šPDF
    pdf_converter = PyPDFToDocument()
    # çµ„ä»¶ï¼šæ–‡ä»¶åˆä½µå™¨å¯¦é«”
    document_joiner = DocumentJoiner()
    ```

<br>

4. æ•¸æ“šæ¸…æ´—ï¼šä½¿ç”¨ `DocumentCleaner` åˆªé™¤ç©ºç™½ï¼Œç„¶å¾Œ `DocumentSplitter` å°‡å®ƒå€‘åˆ†æˆ `150` å­—çš„å¡Šï¼Œä¸¦é€²è¡Œä¸€äº›é‡ç–Šä»¥é¿å…ä¸Ÿå¤±ä¸Šä¸‹æ–‡ã€‚

    ```python
    # çµ„ä»¶ï¼šåˆªé™¤ç©ºç™½å™¨
    document_cleaner = DocumentCleaner()
    # çµ„ä»¶ï¼šåˆ‡å‰²æ–‡ä»¶å™¨
    document_splitter = DocumentSplitter(
        split_by="word",
        split_length=150,
        split_overlap=50
    )
    ```

<br>

5. æ·»åŠ ä¸€å€‹ `SentenceTransformersDocumentEmbedder` ä¾†å‰µå»ºæ–‡ä»¶çš„åµŒå…¥ï¼Œç„¶å¾Œä½¿ç”¨ `DocumentWriter` å°‡é€™äº›æ–‡ä»¶å¯«å…¥å…§å­˜æ–‡ä»¶å„²å­˜ã€‚

    ```python
    # çµ„ä»¶ï¼šæ–‡ä»¶åµŒå…¥å™¨
    document_embedder = SentenceTransformersDocumentEmbedder(
        model="sentence-transformers/all-MiniLM-L6-v2"
    )
    # çµ„ä»¶ï¼šå¯«å…¥æ–‡ä»¶å„²å­˜
    document_writer = DocumentWriter(document_store)
    ```

<br>

6. å‰µå»ºæ‰€æœ‰æ¨¡çµ„å¾Œï¼Œå°‡å®ƒå€‘æ·»åŠ åˆ° `ç´¢å¼•ç®¡é“` ä¸­ã€‚

    ```python
    # å»ºç«‹ç´¢å¼•ç®¡é“å¯¦é«”
    preprocessing_pipeline = Pipeline()
    # æ·»åŠ ä»¥ä¸Šå„ç¨®çµ„ä»¶
    preprocessing_pipeline.add_component(
        # æ–‡ä»¶è½‰æ›è·¯ç”±å™¨
        instance=file_type_router, name="file_type_router"
    )
    preprocessing_pipeline.add_component(
        # TXT æ–‡ä»¶è½‰æ›
        instance=text_file_converter, name="text_file_converter"
    )
    preprocessing_pipeline.add_component(
        # MD æ–‡ä»¶è½‰æ›
        instance=markdown_converter, name="markdown_converter"
    )
    preprocessing_pipeline.add_component(
        # PDF æ–‡ä»¶è½‰æ›
        instance=pdf_converter, name="pypdf_converter"
    )
    preprocessing_pipeline.add_component(
        # æ–‡ä»¶åˆä½µ
        instance=document_joiner, name="document_joiner"
    )
    preprocessing_pipeline.add_component(
        # åˆªé™¤ç©ºç™½
        instance=document_cleaner, name="document_cleaner"
    )
    preprocessing_pipeline.add_component(
        # æ–‡ä»¶åˆ‡å‰²
        instance=document_splitter, name="document_splitter"
    )
    preprocessing_pipeline.add_component(
        # æ–‡ä»¶åµŒå…¥
        instance=document_embedder, name="document_embedder"
    )
    preprocessing_pipeline.add_component(
        # å¯«å…¥æ–‡ä»¶å„²å­˜
        instance=document_writer, name="document_writer"
    )
    ```

<br>

7. æ¥ä¸‹ä¾†ï¼Œè¨­å®š `é è™•ç†ç®¡é“`çš„ `connect` æ–¹æ³•ï¼Œå°‡ä¸åŒé¡å‹çš„æ–‡ä»¶é€²è¡Œè½‰æ›ã€æ¸…ç†ã€åµŒå…¥å¾Œå¯«å…¥æ–‡ä»¶å„²å­˜ä¸­ã€‚

    ```python
    preprocessing_pipeline.connect(
        # è™•ç† text/plain é¡å‹æ–‡ä»¶ï¼Œå°‡å…¶å‚³éçµ¦ text_file_converter é€²è¡Œè½‰æ›
        "file_type_router.text/plain", "text_file_converter.sources"
    )
    preprocessing_pipeline.connect(
        # è™•ç† application/pdf é¡å‹æ–‡ä»¶ï¼Œå°‡å…¶å‚³éçµ¦ pypdf_converter
        # ä¹Ÿå°±æ˜¯é€™å€‹é è™•ç†ç®¡é“å¯æ¥æ”¶ PDF æ–‡ä»¶ï¼Œä¸¦é€šé PDF è½‰æ›å™¨é€²è¡Œè™•ç†
        "file_type_router.application/pdf", "pypdf_converter.sources"
    )
    preprocessing_pipeline.connect(
        # è™•ç† Markdown æ–‡ä»¶ï¼Œä¸¦é€šé Markdown è½‰æ›å™¨ä¾†è™•ç†
        "file_type_router.text/markdown", "markdown_converter.sources"
    )
    preprocessing_pipeline.connect(
        # å°‡ç¶“éæ–‡æœ¬è½‰æ›å™¨è™•ç†çš„æ–‡ä»¶å‚³éçµ¦æ–‡ä»¶åˆä½µå™¨
        "text_file_converter", "document_joiner"
    )
    preprocessing_pipeline.connect(
        # è™•ç† application/pdf é¡å‹æ–‡ä»¶ï¼Œå°‡å…¶å‚³éçµ¦ pypdf_converter
        "pypdf_converter", "document_joiner"
    )
    preprocessing_pipeline.connect(
        "markdown_converter", "document_joiner"
    )
    preprocessing_pipeline.connect(
        # document_joiner æ˜¯ç”¨æ–¼åˆä½µä¸åŒçš„æ–‡ä»¶ç‰‡æ®µçš„çµ„ä»¶
        "document_joiner", "document_cleaner"
    )
    preprocessing_pipeline.connect(
        # å°‡æ–‡ä»¶åˆä½µå™¨çš„è¼¸å‡ºé€£æ¥åˆ°æ–‡ä»¶æ¸…ç†å™¨
        "document_cleaner", "document_splitter"
    )
    preprocessing_pipeline.connect(
        # å°‡æ‹†åˆ†å¾Œçš„æ–‡ä»¶å‚³çµ¦ document_embedderï¼Œç”¨æ–¼ç”Ÿæˆé€™äº›ç‰‡æ®µçš„åµŒå…¥å‘é‡
        # é€™æ¨£æ¯å€‹æ–‡ä»¶ç‰‡æ®µéƒ½æœƒè¢«è½‰æ›æˆåµŒå…¥å‘é‡ï¼Œä¾¿æ–¼å¾ŒçºŒçš„æª¢ç´¢å’Œåˆ†æ
        "document_splitter", "document_embedder"
    )
    preprocessing_pipeline.connect(
        # å°‡åµŒå…¥å¾Œçš„æ–‡ä»¶å‚³éçµ¦ document_writer
        # æœ€çµ‚å°‡é€™äº›åµŒå…¥å¯«å…¥åˆ°æ–‡ä»¶å„²å­˜ä¸­
        "document_embedder", "document_writer"
    )
    ```

<br>

8. è¼¸å‡ºå¦‚ä¸‹ï¼Œä»¥ä¸Šå®Œæˆè³‡æ–™çš„é è™•ç†ã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x3173b1480>
    ğŸš… Components
    - file_type_router: FileTypeRouter
    - text_file_converter: TextFileToDocument
    - markdown_converter: MarkdownToDocument
    - pypdf_converter: PyPDFToDocument
    - document_joiner: DocumentJoiner
    - document_cleaner: DocumentCleaner
    - document_splitter: DocumentSplitter
    - document_embedder: SentenceTransformersDocumentEmbedder
    - document_writer: DocumentWriter
    ğŸ›¤ï¸ Connections
    - file_type_router.text/plain -> text_file_converter.sources (List[Path])
    - file_type_router.application/pdf -> pypdf_converter.sources (List[Path])
    - file_type_router.text/markdown -> markdown_converter.sources (List[Path])
    - text_file_converter.documents -> document_joiner.documents (List[Document])
    - markdown_converter.documents -> document_joiner.documents (List[Document])
    - pypdf_converter.documents -> document_joiner.documents (List[Document])
    - document_joiner.documents -> document_cleaner.documents (List[Document])
    - document_cleaner.documents -> document_splitter.documents (List[Document])
    - document_splitter.documents -> document_embedder.documents (List[Document])
    - document_embedder.documents -> document_writer.documents (List[Document])
    ```

<br>

## åœ¨ RAG ç®¡é“ä¸­ä½¿ç”¨é€™äº›æ–‡ä»¶

1. ç”¨é£Ÿè­œæ–‡ä»¶ `recipe.txt` ä¾†æ¸¬è©¦é€™å€‹ç®¡é“ï¼šèªªæ˜ä¸€ä¸‹é€™å€‹é£Ÿè­œæ–‡ä»¶ä¸‹è¼‰çš„æ™‚å€™å…¨åæ˜¯ `vegan_sunflower_hemp_cheese_recipe.txt`ï¼Œæˆ‘æ‰‹å‹•é€²è¡Œä¿®æ”¹ç‚ºçŸ­åç¨±ï¼Œç›®çš„æ˜¯åœ¨é–±è®€è¼¸å‡ºçš„æ™‚å€™æ¯”è¼ƒç°¡æ½”ã€‚

    ![](images/img_25.png)

<br>

2. ç¨‹å¼ç¢¼ã€‚

    ```python
    from pathlib import Path

    # æ¸¬è©¦æ–‡ä»¶è·¯å¾‘æ˜¯å¦å­˜åœ¨
    file_path = Path("recipe_files/recipe.txt")
    if file_path.exists():
        print(f"æ­£åœ¨è™•ç†æ–‡ä»¶: {file_path}")

        try:
            # è¨­ç½®æ—¥èªŒç´šåˆ¥ä»¥æŸ¥çœ‹è™•ç†éç¨‹
            import logging
            logging.basicConfig(level=logging.INFO)
            
            preprocessing_pipeline.run(
                {"file_type_router": {"sources": [file_path]}}
            )
        except Exception as e:
            print(f"è™•ç†éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
    else:
        print(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ã€‚")

    ```

    ![](images/img_20.png)

<br>

## ä½¿ç”¨

1. åœ¨ä¸‹ä¸€å€‹æ­¥é©Ÿä¸­å°‡æœƒä½¿ç”¨åˆ° `HuggingFaceAPIGenerator`ï¼Œå› æ­¤éœ€è¦æ“æœ‰ä¸€å€‹ `Hugging Face API Key`ï¼Œè‹¥é‚„æ²’æœ‰å¯†é‘°è«‹å‰å¾€ [å®˜ç¶²](https://huggingface.co/settings/tokens) å»ºç«‹ã€‚

    ![](images/img_21.png)

<br>

2. å‹™å¿…ç¢ºä¿ `Hugging Face Token` å…·æœ‰ç›¸å°çš„æ¬Šé™ï¼Œå¯åœ¨é¸å–®ä¸­é€²è¡Œæ¬Šé™ç·¨è¼¯ã€‚

    ![](images/img_23.png)

<br>

3. åœ¨æ­¤éšæ®µå¯å°‡å…¨éƒ¨æ¬Šé™å‹¾é¸é–‹å•Ÿï¼ŒåŒ…å«å¾ŒçºŒæœªé¡¯ç¤ºåœ¨åœ–ç‰‡ä¸­çš„æ¬Šé™é …ç›®ã€‚

    ![](images/img_24.png)

<br>

4. å¦å¤–å°æ–¼å°‡ä½¿ç”¨åˆ°çš„ `HuggingFaceH4/zephyr-7b-beta` æ¨¡å‹ï¼Œå¯åœ¨ `Models` é ç±¤ä¸­ç¢ºèªæ¨¡å‹ IDã€‚

    ![](images/img_22.png)

<br>

5. å¯«å…¥ `.env` æ–‡ä»¶ã€‚

    ```json
    HF_API_TOKEN=<å¯«å…¥è‡ªå·±çš„ API KEY>
    ```

<br>

## å»ºç«‹è…³æœ¬å›ç­”å•é¡Œ

1. å®Œæˆè¨­ç½®å¾Œï¼Œç·¨å¯«è…³æœ¬æ§‹å»ºä¸€å€‹ `RAG ç®¡é“` ä¾†æ ¹æ“šå‰›å‰›å‰µå»ºçš„æ–‡ä»¶å›ç­”æŸ¥è©¢ã€‚
    ```python
    import os
    from getpass import getpass
    from dotenv import load_dotenv

    # ç’°å¢ƒè®Šæ•¸
    load_dotenv()
    os.environ["HF_API_TOKEN"] = os.getenv("HF_API_TOKEN")
    # æª¢æŸ¥æ˜¯å¦å·²ç¶“å¯«å…¥
    if "HF_API_TOKEN" not in os.environ:
        # å°šæœªå¯«å…¥å‰‡é€é `getpass` é€²è¡Œè¨­å®š
        os.environ["HF_API_TOKEN"] = getpass("Enter Hugging Face token:")
    ```

<br>

2. è©²ç®¡é“æ¥æ”¶æç¤ºï¼Œå¾æ–‡ä»¶å„²å­˜ä¸­æœç´¢ç›¸é—œæ–‡ä»¶ï¼Œä¸¦å°‡é€™äº›æ–‡ä»¶å‚³éçµ¦ LLM ä»¥å½¢æˆç­”æ¡ˆã€‚

    ```python
    from haystack.components.embedders import SentenceTransformersTextEmbedder
    from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
    from haystack.components.builders import PromptBuilder
    from haystack.components.generators import HuggingFaceAPIGenerator

    template = """
    Answer the questions based on the given context.

    Context:
    {% for document in documents %}
        {{ document.content }}
    {% endfor %}

    Question: {{ question }}
    Answer:
    """
    pipe = Pipeline()
    pipe.add_component("embedder", SentenceTransformersTextEmbedder(model="sentence-transformers/all-MiniLM-L6-v2"))
    pipe.add_component("retriever", InMemoryEmbeddingRetriever(document_store=document_store))
    pipe.add_component("prompt_builder", PromptBuilder(template=template))
    pipe.add_component(
        "llm",
        HuggingFaceAPIGenerator(api_type="serverless_inference_api", api_params={"model": "HuggingFaceH4/zephyr-7b-beta"}),
    )

    pipe.connect("embedder.embedding", "retriever.query_embedding")
    pipe.connect("retriever", "prompt_builder.documents")
    pipe.connect("prompt_builder", "llm")
    ```

<br>

3. æœƒè¼¸å‡ºä»¥ä¸‹è¨Šæ¯ã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x36b59c040>
    ğŸš… Components
    - embedder: SentenceTransformersTextEmbedder
    - retriever: InMemoryEmbeddingRetriever
    - prompt_builder: PromptBuilder
    - llm: HuggingFaceAPIGenerator
    ğŸ›¤ï¸ Connections
    - embedder.embedding -> retriever.query_embedding (List[float])
    - retriever.documents -> prompt_builder.documents (List[Document])
    - prompt_builder.prompt -> llm.prompt (str)
    ```

<br>

4. å¦‚æœä¸€åˆ‡æ­£ç¢ºï¼Œä»¥ä¸‹ä»£ç¢¼å°‡æœƒå¾—åˆ°ä¸€ä»½å®Œæ•´çš„è³¼ç‰©æ¸…å–®ï¼ŒåŒ…æ‹¬æ‰€æœ‰çš„é£Ÿè­œä¾†æºã€‚

    ```python
    # æå•ï¼šè£½ä½œç´”ç´ é…®èŒ„å­çƒ¤å¯¬éºµæ¢ã€ç´”ç´ æŸ¿å­é¤¡é¤…å’Œç´”ç´ å¤§éº»èµ·å¸éœ€è¦å“ªäº›åŸæ–™ï¼Ÿ
    question = (
        "What ingredients would I need to make vegan keto eggplant lasagna, vegan persimmon flan, and vegan hemp cheese?"
    )

    pipe.run(
        {
            "embedder": {"text": question},
            "prompt_builder": {"question": question},
            "llm": {"generation_kwargs": {"max_new_tokens": 350}},
        }
    )
    ```

<br>

5. è¼¸å‡ºçµæœã€‚

    ```json
    {
        'llm': {
            'replies': [
                ' For vegan keto eggplant lasagna, you would need eggplant, tomato sauce, vegan cheese (such as mozzarella or parmesan), lasagna noodles made from low-carb ingredients (such as zucchini or mushrooms), and any additional seasonings or herbs you prefer. For vegan persimmon flan, you would need persimmons, coconut milk, agar agar (a vegan substitute for gelatin), sugar, and vanilla extract. For vegan hemp cheese, you would need sunflower seeds, hemp hearts, miso paste, nutritional yeast, rejuvelac (a fermented liquid made from sprouted grains), salt, and any additional flavorings you prefer.'
            ],
            'meta': [{
                'model': 'HuggingFaceH4/zephyr-7b-beta',
                'finish_reason': 'eos_token',
                'usage': {'completion_tokens': 168}
            }]
        }
    }
    ```

<br>

___

_END_