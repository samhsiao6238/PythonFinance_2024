# 預處理不同文件類型

_使用 `FileTypeRouter`_

## 說明

1. 使用 [Haystack 2.0](https://haystack.deepset.ai/overview/quick-start)，可查詢官方 [Haystack 2.0 文件](https://haystack.deepset.ai/docs/latest).

2. 建立索引管道後，還可搭配 `Hugging Face API` 來彙整文件儲存構建 RAG 管道。

3. 這個範例的目標是構建一個索引管道，該管道可以預處理不同類型的文件，包括 Markdown、TXT 和 PDF 文件。每種文件類型都需要使用特定的文件轉換器來進行處理。這些轉換器將各自的文件格式轉換為標準的 Haystack 文件格式。

## 文件轉換器

1. MarkdownToDocument：將 Markdown 文件轉換為 Haystack 文件。
2. TextFileToDocument：將文本文件（如 TXT）轉換為 Haystack 文件。
3. PyPDFToDocument：將 PDF 文件轉換為 Haystack 文件。

## 索引管道的其他步驟

_一旦所有文件都被轉換為 Haystack 文件格式，索引管道的其餘部分主要包括以下幾個標準步驟_

1. 文件清理（DocumentCleaner）：去除文件中的多餘空白和其他不需要的字符，使文件更加整潔。

2. 文件分塊（DocumentSplitter）：將文件分成小塊（例如每塊 150 個單詞），這樣可以更方便地進行檢索和分析，並且可以避免丟失上下文。

3. 創建嵌入（SentenceTransformersDocumentEmbedder）：使用嵌入模型（例如 `sentence-transformers/all-MiniLM-L6-v2`）為文件創建嵌入向量，這些向量用於後續的文本檢索和查詢。

4. 寫入文件儲存（DocumentWriter）：最後，將處理好的文件寫入到文件儲存（例如 `InMemoryDocumentStore`）中，這樣可以在後續的查詢中進行高效檢索。

## 其他重要模組

1. FileTypeRouter：是一個路由器，用於根據文件的 MIME 類型將文件路由到不同的模組或轉換器。當文件來自不同的數據源，且每個文件的類型不同（如 PDF、Markdown 或 TXT 文件）時，FileTypeRouter 可以根據文件的 MIME 類型將其路由到相應的文件轉換器進行處理，這樣可以確保每個文件都能被正確地轉換成 Haystack 文件格式。

2. DocumentJoiner：用於將來自不同管道分支的文件合併成一個統一的文件列表，當不同的文件類型被不同的轉換器處理並生成 Haystack 文件後，DocumentJoiner 將這些分散的文件合併成一個統一的文件列表，以便後續的清理、分塊和嵌入處理。



## 開始

1. 安裝依賴。

```bash
pip install haystack-ai
pip install "sentence-transformers>=2.2.0" "huggingface_hub>=0.22.0"
pip install markdown-it-py mdit_plain pypdf
# 下載文件
pip install gdown
```

2. 下載所有文件。

```python
import gdown

url = "https://drive.google.com/drive/folders/1n9yqq5Gl_HWfND5bTlrCwAOycMDt5EMj"
output_dir = "recipe_files"

gdown.download_folder(url, quiet=True, output=output_dir)
```

3. 創建索引文件的管道：使用 `InMemoryDocumentStore`，但此方法也適用於任何其他類型的 `DocumentStore`，需要為數據源中的每種類型的文件使用不同的文件轉換器類：PDF、TXT 和 MD 文件， `FileTypeRouter` 將每種類型的文件連接到適當的轉換器。將所有文件轉換為 Haystack 文件後，便可使用 `DocumentJoiner` 模組將這些文件合併成一個文件列表，然後一起傳遞給索引管道的其餘部分。

```python
from haystack.components.writers import DocumentWriter
from haystack.components.converters import MarkdownToDocument, PyPDFToDocument, TextFileToDocument
from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner
from haystack.components.routers import FileTypeRouter
from haystack.components.joiners import DocumentJoiner
from haystack.components.embedders import SentenceTransformersDocumentEmbedder
from haystack import Pipeline
from haystack.document_stores.in_memory import InMemoryDocumentStore

document_store = InMemoryDocumentStore()
file_type_router = FileTypeRouter(
    mime_types=[
        "text/plain",
        "application/pdf",
        "text/markdown"
    ]
)
text_file_converter = TextFileToDocument()
markdown_converter = MarkdownToDocument()
pdf_converter = PyPDFToDocument()
document_joiner = DocumentJoiner()
```


4. 使用 `DocumentCleaner` 刪除空白，然後 `DocumentSplitter` 將它們分成 `150` 字的塊，並進行一些重疊以避免丟失上下文。

```python
document_cleaner = DocumentCleaner()
document_splitter = DocumentSplitter(
    split_by="word",
    split_length=150,
    split_overlap=50
)
```

5. 添加一個 `SentenceTransformersDocumentEmbedder` 來創建文件的嵌入，然後使用 `DocumentWriter` 將這些文件寫入內存文件儲存。

```python
document_embedder = SentenceTransformersDocumentEmbedder(
    model="sentence-transformers/all-MiniLM-L6-v2"
)
document_writer = DocumentWriter(document_store)
```

6. 創建所有模組後，將它們添加到索引管道中。

```python
preprocessing_pipeline = Pipeline()
preprocessing_pipeline.add_component(
    instance=file_type_router, name="file_type_router"
)
preprocessing_pipeline.add_component(
    instance=text_file_converter, name="text_file_converter"
)
preprocessing_pipeline.add_component(
    instance=markdown_converter, name="markdown_converter"
)
preprocessing_pipeline.add_component(
    instance=pdf_converter, name="pypdf_converter"
)
preprocessing_pipeline.add_component(
    instance=document_joiner, name="document_joiner"
)
preprocessing_pipeline.add_component(
    instance=document_cleaner, name="document_cleaner"
)
preprocessing_pipeline.add_component(
    instance=document_splitter, name="document_splitter"
)
preprocessing_pipeline.add_component(
    instance=document_embedder, name="document_embedder"
)
preprocessing_pipeline.add_component(
    instance=document_writer, name="document_writer"
)
```

7. 接下來，將它們連接起來。

```python
preprocessing_pipeline.connect(
    "file_type_router.text/plain", "text_file_converter.sources"
)
preprocessing_pipeline.connect(
    "file_type_router.application/pdf", "pypdf_converter.sources"
)
preprocessing_pipeline.connect(
    "file_type_router.text/markdown", "markdown_converter.sources"
)
preprocessing_pipeline.connect(
    "text_file_converter", "document_joiner"
)
preprocessing_pipeline.connect(
    "pypdf_converter", "document_joiner"
)
preprocessing_pipeline.connect(
    "markdown_converter", "document_joiner"
)
preprocessing_pipeline.connect(
    "document_joiner", "document_cleaner"
)
preprocessing_pipeline.connect(
    "document_cleaner", "document_splitter"
)
preprocessing_pipeline.connect(
    "document_splitter", "document_embedder"
)
preprocessing_pipeline.connect(
    "document_embedder", "document_writer"
)
```

_以上完成資料的預處理_

## 在 RAG 管道中使用這些文件

1. 務必確保 Hugging Face 帳號具有相對的權限，在此階段可將全部權限勾選開啟。

2. 用食譜文件 `recipe.txt` 來測試這個管道。

```python
from pathlib import Path

# 測試文件路徑是否存在
file_path = Path("recipe_files/recipe.txt")
if file_path.exists():
    print(f"正在處理文件: {file_path}")

    try:
        # 設置日誌級別以查看處理過程
        import logging
        logging.basicConfig(level=logging.INFO)
        
        preprocessing_pipeline.run(
            {"file_type_router": {"sources": [file_path]}}
        )
    except Exception as e:
        print(f"處理過程中出現錯誤: {e}")
else:
    print(f"文件 {file_path} 不存在。")

```

3. 構建一個 RAG 管道來根據剛剛創建的文件回答查詢。在這一步將使用 `HuggingFaceAPIGenerator`，因此需要擁有一個 Hugging Face API Key，同時將使用 `HuggingFaceH4/zephyr-7b-beta` 模型。

```python
import os
from getpass import getpass
from dotenv import load_dotenv()

os.environ["HF_API_TOKEN"] = os.getenv["HF_API_TOKEN"]

if "HF_API_TOKEN" not in os.environ:
    os.environ["HF_API_TOKEN"] = getpass("Enter Hugging Face token:")
```

## 回答有關文件的問題

1. 該管道接收提示，從文件儲存中搜索相關文件，並將這些文件傳遞給 LLM 以形成答案。

```python
from haystack.components.embedders import SentenceTransformersTextEmbedder
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
from haystack.components.builders import PromptBuilder
from haystack.components.generators import HuggingFaceAPIGenerator

template = """
Answer the questions based on the given context.

Context:
{% for document in documents %}
    {{ document.content }}
{% endfor %}

Question: {{ question }}
Answer:
"""
pipe = Pipeline()
pipe.add_component("embedder", SentenceTransformersTextEmbedder(model="sentence-transformers/all-MiniLM-L6-v2"))
pipe.add_component("retriever", InMemoryEmbeddingRetriever(document_store=document_store))
pipe.add_component("prompt_builder", PromptBuilder(template=template))
pipe.add_component(
    "llm",
    HuggingFaceAPIGenerator(api_type="serverless_inference_api", api_params={"model": "HuggingFaceH4/zephyr-7b-beta"}),
)

pipe.connect("embedder.embedding", "retriever.query_embedding")
pipe.connect("retriever", "prompt_builder.documents")
pipe.connect("prompt_builder", "llm")
```

2. 如果一切正確，以下代碼將會得到一份完整的購物清單，包括所有的食譜來源。

```python
question = (
    "What ingredients would I need to make vegan keto eggplant lasagna, vegan persimmon flan, and vegan hemp cheese?"
)

pipe.run(
    {
        "embedder": {"text": question},
        "prompt_builder": {"question": question},
        "llm": {"generation_kwargs": {"max_new_tokens": 350}},
    }
)
```

### 續後
恭喜，成功構建了一個可以預處理不同文件類型的索引管道。去吧，將所有混亂的現實世界數據導入的工作流程中。💥

如果喜歡本教程，可能還會喜歡：

[序列化 Haystack 管道](https://haystack.deepset.ai/docs/latest/tutorials/serialize_pipeline)##
[創建的第一個基於檢索增強的問答管道](https://haystack.deepset.ai/docs/latest/tutorials/qa_rag)##

保持關注最新的 Haystack 動態，可以註冊我們的新聞通訊。感謝的閱讀！

--- 

這個整理提供了一個全面的指南，展示了如何使用 Haystack 2.0 構建和查詢一個能處理多種文件格式的索引管道，並使用 RAG 方法進行查詢。