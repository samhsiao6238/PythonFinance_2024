{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# È†êËôïÁêÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‰∏ãËºâÊâÄÊúâÊñá‰ª∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recipe_files/vegan_flan_recipe.md',\n",
       " 'recipe_files/vegan_keto_eggplant_recipe.pdf',\n",
       " 'recipe_files/vegan_sunflower_hemp_cheese_recipe.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Â∞éÂÖ•Â∫´\n",
    "import gdown\n",
    "# ‰∏ãËºâÁ∂≤ÂùÄ\n",
    "url = \"https://drive.google.com/drive/folders/1n9yqq5Gl_HWfND5bTlrCwAOycMDt5EMj\"\n",
    "# ÂÑ≤Â≠òÁöÑË≥áÊñôÂ§æ\n",
    "output_dir = \"recipe_files\"\n",
    "# ‰ª•ÈùúÈªòÊñπÂºè‰∏ãËºâÂà∞ÊåáÂÆöË≥áÊñôÂ§æ‰∏≠\n",
    "gdown.download_folder(\n",
    "    url,\n",
    "    quiet=True,\n",
    "    output=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.converters import MarkdownToDocument, PyPDFToDocument, TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "file_type_router = FileTypeRouter(\n",
    "    mime_types=[\n",
    "        \"text/plain\",\n",
    "        \"application/pdf\",\n",
    "        \"text/markdown\"\n",
    "    ]\n",
    ")\n",
    "text_file_converter = TextFileToDocument()\n",
    "markdown_converter = MarkdownToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "document_joiner = DocumentJoiner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Êï∏ÊìöÊ∏ÖÊ¥ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_cleaner = DocumentCleaner()\n",
    "document_splitter = DocumentSplitter(\n",
    "    split_by=\"word\",\n",
    "    split_length=150,\n",
    "    split_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embedder = SentenceTransformersDocumentEmbedder(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "document_writer = DocumentWriter(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Pipeline()\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=file_type_router, name=\"file_type_router\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=text_file_converter, name=\"text_file_converter\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=markdown_converter, name=\"markdown_converter\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=pdf_converter, name=\"pypdf_converter\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=document_joiner, name=\"document_joiner\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=document_cleaner, name=\"document_cleaner\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=document_splitter, name=\"document_splitter\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=document_embedder, name=\"document_embedder\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=document_writer, name=\"document_writer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x30b4d7910>\n",
       "üöÖ Components\n",
       "  - file_type_router: FileTypeRouter\n",
       "  - text_file_converter: TextFileToDocument\n",
       "  - markdown_converter: MarkdownToDocument\n",
       "  - pypdf_converter: PyPDFToDocument\n",
       "  - document_joiner: DocumentJoiner\n",
       "  - document_cleaner: DocumentCleaner\n",
       "  - document_splitter: DocumentSplitter\n",
       "  - document_embedder: SentenceTransformersDocumentEmbedder\n",
       "  - document_writer: DocumentWriter\n",
       "üõ§Ô∏è Connections\n",
       "  - file_type_router.text/plain -> text_file_converter.sources (List[Path])\n",
       "  - file_type_router.application/pdf -> pypdf_converter.sources (List[Path])\n",
       "  - file_type_router.text/markdown -> markdown_converter.sources (List[Path])\n",
       "  - text_file_converter.documents -> document_joiner.documents (List[Document])\n",
       "  - markdown_converter.documents -> document_joiner.documents (List[Document])\n",
       "  - pypdf_converter.documents -> document_joiner.documents (List[Document])\n",
       "  - document_joiner.documents -> document_cleaner.documents (List[Document])\n",
       "  - document_cleaner.documents -> document_splitter.documents (List[Document])\n",
       "  - document_splitter.documents -> document_embedder.documents (List[Document])\n",
       "  - document_embedder.documents -> document_writer.documents (List[Document])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline.connect(\n",
    "    \"file_type_router.text/plain\", \"text_file_converter.sources\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"file_type_router.application/pdf\", \"pypdf_converter.sources\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"file_type_router.text/markdown\", \"markdown_converter.sources\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"text_file_converter\", \"document_joiner\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"pypdf_converter\", \"document_joiner\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"markdown_converter\", \"document_joiner\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"document_joiner\", \"document_cleaner\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"document_cleaner\", \"document_splitter\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"document_splitter\", \"document_embedder\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"document_embedder\", \"document_writer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‰ª•‰∏äÂÆåÊàêÈ†êËôïÁêÜÁ®ãÂ∫è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‰ª•‰∏ãÊòØÈÄ≤Ë°å RAG Ê∏¨Ë©¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:haystack.core.pipeline.base:Warming up component document_embedder...\n",
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:174: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v3 of SentenceTransformers.\n",
      "  warnings.warn(\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê≠£Âú®ËôïÁêÜÊñá‰ª∂: recipe_files/recipe.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envHaystack/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:haystack.core.pipeline.pipeline:Running component file_type_router\n",
      "INFO:haystack.core.pipeline.pipeline:Running component text_file_converter\n",
      "INFO:haystack.core.pipeline.pipeline:Running component document_joiner\n",
      "INFO:haystack.components.joiners.document_joiner:Some of the Documents DocumentJoiner got have score=None. It was configured to sort Documents by score, so those with score=None were sorted as if they had a score of -infinity.\n",
      "INFO:haystack.core.pipeline.pipeline:Running component document_cleaner\n",
      "INFO:haystack.core.pipeline.pipeline:Running component document_splitter\n",
      "INFO:haystack.core.pipeline.pipeline:Running component document_embedder\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.85it/s]\n",
      "INFO:haystack.core.pipeline.pipeline:Running component document_writer\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Ê∏¨Ë©¶Êñá‰ª∂Ë∑ØÂæëÊòØÂê¶Â≠òÂú®\n",
    "file_path = Path(\"recipe_files/recipe.txt\")\n",
    "if file_path.exists():\n",
    "    print(f\"Ê≠£Âú®ËôïÁêÜÊñá‰ª∂: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Ë®≠ÁΩÆÊó•Ë™åÁ¥öÂà•‰ª•Êü•ÁúãËôïÁêÜÈÅéÁ®ã\n",
    "        import logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        \n",
    "        preprocessing_pipeline.run(\n",
    "            {\"file_type_router\": {\"sources\": [file_path]}}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"ËôïÁêÜÈÅéÁ®ã‰∏≠Âá∫ÁèæÈåØË™§: {e}\")\n",
    "else:\n",
    "    print(f\"Êñá‰ª∂ {file_path} ‰∏çÂ≠òÂú®„ÄÇ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token found in environment variables.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HF_API_TOKEN\"] = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "if \"HF_API_TOKEN\" not in os.environ:\n",
    "    print(\"Hugging Face token not found in environment variables.\")\n",
    "    os.environ[\"HF_API_TOKEN\"] = getpass(\"Enter Hugging Face token:\")\n",
    "else:\n",
    "    print(\"Hugging Face token found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x105e07d90>\n",
       "üöÖ Components\n",
       "  - embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: HuggingFaceAPIGenerator\n",
       "üõ§Ô∏è Connections\n",
       "  - embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import HuggingFaceAPIGenerator\n",
    "\n",
    "template = \"\"\"\n",
    "Ê†πÊìöÁµ¶ÂÆöÁöÑ‰∏ä‰∏ãÊñáÂõûÁ≠îÂïèÈ°å„ÄÇ\n",
    "\n",
    "‰∏ä‰∏ãÊñá:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "ÂïèÈ°å: {{ question }}\n",
    "ÂõûÁ≠î:\n",
    "\"\"\"\n",
    "pipe = Pipeline()\n",
    "# Ê∑ªÂä†\n",
    "pipe.add_component(\n",
    "    \"embedder\",\n",
    "    SentenceTransformersTextEmbedder(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    ")\n",
    "# Ê∑ªÂä†\n",
    "pipe.add_component(\n",
    "    \"retriever\",\n",
    "    InMemoryEmbeddingRetriever(document_store=document_store)\n",
    ")\n",
    "# Ê∑ªÂä†\n",
    "pipe.add_component(\n",
    "    \"prompt_builder\", PromptBuilder(template=template)\n",
    ")\n",
    "# Ê∑ªÂä†\n",
    "pipe.add_component(\n",
    "    \"llm\",\n",
    "    HuggingFaceAPIGenerator(\n",
    "        api_type=\"serverless_inference_api\",\n",
    "        api_params={\"model\": \"HuggingFaceH4/zephyr-7b-beta\"}),\n",
    ")\n",
    "# ÈÄ£Á∑ö\n",
    "pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "pipe.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:haystack.core.pipeline.base:Warming up component embedder...\n",
      "INFO:haystack.core.pipeline.pipeline:Running component embedder\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.50it/s]\n",
      "INFO:haystack.core.pipeline.pipeline:Running component retriever\n",
      "INFO:haystack.core.pipeline.pipeline:Running component prompt_builder\n",
      "INFO:haystack.core.pipeline.pipeline:Running component llm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'replies': [' \\n\\nË¶ÅË£Ω‰ΩúÁ¥îÁ¥†ÈÖÆËåÑÂ≠êÁÉ§ÂØ¨È∫µÊ¢ù„ÄÅÁ¥îÁ¥†ÊüøÂ≠êÈ§°È§ÖÂíåÁ¥îÁ¥†Â§ßÈ∫ªËµ∑Âè∏Ôºå‰Ω†ÈúÄË¶Å‰∏ãÂàóÂéüÊñôÔºö\\n\\n1. Á¥îÁ¥†ÈÖÆËåÑÂ≠êÁÉ§ÂØ¨È∫µÊ¢ùÔºö\\n   - Á¥îÁ¥†ËåÑÂ≠ê\\n   - Á¥îÁ¥†ÂØ¨È∫µ\\n   - Á¥îÁ¥†ÈÖ±Ê≤π\\n   - Á¥îÁ¥†Áõê\\n   - Á¥îÁ¥†ÁîúÂë≥Â≠ê (ÂèØÈÅ∏)\\n\\n2. Á¥îÁ¥†ÊüøÂ≠êÈ§°È§ÖÔºö\\n   - Á¥îÁ¥†ÊüøÂ≠ê\\n   - Á¥îÁ¥†È∫µÁ≤â\\n   - Á¥îÁ¥†ÈÖ±Ê≤π\\n   - Á¥îÁ¥†Áõê\\n   - Á¥îÁ¥†ÁîúÂë≥Â≠ê (ÂèØÈÅ∏)\\n\\n3. Á¥îÁ¥†Â§ßÈ∫ªËµ∑Âè∏Ôºö\\n   - Á¥îÁ¥†Â§ßÈ∫ª\\n   - Á¥îÁ¥†ÈÖ±Ê≤π\\n   - Á¥îÁ¥†Áõê\\n   - Á¥îÁ¥†Ê∞¥\\n   - Á¥îÁ¥†ÁîúÂë≥Â≠ê (ÂèØÈÅ∏)\\n\\nÊ≥®ÊÑèÔºö ÈÄô‰∫õÈ£üË≠òÊòØÁ¥îÁ¥†ÁöÑÔºåÊâÄ‰ª•Ë´ãÁ¢∫‰øùÊâÄÊúâÁöÑÂéüÊñôÈÉΩÊòØÁ¥îÁ¥†ÁöÑ'],\n",
       "  'meta': [{'model': 'HuggingFaceH4/zephyr-7b-beta',\n",
       "    'finish_reason': 'length',\n",
       "    'usage': {'completion_tokens': 350}}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = (\n",
    "    \"Ë£Ω‰ΩúÁ¥îÁ¥†ÈÖÆËåÑÂ≠êÁÉ§ÂØ¨È∫µÊ¢ù„ÄÅÁ¥îÁ¥†ÊüøÂ≠êÈ§°È§ÖÂíåÁ¥îÁ¥†Â§ßÈ∫ªËµ∑Âè∏ÈúÄË¶ÅÂì™‰∫õÂéüÊñôÔºü\"\n",
    ")\n",
    "result = pipe.run(\n",
    "    {\n",
    "        \"embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "        \"llm\": {\"generation_kwargs\": {\"max_new_tokens\": 350}},\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÈÄêË°åËº∏Âá∫ÁîüÊàêÁöÑÊñáÊú¨Ôºå‰∏¶ÂéªÈô§Â§öÈ§òÁöÑÁ©∫ÁôΩÂ≠óÂÖÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÁîüÊàêÁöÑÂéüÊñôÂàóË°®Ôºö\n",
      "\n",
      "\n",
      "\n",
      "Ë¶ÅË£Ω‰ΩúÁ¥îÁ¥†ÈÖÆËåÑÂ≠êÁÉ§ÂØ¨È∫µÊ¢ù„ÄÅÁ¥îÁ¥†ÊüøÂ≠êÈ§°È§ÖÂíåÁ¥îÁ¥†Â§ßÈ∫ªËµ∑Âè∏Ôºå‰Ω†ÈúÄË¶Å‰∏ãÂàóÂéüÊñôÔºö\n",
      "\n",
      "1. Á¥îÁ¥†ÈÖÆËåÑÂ≠êÁÉ§ÂØ¨È∫µÊ¢ùÔºö\n",
      "- Á¥îÁ¥†ËåÑÂ≠ê\n",
      "- Á¥îÁ¥†ÂØ¨È∫µ\n",
      "- Á¥îÁ¥†ÈÖ±Ê≤π\n",
      "- Á¥îÁ¥†Áõê\n",
      "- Á¥îÁ¥†ÁîúÂë≥Â≠ê (ÂèØÈÅ∏)\n",
      "\n",
      "2. Á¥îÁ¥†ÊüøÂ≠êÈ§°È§ÖÔºö\n",
      "- Á¥îÁ¥†ÊüøÂ≠ê\n",
      "- Á¥îÁ¥†È∫µÁ≤â\n",
      "- Á¥îÁ¥†ÈÖ±Ê≤π\n",
      "- Á¥îÁ¥†Áõê\n",
      "- Á¥îÁ¥†ÁîúÂë≥Â≠ê (ÂèØÈÅ∏)\n",
      "\n",
      "3. Á¥îÁ¥†Â§ßÈ∫ªËµ∑Âè∏Ôºö\n",
      "- Á¥îÁ¥†Â§ßÈ∫ª\n",
      "- Á¥îÁ¥†ÈÖ±Ê≤π\n",
      "- Á¥îÁ¥†Áõê\n",
      "- Á¥îÁ¥†Ê∞¥\n",
      "- Á¥îÁ¥†ÁîúÂë≥Â≠ê (ÂèØÈÅ∏)\n",
      "\n",
      "Ê≥®ÊÑèÔºö ÈÄô‰∫õÈ£üË≠òÊòØÁ¥îÁ¥†ÁöÑÔºåÊâÄ‰ª•Ë´ãÁ¢∫‰øùÊâÄÊúâÁöÑÂéüÊñôÈÉΩÊòØÁ¥îÁ¥†ÁöÑ\n"
     ]
    }
   ],
   "source": [
    "# ÂèñÂæóÁîüÊàêÁöÑÊñáÊú¨\n",
    "response = result[\"llm\"][\"replies\"][0]\n",
    "\n",
    "# Ê†ºÂºèÂåñÊñáÊú¨‰∏¶Ëº∏Âá∫\n",
    "print(\"ÁîüÊàêÁöÑÂéüÊñôÂàóË°®Ôºö\\n\")\n",
    "lines = response.split(\"\\n\")\n",
    "for line in lines:\n",
    "    print(line.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envHaystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
