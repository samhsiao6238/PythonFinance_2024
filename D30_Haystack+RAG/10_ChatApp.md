# Chat App

![](images/img_35.png)

<br>

## èªªæ˜

1. é€™æ˜¯å®˜æ–¹åœ¨ `2024/04/25` ç™¼å¸ƒçš„ [å®˜æ–¹æ•™ç¨‹](https://haystack.deepset.ai/tutorials/40_building_chat_application_with_function_calling)ï¼Œæ§‹å»ºä¸€å€‹å…·æœ‰ `å‡½æ•¸èª¿ç”¨` åŠŸèƒ½çš„ `èŠå¤©æ‡‰ç”¨ç¨‹åº` ã€‚

<br>

2. ä½¿ç”¨åˆ°çš„çµ„ä»¶åŒ…å« `InMemoryDocumentStore`ã€`SentenceTransformersDocumentEmbedder`ã€`SentenceTransformersTextEmbedder`ã€`InMemoryEmbeddingRetriever`ã€`PromptBuilder`ã€`OpenAIGenerator`ã€`OpenAIChatGenerator`ï¼Œå¦å¤–ä¹Ÿæœƒä½¿ç”¨ `OpenAI API`ã€‚

<br>

3. é€™å€‹ç¯„ä¾‹çš„ç›®çš„æ˜¯ä½¿ç”¨ `OpenAI` çš„ `å‡½æ•¸èª¿ç”¨åŠŸèƒ½ `ä¾†æ§‹å»ºå…·å‚™ `é¡ä»£ç†è¡Œç‚º` çš„èŠå¤©æ‡‰ç”¨ç¨‹åºï¼Œå°‡ Haystack ç®¡é“è½‰æ›ç‚ºå‡½æ•¸èª¿ç”¨å·¥å…·ï¼Œä»¥åŠä½¿ç”¨ OpenAI çš„ Chat Completion API é€šé OpenAIChatGenerator ä¾†å¯¦ç¾é¡ä»£ç†è¡Œç‚ºçš„æ‡‰ç”¨ç¨‹åºã€‚ç›¸é—œæ–‡ä»¶å¯åƒè€ƒ Haystack çš„ [OpenAIChatGenerator æ–‡ä»¶](https://docs.haystack.deepset.ai/docs/openaichatgenerator)ã€‚

<br>

4. OpenAI çš„ `å‡½æ•¸èª¿ç”¨åŠŸèƒ½` å°‡ `LLM` é€£æ¥åˆ°å¤–éƒ¨å·¥å…·ï¼Œé€šéå‘ `OpenAI API` èª¿ç”¨æä¾›å‡½æ•¸åˆ—è¡¨åŠå…¶è¦ç¯„å¯è¼•é¬†æ§‹å»ºèŠå¤©åŠ©æ‰‹ï¼Œé€™äº›åŠ©æ‰‹å¯ä»¥é€šéèª¿ç”¨å¤–éƒ¨ API ä¾†å›ç­”å•é¡Œæˆ–å¾æ–‡æœ¬ä¸­æå–çµæ§‹åŒ–ä¿¡æ¯ã€‚

<br>

## é€²è¡Œé–‹ç™¼

1. å®‰è£ Haystack 2.0 å’Œ `sentence-transformers`ã€‚

    ```bash
    pip install haystack-ai "sentence-transformers>=2.2.0"
    ```

<br>

2. ä¿å­˜ OpenAI API Key ç‚ºç’°å¢ƒè®Šé‡ã€‚

    ```python
    from getpass import getpass
    import os
    from dotenv import load_dotenv

    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    load_dotenv()
    # å…©å€‹ API çš„å¯†é‘°
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

    if "OPENAI_API_KEY" not in os.environ:
        os.environ["OPENAI_API_KEY"] = getpass("Enter OpenAI API key:")
    ```

<br>

## OpenAIChatGenerator vs OpenAIGenerator

1. `OpenAIChatGenerator` æ˜¯æ”¯æŒé€šé `Chat Completion API` ä½¿ç”¨ `OpenAI` çš„å‡½æ•¸èª¿ç”¨åŠŸèƒ½çš„çµ„ä»¶ï¼Œèˆ‡ `OpenAIGenerator` ä¸åŒï¼Œ`OpenAIChatGenerator` æ˜¯é€šé `ChatMessage` åˆ—è¡¨ä¾†é€²è¡Œé€šä¿¡çš„ã€‚

<br>

2. å¯¦ä½œä¸Šï¼Œä½¿ç”¨ `ChatMessage.from_system()` å‰µå»ºä¸€å€‹å…·æœ‰ `SYSTEM` è§’è‰²çš„ `ChatMessage` å°è±¡ï¼Œç„¶å¾Œä½¿ç”¨ `ChatMessage.from_user()` å‰µå»ºå¦ä¸€å€‹å…·æœ‰ `USER` è§’è‰²çš„ `ChatMessage`ã€‚æ¥è‘—ï¼Œå°‡é€™äº›æ¶ˆæ¯åˆ—è¡¨å‚³éçµ¦ `OpenAIChatGenerator` ä¸¦é‹è¡Œã€‚

    ```python
    from haystack.dataclasses import ChatMessage
    from haystack.components.generators.chat import OpenAIChatGenerator

    # å‰µå»ºç³»çµ±æ¶ˆæ¯å’Œç”¨æˆ¶æ¶ˆæ¯çš„ ChatMessage å°è±¡
    messages = [
        ChatMessage.from_system(
            "å³ä½¿æŸäº›è¼¸å…¥è³‡æ–™æ¡ç”¨å…¶ä»–èªè¨€ï¼Œä¹Ÿå§‹çµ‚ä»¥ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚"
        ),
        ChatMessage.from_user(
            "ä»€éº¼æ˜¯è‡ªç„¶èªè¨€è™•ç†ï¼Ÿè¦ç°¡æ½”ã€‚"
        ),
    ]

    # åˆå§‹åŒ– OpenAIChatGenerator
    chat_generator = OpenAIChatGenerator(model="gpt-4-turbo")
    # å‚³å…¥æ¶ˆæ¯ä¸¦é‹è¡Œ
    chat_generator.run(messages=messages)
    ```

<br>

3. è¼¸å‡ºå¦‚ä¸‹çµæœã€‚

    ```python
    {
        "replies": [
            ChatMessage(
                content="è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒä½¿è¨ˆç®—æ©Ÿèƒ½å¤ ç†è§£ã€è§£é‡‹å’Œç”Ÿæˆäººé¡èªè¨€ã€‚ä¸»è¦æ‡‰ç”¨åŒ…æ‹¬èªéŸ³è­˜åˆ¥ã€èªè¨€ç¿»è­¯å’Œæƒ…æ„Ÿåˆ†æã€‚",
                role=<ChatRole.ASSISTANT: "assistant">,
                name=None,
                meta={
                    "model": "gpt-4-turbo-2024-04-09",
                    "index": 0,
                    "finish_reason": "stop",
                    "usage": {
                        "completion_tokens": 82,
                        "prompt_tokens": 67,
                        "total_tokens": 149
                    }
                }
            )
        ]
    }
    ```

<br>

## åŸºæœ¬æµå¼è™•ç†

1. `OpenAIChatGenerator` æ”¯æŒæµå¼è™•ç†ï¼Œä»¥ä¸‹æä¾›ä¸€å€‹ `streaming_callback` å‡½æ•¸ä¸¦é‡æ–°é‹è¡Œ `chat_generator` ä¾†æŸ¥çœ‹å·®ç•°ã€‚

    ```python
    from haystack.dataclasses import ChatMessage
    from haystack.components.generators.chat import OpenAIChatGenerator
    from haystack.components.generators.utils import print_streaming_chunk

    # ä½¿ç”¨æµå¼å›èª¿å‡½æ•¸åˆå§‹åŒ– OpenAIChatGenerator
    chat_generator = OpenAIChatGenerator(
        model="gpt-4-turbo",
        streaming_callback=print_streaming_chunk
    )
    # å‚³å…¥æ¶ˆæ¯ä¸¦é‹è¡Œ
    response = chat_generator.run(messages=messages)
    ```

<br>

## å¾ Haystack ç®¡é“å‰µå»ºå‡½æ•¸èª¿ç”¨å·¥å…·

1. è¦ä½¿ç”¨ `OpenAI` çš„ `å‡½æ•¸èª¿ç”¨åŠŸèƒ½`ï¼Œéœ€è¦é€šé `generation_kwargs` åƒæ•¸å°‡å·¥å…·ä»‹ç´¹çµ¦ `OpenAIChatGenerator`ï¼Œæœ¬ç¯„ä¾‹ä½¿ç”¨ `Haystack RAG` ç®¡é“ä½œç‚ºå·¥å…·ä¹‹ä¸€ï¼Œå› æ­¤éœ€è¦å°‡æ–‡ä»¶ç´¢å¼•åˆ°æ–‡ä»¶å„²å­˜ä¸­ï¼Œç„¶å¾Œåœ¨å…¶ä¸Šæ§‹å»º RAG ç®¡é“ã€‚

<br>

2. å‰µå»ºä¸€å€‹ç®¡é“ï¼Œå°‡ç¯„ä¾‹æ•¸æ“šé›†å„²å­˜åˆ° `InMemoryDocumentStore` ä¸­ï¼Œä¸¦ä½¿ç”¨ `SentenceTransformersDocumentEmbedder` ä¾†ç”Ÿæˆæ–‡ä»¶çš„åµŒå…¥ï¼Œç„¶å¾Œä½¿ç”¨ `DocumentWriter` å°‡å®ƒå€‘å¯«å…¥ `æ–‡ä»¶å„²å­˜`ä¸­ï¼Œå°‡é€™äº›çµ„ä»¶æ·»åŠ åˆ°ç®¡é“å¾Œï¼Œå°‡å®ƒå€‘é€£æ¥ä¸¦é‹è¡Œç®¡é“ã€‚

    ```python
    from haystack import Pipeline, Document
    from haystack.document_stores.in_memory import InMemoryDocumentStore
    from haystack.components.writers import DocumentWriter
    from haystack.components.embedders import SentenceTransformersDocumentEmbedder

    # å‰µå»ºæ–‡ä»¶
    documents = [
        Document(content="æˆ‘çš„åå­—æ˜¯ Jeanï¼Œæˆ‘ä½åœ¨ Parisã€‚"),
        Document(content="æˆ‘çš„åå­—æ˜¯ Markï¼Œæˆ‘ä½åœ¨ Berlinã€‚"),
        Document(content="æˆ‘çš„åå­—æ˜¯ Giorgioï¼Œæˆ‘ä½åœ¨ Romeã€‚"),
        Document(content="æˆ‘çš„åå­—æ˜¯ Martaï¼Œæˆ‘ä½åœ¨ Madridã€‚"),
        Document(content="æˆ‘çš„åå­—æ˜¯ Harryï¼Œæˆ‘ä½åœ¨ Londonã€‚"),
    ]

    # åˆå§‹åŒ–å…§å­˜æ–‡ä»¶å„²å­˜
    document_store = InMemoryDocumentStore()

    # å‰µå»ºç´¢å¼•ç®¡é“
    indexing_pipeline = Pipeline()
    indexing_pipeline.add_component(
        instance=SentenceTransformersDocumentEmbedder(model="sentence-transformers/all-MiniLM-L6-v2"), name="doc_embedder"
    )
    indexing_pipeline.add_component(
        instance=DocumentWriter(document_store=document_store),
        name="doc_writer"
    )

    # é€£æ¥åµŒå…¥å™¨å’Œæ–‡ä»¶å¯«å…¥å™¨
    indexing_pipeline.connect(
        "doc_embedder.documents",
        "doc_writer.documents"
    )

    # é‹è¡Œç®¡é“
    indexing_pipeline.run({
        "doc_embedder": {"documents": documents}
    })
    ```

<br>

3. é‹è¡Œå¾Œé¡¯ç¤ºã€‚

    ![](images/img_36.png)

<br>

## æ§‹å»º RAG ç®¡é“

1. ä½¿ç”¨ `SentenceTransformersTextEmbedder`ã€`InMemoryEmbeddingRetriever`ã€`PromptBuilder` å’Œ `OpenAIGenerator` æ§‹å»ºåŸºæœ¬çš„æª¢ç´¢å¢å¼·ç”Ÿæˆç®¡é“ã€‚

    ```python
    from haystack.components.embedders import SentenceTransformersTextEmbedder
    from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
    from haystack.components.builders import PromptBuilder
    from haystack.components.generators import OpenAIGenerator

    # å®šç¾©æç¤ºæ¨¡æ¿
    template = """
    æ ¹æ“šçµ¦å®šçš„ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚

    ä¸Šä¸‹æ–‡:
    {% for document in documents %}
        {{ document.content }}
    {% endfor %}
    å•é¡Œ: {{ question }}
    ç­”æ¡ˆ:
    """

    # å‰µå»º RAG ç®¡é“
    rag_pipe = Pipeline()
    rag_pipe.add_component(
        "embedder",
        SentenceTransformersTextEmbedder(
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
    )
    rag_pipe.add_component(
        "retriever",
        InMemoryEmbeddingRetriever(
            document_store=document_store
        )
    )
    rag_pipe.add_component(
        "prompt_builder",
        PromptBuilder(
            template=template
        )
    )
    rag_pipe.add_component(
        "llm",
        OpenAIGenerator(model="gpt-4-turbo")
    )

    # é€£æ¥çµ„ä»¶
    rag_pipe.connect(
        "embedder.embedding",
        "retriever.query_embedding"
    )
    rag_pipe.connect(
        "retriever",
        "prompt_builder.documents"
    )
    rag_pipe.connect(
        "prompt_builder",
        "llm"
    )
    ```

<br>

2. æœƒè¼¸å‡ºä»¥ä¸‹è¨Šæ¯ã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x3169ca710>

    ğŸš… Components
        - embedder: SentenceTransformersTextEmbedder
        - retriever: InMemoryEmbeddingRetriever
        - prompt_builder: PromptBuilder
        - llm: OpenAIGenerator

    ğŸ›¤ï¸ Connections
        - embedder.embedding -> retriever.query_embedding (List[float])
        - retriever.documents -> prompt_builder.documents (List[Document])
        - prompt_builder.prompt -> llm.prompt (str)
    ```

<br>

## é‹è¡Œç®¡é“

1. ä½¿ç”¨ä¸€å€‹æŸ¥è©¢ä¾†æ¸¬è©¦ç®¡é“ï¼Œä¸¦ç¢ºä¿å®ƒæŒ‰ç…§é æœŸå·¥ä½œï¼Œç„¶å¾Œå†å°‡å…¶ç”¨ä½œå‡½æ•¸èª¿ç”¨å·¥å…·ã€‚

    ```python
    query = "Mark ä½åœ¨å“ªè£¡ï¼Ÿ"
    rag_pipe.run({
        "embedder": {"text": query},
        "prompt_builder": {"question": query}
    })
    ```

<br>

2. æœƒé¡¯ç¤ºä»¥ä¸‹çµæœã€‚

    ![](images/img_37.png)

<br>

## å°‡ Haystack ç®¡é“è½‰æ›ç‚ºå·¥å…·

1. å°‡ `rag_pipe.run` èª¿ç”¨åŒ…è£æˆä¸€å€‹å‡½æ•¸ `rag_pipeline_func`ï¼Œé€™å€‹ `rag_pipeline_func` å‡½æ•¸å°‡æ¥å—ä¸€å€‹æŸ¥è©¢ä¸¦è¿”å›ä¾†è‡ª RAG ç®¡é“çš„ LLM çš„å›æ‡‰ï¼Œç„¶å¾Œå¯å°‡æ­¤å‡½æ•¸ä½œç‚ºå·¥å…·å¼•å…¥åˆ° `OpenAIChatGenerator` ä¸­ã€‚

    ```python
    def rag_pipeline_func(query: str):
        result = rag_pipe.run({
            "embedder": {"text": query},
            "prompt_builder": {"question": query}
        })
        return {"reply": result["llm"]["replies"][0]}
    ```

<br>

## å‰µå»ºå·¥å…·åˆ—è¡¨

1. é™¤äº† `rag_pipeline_func` å·¥å…·å¤–ï¼Œé‚„å‰µå»ºä¸€å€‹åç‚º `get_current_weather` çš„æ–°å·¥å…·ï¼Œç”¨æ–¼ç²å– `åŸå¸‚çš„å¤©æ°£ä¿¡æ¯`ï¼Œä»¥ä¸‹å‡½æ•¸ä¸­ä½¿ç”¨ç¡¬ç·¨ç¢¼çš„æ•¸æ“šä¾†å±•ç¤ºåŠŸèƒ½ã€‚

    ```python
    WEATHER_INFO = {
        "Berlin": {
            "weather": "mostly sunny", "temperature": 7, "unit": "celsius"
        },
        "Paris": {
            "weather": "mostly cloudy", "temperature": 8, "unit": "celsius"
        },
        "Rome": {
            "weather": "sunny", "temperature": 14, "unit": "celsius"
        },
        "Madrid": {
            "weather": "sunny", "temperature": 10, "unit": "celsius"
        },
        "London": {
            "weather": "cloudy", "temperature": 9, "unit": "celsius"
        },
    }

    def get_current_weather(location: str):
        if location in WEATHER_INFO:
            return WEATHER_INFO[location]
        else:
            # å›é€€æ•¸æ“š
            return {
                "weather": "sunny",
                "temperature": 21.8,
                "unit": "fahrenheit"
            }
    ```

<br>

2. æ¥ä¸‹ä¾†ï¼ŒæŒ‰ç…§ `OpenAI` çš„å·¥å…·æ¶æ§‹ç‚º `rag_pipeline_func` å’Œ `get_current_weather` æ·»åŠ å‡½æ•¸èªªæ˜ã€‚è©³ç´°èªªæ˜ `rag_pipeline_func` å’Œ `query`ï¼Œä»¥ä¾¿ OpenAI å¯ä»¥ç”Ÿæˆé©ç•¶çš„åƒæ•¸ã€‚

    ```python
    tools = [
        {
            "type": "function",
            "function": {
                "name": "rag_pipeline_func",
                "description": "ç²å–æœ‰é—œäººå€‘å±…ä½åœ°é»çš„ä¿¡æ¯",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "æœå°‹ä¸­ä½¿ç”¨çš„æŸ¥è©¢ã€‚å¾ç”¨æˆ¶çš„æ¶ˆæ¯ä¸­æ¨æ–·å‡ºé€™ä¸€é»ã€‚å®ƒæ‡‰è©²æ˜¯ä¸€å€‹å•é¡Œæˆ–ä¸€å€‹é™³è¿°ã€‚",
                        }
                    },
                    "required": ["query"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "å–å¾—ç•¶å‰å¤©æ°£",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚åŠ å·èˆŠé‡‘å±±"
                        }
                    },
                    "required": ["location"],
                },
            },
        },
    ]
    ```

<br>

## ä½¿ç”¨å·¥å…·é‹è¡Œ OpenAIChatGenerator

1. è¦ä½¿ç”¨å‡½æ•¸èª¿ç”¨åŠŸèƒ½ï¼Œæ‚¨éœ€è¦åœ¨ `OpenAIChatGenerator` çš„ `run()` æ–¹æ³•ä¸­å‚³å…¥å·¥å…·åˆ—è¡¨ä½œç‚º `generation_kwargs`ã€‚ç”¨ç³»çµ±æ¶ˆæ¯æŒ‡å°æ¨¡å‹ä½¿ç”¨æä¾›çš„å·¥å…·ï¼Œç„¶å¾Œä½œç‚ºç”¨æˆ¶æ¶ˆæ¯æä¾›ä¸€å€‹éœ€è¦å‡½æ•¸èª¿ç”¨çš„æŸ¥è©¢ã€‚

    ```python
    from haystack.dataclasses import ChatMessage
    from haystack.components.generators.chat import OpenAIChatGenerator
    from haystack.components.generators.utils import print_streaming_chunk

    # å‰µå»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç³»çµ±æ¶ˆæ¯å’Œç”¨æˆ¶æŸ¥è©¢
    messages = [
        ChatMessage.from_system(
            "ä¸è¦å‡è¨­å°‡å“ªäº›å€¼æ’å…¥å‡½æ•¸ä¸­ã€‚å¦‚æœç”¨æˆ¶è¦æ±‚ä¸æ˜ç¢ºï¼Œè«‹è¦æ±‚æ¾„æ¸…ã€‚"
        ),
        ChatMessage.from_user("ä½ èƒ½å‘Šè¨´æˆ‘ Mark ä½åœ¨å“ªè£¡å—ï¼Ÿ"),
    ]

    # åˆå§‹åŒ– OpenAIChatGenerator
    chat_generator = OpenAIChatGenerator(
        model="gpt-4-turbo",
        streaming_callback=print_streaming_chunk
    )
    # å‚³å…¥æ¶ˆæ¯å’Œå·¥å…·åˆ—è¡¨ä¸¦é‹è¡Œ
    response = chat_generator.run(
        messages=messages,
        generation_kwargs={"tools": tools}
    )
    # è¼¸å‡ºæŸ¥çœ‹
    print(response)
    ```

<br>

2. å°‡ç²å¾—ä¸€å€‹ä»¥ JSON æ ¼å¼é¡¯ç¤ºä¸”åŒ…å« `å·¥å…·åç¨±` å’Œ `åƒæ•¸` çš„ `ChatMessage`ã€‚

    ```python
    {
        'replies': [
            ChatMessage(
                content='[{"index": 0, "id": "call_2ARyzeivzPqS0ETx3jVHt4ct", "function": {"arguments": "{\\"query\\":\\"Where does Mark live?\\"}", "name": "rag_pipeline_func"}, "type": "function"}]',
                role=<ChatRole.ASSISTANT: 'assistant'>,
                name=None,
                meta={
                    'model': 'gpt-4-turbo-2024-04-09',
                    'index': 0,
                    'finish_reason': 'tool_calls',
                    'usage': {}
                }
            )
        ]
    }
    ```

<br>

3. æ¥ä¸‹ä¾†å°‡æ¶ˆæ¯å…§å®¹å­—ä¸²è§£æç‚º JSON ä¸¦ä½¿ç”¨æä¾›çš„åƒæ•¸èª¿ç”¨ç›¸æ‡‰çš„å‡½æ•¸ã€‚

    ```python
    import json

    # è§£æå‡½æ•¸èª¿ç”¨ä¿¡æ¯
    # æå–ç¬¬ä¸€å€‹å›æ‡‰ä¸­çš„ content
    content = response['replies'][0].content

    # å°‡ content è§£æç‚º JSON
    # content æ˜¯ä¸€å€‹ JSON å­—ä¸²ï¼Œéœ€è¦è½‰æ›ç‚º Python å­—å…¸
    function_calls = json.loads(content)

    # æå–ç¬¬ä¸€å€‹å‡½æ•¸èª¿ç”¨ä¿¡æ¯
    # æå–å‡½æ•¸èª¿ç”¨åˆ—è¡¨ä¸­çš„ç¬¬ä¸€å€‹å…ƒç´ 
    function_call = function_calls[0]

    # ç²å–å‡½æ•¸åç¨±
    # ç²å–å‡½æ•¸åç¨±ï¼Œé€™æ˜¯æˆ‘å€‘éœ€è¦èª¿ç”¨çš„å‡½æ•¸
    function_name = function_call['function']['name']

    # è§£æå‡½æ•¸åƒæ•¸
    # å°‡åƒæ•¸è§£æç‚ºå­—å…¸æ ¼å¼
    function_args = json.loads(function_call['function']['arguments'])

    # æ‰“å°å‡½æ•¸åç¨±å’Œåƒæ•¸
    print("Function Name:", function_name)
    print("Function Arguments:", function_args)

    # å®šç¾©å¯ç”¨çš„å‡½æ•¸
    def rag_pipeline_func(query: str):
        # é€™è£¡å‡è¨­ä½ çš„ `rag_pipeline_func` å‡½æ•¸å®šç¾©
        return {"reply": f"Mark lives in Berlin, query was: {query}"}

    def get_current_weather(location: str):
        # é€™è£¡å‡è¨­ä½ çš„ `get_current_weather` å‡½æ•¸å®šç¾©
        return {"weather": "sunny", "temperature": 20, "location": location}

    # å¯ç”¨å‡½æ•¸å­—å…¸
    available_functions = {
        "rag_pipeline_func": rag_pipeline_func,
        "get_current_weather": get_current_weather
    }

    # æŸ¥æ‰¾ç›¸æ‡‰çš„å‡½æ•¸ä¸¦ä½¿ç”¨çµ¦å®šçš„åƒæ•¸èª¿ç”¨å®ƒ
    if function_name in available_functions:
        # æ ¹æ“šå‡½æ•¸åç¨±æ‰¾åˆ°å°æ‡‰çš„å‡½æ•¸
        function_to_call = available_functions[function_name]
        # ä½¿ç”¨è§£åŒ…æ“ä½œå°‡åƒæ•¸å‚³éçµ¦å‡½æ•¸
        function_response = function_to_call(**function_args)
        # æ‰“å°å‡½æ•¸çš„è¿”å›å€¼
        print("Function Response:", function_response)
    else:
        # å¦‚æœå‡½æ•¸åç¨±æœªæ‰¾åˆ°ï¼Œæ‰“å°éŒ¯èª¤è¨Šæ¯
        print(f"Function {function_name} not found.")
    ```

<br>

4. å¾—åˆ°ä»¥ä¸‹çš„è¼¸å‡ºçµæœã€‚

    ```bash
    Function Name: rag_pipeline_func
    Function Arguments: {'query': 'Mark lives in which location?'}
    Function Response: {'reply': 'Mark lives in Berlin, query was: Mark lives in which location?'}
    ```

<br>

5. æœ€å¾Œä¸€æ­¥ï¼Œé€šéå°‡å‡½æ•¸å›æ‡‰é™„åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä½œç‚ºæ–°æ¶ˆæ¯ï¼Œä½¿ç”¨ `ChatMessage.from_function()` ä¸¦é‡æ–°é‹è¡Œ `OpenAIChatGenerator`ï¼Œè®“æ¨¡å‹ç¸½çµçµæœã€‚

    ```python
    from haystack.dataclasses import ChatMessage

    # å‰µå»ºå‡½æ•¸å›æ‡‰æ¶ˆæ¯
    function_message = ChatMessage.from_function(
        content=json.dumps(function_response),
        name=function_name
    )
    # å°‡å‡½æ•¸å›æ‡‰æ¶ˆæ¯æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
    messages.append(function_message)

    # å†æ¬¡é‹è¡Œ OpenAIChatGenerator
    response = chat_generator.run(
        messages=messages,
        generation_kwargs={"tools": tools}
    )
    ```

<br>

6. è¼¸å‡ºã€‚

    ![](images/img_38.png)

<br>

## æ§‹å»ºèŠå¤©æ‡‰ç”¨ç¨‹åº

1. `OpenAI Chat Completions API` ä¸¦ä¸æœƒç›´æ¥èª¿ç”¨å‡½æ•¸ï¼›ç›¸åï¼Œæ¨¡å‹æœƒç”Ÿæˆå¯ä»¥åœ¨ä»£ç¢¼ä¸­èª¿ç”¨çš„ JSONã€‚å› æ­¤ï¼Œç‚ºäº†æ§‹å»º `ç«¯åˆ°ç«¯` çš„èŠå¤©æ‡‰ç”¨ç¨‹åºï¼Œéœ€è¦åœ¨æ¯æ¬¡æ¶ˆæ¯ä¸­æª¢æŸ¥ `OpenAI` å›æ‡‰æ˜¯å¦ç‚º `å·¥å…·èª¿ç”¨`ã€‚å¦‚æœæ˜¯ï¼Œéœ€è¦ä½¿ç”¨æä¾›çš„åƒæ•¸èª¿ç”¨ç›¸æ‡‰çš„å‡½æ•¸ï¼Œä¸¦å°‡å‡½æ•¸å›æ‡‰ç™¼é€å› OpenAIã€‚å¦å‰‡ï¼Œå°‡ç”¨æˆ¶å’Œæ¶ˆæ¯éƒ½é™„åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­ï¼Œèˆ‡æ¨¡å‹é€²è¡Œå¸¸è¦å°è©±ã€‚

<br>

2. è¦ç‚ºæ‡‰ç”¨ç¨‹åºæ§‹å»ºä¸€å€‹æ¼‚äº®çš„ç”¨æˆ¶ç•Œé¢ï¼Œå¯ä»¥ä½¿ç”¨å¸¶æœ‰èŠå¤©ç•Œé¢çš„ `Gradio`ã€‚å®‰è£ `gradio`ï¼Œé‹è¡Œä¸‹é¢çš„ä»£ç¢¼å–®å…ƒï¼Œä¸¦ä½¿ç”¨è¼¸å…¥æ¡†èˆ‡å…·æœ‰è¨ªå•æ¬Šé™çš„èŠå¤©æ‡‰ç”¨ç¨‹åºé€²è¡Œäº¤äº’ã€‚

    ```bash
    pip install gradio
    ```

<br>

3. é›–ç„¶ `gradio` å·²æˆåŠŸå®‰è£ï¼Œä½†å‡ºç¾äº†ä¸€äº›ä¾è³´æ€§è¡çªï¼Œé€™æ˜¯å› ç‚º `farm-haystack` éœ€è¦çš„ `pydantic` ç‰ˆæœ¬èˆ‡ `gradio` éœ€è¦çš„ç‰ˆæœ¬ä¸ç›¸å®¹ï¼Œ`farm-haystack` è¦æ±‚ pydantic çš„ç‰ˆæœ¬å¿…é ˆ `å°æ–¼ 2`ï¼Œä½†å®‰è£çš„ pydantic ç‰ˆæœ¬æ˜¯ `2.7.3`ï¼Œé€™è£¡å…ˆå¿½ç•¥ä¸ç®¡ï¼Œçœ‹çœ‹å¾ŒçºŒé‹ä½œæƒ…æ³å†ä¾†è™•ç†ã€‚

    ![](images/img_39.png)

<br>

4. å¯ä»¥å˜—è©¦çš„æŸ¥è©¢ã€‚

    ```bash
    "ç‘å…¸çš„é¦–éƒ½æ˜¯ä»€éº¼ï¼Ÿ"ï¼šä¸€å€‹åŸºæœ¬çš„æŸ¥è©¢ï¼Œæ²’æœ‰ä»»ä½•å‡½æ•¸èª¿ç”¨ã€‚
    "ä½ èƒ½å‘Šè¨´æˆ‘ Giorgio ä½åœ¨å“ªè£¡å—ï¼Ÿ"ï¼šä¸€å€‹åŸºæœ¬çš„æŸ¥è©¢ï¼Œå¸¶æœ‰ä¸€æ¬¡å‡½æ•¸èª¿ç”¨ã€‚
    "é¦¬å¾·é‡Œçš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ"ã€"é‚£è£¡ç¾åœ¨æ˜¯æ™´å¤©å—ï¼Ÿ"ï¼šæŸ¥çœ‹æ¶ˆæ¯æ˜¯å¦è¢«è¨˜éŒ„ä¸¦ç™¼é€ã€‚
    "Jean ä½çš„åœ°æ–¹å¤©æ°£æ€éº¼æ¨£ï¼Ÿ"ï¼šå¼·åˆ¶èª¿ç”¨å…©æ¬¡å‡½æ•¸ã€‚
    "ä»Šå¤©çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ"ï¼šå¼·åˆ¶ OpenAI è©¢å•æ›´å¤šæ¾„æ¸…å•é¡Œã€‚
    ```

<br>

5. ç¨‹å¼ç¢¼ã€‚

    ```python
    import gradio as gr
    import json

    from haystack.dataclasses import ChatMessage
    from haystack.components.generators.chat import OpenAIChatGenerator

    chat_generator = OpenAIChatGenerator(model="gpt-3.5-turbo")
    response = None
    messages = [
        ChatMessage.from_system(
            "ä¸è¦å‡è¨­å°‡å“ªäº›å€¼æ’å…¥å‡½æ•¸ä¸­ã€‚"
            "å¦‚æœç”¨æˆ¶è¦æ±‚ä¸æ˜ç¢ºï¼Œè«‹è¦æ±‚æ¾„æ¸…ã€‚"
        )
    ]

    # å®šç¾©èŠå¤©æ©Ÿå™¨äººå‡½æ•¸
    def chatbot_with_fc(message, history):
        messages.append(ChatMessage.from_user(message))
        response = chat_generator.run(messages=messages, generation_kwargs={"tools": tools})

        while True:
            # å¦‚æœ OpenAI å›æ‡‰æ˜¯ä¸€å€‹å·¥å…·èª¿ç”¨
            if response and response["replies"][0].meta["finish_reason"] == "tool_calls":
                function_calls = json.loads(response["replies"][0].content)
                print(response["replies"][0])
                for function_call in function_calls:
                    # è§£æå‡½æ•¸èª¿ç”¨ä¿¡æ¯
                    function_name = function_call["function"]["name"]
                    function_args = json.loads(function_call["function"]["arguments"])

                    # æŸ¥æ‰¾ç›¸æ‡‰çš„å‡½æ•¸ä¸¦ä½¿ç”¨çµ¦å®šçš„åƒæ•¸èª¿ç”¨å®ƒ
                    function_to_call = available_functions[function_name]
                    function_response = function_to_call(function_args)

                    # ä½¿ç”¨ `ChatMessage.from_function` å°‡å‡½æ•¸å›æ‡‰æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
                    messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))
                    response = chat_generator.run(messages=messages, generation_kwargs={"tools": tools})

            # å¸¸è¦å°è©±
            else:
                messages.append(response["replies"][0])
                break
        return response["replies"][0].content

    # å‰µå»ºèŠå¤©ç•Œé¢
    demo = gr.ChatInterface(
        fn=chatbot_with_fc,
        # é¡¯ç¤ºåœ¨ä¸‹æ–¹çš„ç¯„ä¾‹æ¬„ä½
        examples=[
            "ä½ èƒ½å‘Šè¨´æˆ‘ Giorgio ä½åœ¨å“ªè£¡å—ï¼Ÿ",
            "Madrid çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ",
            "èª°ä½åœ¨ London?",
            "Mark ä½çš„åœ°æ–¹çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ",
        ],
        title="è«‹è©¢å•æœ‰é—œå¤©æ°£æˆ–äººå€‘å±…ä½çš„åœ°æ–¹ã€‚",
    )
    ```

<br>

6. å•Ÿå‹•èŠå¤©è¦–çª—ã€‚

    ```python
    # å•Ÿå‹•èŠå¤©æ‡‰ç”¨ç¨‹åº
    demo.launch()
    ```

<br>

7. å•Ÿå‹• `Gradio` çš„è¦–çª—ç•Œé¢ã€‚

    ![](images/img_40.png)

<br>

___

_END_