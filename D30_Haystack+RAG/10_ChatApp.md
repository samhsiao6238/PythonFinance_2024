# Chat App

![](images/img_35.png)

_Building a Chat Application with Function Calling_

<br>

## èªªæ˜

1. é€™æ˜¯å®˜æ–¹åœ¨ `2024/04/25` ç™¼å¸ƒçš„ [å®˜æ–¹æ•™ç¨‹](https://haystack.deepset.ai/tutorials/40_building_chat_application_with_function_calling)ï¼Œå»ºç«‹ä¸€å€‹å…·æœ‰ `å‡½æ•¸èª¿ç”¨` åŠŸèƒ½çš„ `èŠå¤©æ‡‰ç”¨ç¨‹åº` ã€‚

<br>

2. ä½¿ç”¨åˆ°çš„çµ„ä»¶åŒ…å« `InMemoryDocumentStore`ã€`SentenceTransformersDocumentEmbedder`ã€`SentenceTransformersTextEmbedder`ã€`InMemoryEmbeddingRetriever`ã€`PromptBuilder`ã€`OpenAIGenerator`ã€`OpenAIChatGenerator`ï¼Œå¦å¤–ä¹Ÿæœƒä½¿ç”¨ `OpenAI API`ã€‚

<br>

3. é€™å€‹ç¯„ä¾‹çš„ç›®çš„æ˜¯ä½¿ç”¨ `OpenAI` çš„ `å‡½æ•¸èª¿ç”¨åŠŸèƒ½ `ä¾†å»ºç«‹å…·å‚™ `é¡ä»£ç†è¡Œç‚º` çš„èŠå¤©æ‡‰ç”¨ç¨‹åºï¼Œå°‡ Haystack ç®¡é“è½‰æ›ç‚ºå‡½æ•¸èª¿ç”¨å·¥å…·ï¼Œä»¥åŠä½¿ç”¨ OpenAI çš„ Chat Completion API é€šé OpenAIChatGenerator ä¾†å¯¦ç¾é¡ä»£ç†è¡Œç‚ºçš„æ‡‰ç”¨ç¨‹åºã€‚ç›¸é—œæ–‡ä»¶å¯åƒè€ƒ Haystack çš„ [OpenAIChatGenerator æ–‡ä»¶](https://docs.haystack.deepset.ai/docs/openaichatgenerator)ã€‚

<br>

4. OpenAI çš„ `å‡½æ•¸èª¿ç”¨åŠŸèƒ½` å°‡ `LLM` é€£æ¥åˆ°å¤–éƒ¨å·¥å…·ï¼Œé€šéå‘ `OpenAI API` èª¿ç”¨æä¾›å‡½æ•¸åˆ—è¡¨åŠå…¶è¦ç¯„å¯è¼•é¬†å»ºç«‹èŠå¤©åŠ©æ‰‹ï¼Œé€™äº›åŠ©æ‰‹å¯ä»¥é€šéèª¿ç”¨å¤–éƒ¨ API ä¾†å›ç­”å•é¡Œæˆ–å¾æ–‡æœ¬ä¸­æå–çµæ§‹åŒ–ä¿¡æ¯ã€‚

<br>

## é–‹ç™¼ä¹‹å‰

1. å®‰è£ `Haystack 2.0` å’Œ `sentence-transformers`ï¼šé€™æ˜¯å®˜æ–¹ç¯„ä¾‹æŒ‡å®šçš„ç‰ˆæœ¬ã€‚

    ```bash
    pip install haystack-ai "sentence-transformers>=2.2.0"
    ```

<br>

## OpenAIChatGenerator vs OpenAIGenerator

_å…ˆç°¡ä»‹å…©è€…å·®ç•°_

<br>

1. `OpenAIChatGenerator` å’Œ `OpenAIGenerator` éƒ½æ˜¯ `Haystack` æ¡†æ¶ä¸­çš„ `çµ„ä»¶`ï¼Œå…©è€…éƒ½å¯ä½¿ç”¨ `OpenAI API` é€²è¡Œ `NLP` ä»»å‹™ï¼Œä½†åŠŸèƒ½å’Œä½¿ç”¨å ´æ™¯æœ‰æ‰€ä¸åŒã€‚

<br>

2. `OpenAIChatGenerator` æ”¯æŒèª¿ç”¨ `OpenAI` çš„å‡½æ•¸èª¿ç”¨åŠŸèƒ½ï¼Œæ˜¯å°ˆé–€è¨­è¨ˆç”¨ä¾†èˆ‡ `OpenAI` çš„ `Chat Completion API` äº¤äº’ï¼Œæ”¯æŒè¤‡é›œå°è©±çš„ä¸Šä¸‹æ–‡ç®¡ç†å’Œå¤šè¼ªå°è©±ï¼Œé©åˆæ§‹å»ºèŠå¤©æ©Ÿå™¨äººæˆ–éœ€è¦ä¸Šä¸‹æ–‡ç†è§£çš„æ‡‰ç”¨ã€‚

<br>

3. `OpenAIChatGenerator` ä½¿ç”¨ `ChatMessage` åˆ—è¡¨é€²è¡Œé€šä¿¡ï¼Œæ¯å€‹ `ChatMessage` å°è±¡å¯ä»¥è¡¨ç¤ºä¸€æ¢ç”¨æˆ¶æˆ–ç³»çµ±æ¶ˆæ¯ï¼Œä¸¦åŒ…å«æ¶ˆæ¯çš„è§’è‰²å’Œå…·é«”çš„æ–‡æœ¬å…§å®¹ï¼Œè§’è‰²å‰‡å¯ä»¥æ˜¯ç”¨æˆ¶æˆ–åŠ©æ‰‹ã€‚

<br>

4. åœ¨ `OpenAIGenerator` éƒ¨åˆ†å‰‡èˆ‡ `OpenAIChatGenerator` ä¸åŒï¼Œ `OpenAIGenerator` æ˜¯ç”¨ä¾†èˆ‡ `OpenAI` çš„ `Completion API` äº’å‹•çš„ï¼Œä¸»è¦ç”¨æ–¼ç”Ÿæˆ `å–®è¼ªæ–‡æœ¬å›ç­”`ï¼Œé©åˆç°¡å–®çš„æ–‡æœ¬ç”Ÿæˆä»»å‹™ï¼Œå¦‚å›ç­”å•é¡Œã€æ–‡æœ¬çºŒå¯«ã€ç¿»è­¯ã€ç¸½çµç­‰ï¼Œä¸¦ä¸”ä¸æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†ã€‚

<br>

5. ä»¥ä¸‹ç¨‹å¼ç¢¼å±•ç¤ºå¦‚ä½•ä½¿ç”¨ `haystack` çš„ `OpenAIChatGenerator` ä¾†ç”Ÿæˆä¸€å€‹å°è©±ç³»çµ±ï¼Œé€™è£¡æœƒä½¿ç”¨ `OpenAI API`ï¼Œå»ºç«‹ç’°å¢ƒè®Šæ•¸éƒ¨åˆ†è«‹åƒè€ƒå¾ŒçºŒæ­¥é©Ÿï¼Œé€™è£¡ä¸è´…è¿°ã€‚

    ```python
    from haystack.dataclasses import ChatMessage
    from haystack.components.generators.chat import OpenAIChatGenerator

    # å»ºç«‹ç³»çµ±æ¶ˆæ¯å’Œç”¨æˆ¶æ¶ˆæ¯çš„ ChatMessage å°è±¡
    # ç³»çµ±æ¶ˆæ¯æç¤ºç”Ÿæˆçš„å›æ‡‰æ‡‰è©²å§‹çµ‚ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œå³ä½¿è¼¸å…¥æ˜¯å…¶ä»–èªè¨€
    # ç”¨æˆ¶æ¶ˆæ¯å‰‡æ˜¯æå• "ä»€éº¼æ˜¯è‡ªç„¶èªè¨€è™•ç†ï¼Ÿè¦ç°¡æ½”ã€‚"ã€‚
    messages = [
        ChatMessage.from_system(
            "å³ä½¿æŸäº›è¼¸å…¥è³‡æ–™æ¡ç”¨å…¶ä»–èªè¨€ï¼Œä¹Ÿå§‹çµ‚ä»¥ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚"
        ),
        ChatMessage.from_user(
            "ä»€éº¼æ˜¯è‡ªç„¶èªè¨€è™•ç†ï¼Ÿè¦ç°¡æ½”ã€‚"
        ),
    ]

    # åˆå§‹åŒ– OpenAIChatGenerator
    chat_generator = OpenAIChatGenerator(model="gpt-4-turbo")
    # å‚³å…¥æ¶ˆæ¯ä¸¦é‹è¡Œç”Ÿæˆå°è©±
    response = chat_generator.run(messages=messages)
    # è¼¸å‡ºæŸ¥çœ‹
    print(response)
    ```

<br>

6. è¼¸å‡ºå¦‚ä¸‹çµæœã€‚

    ```python
    {
        "replies": [
            ChatMessage(
                content="è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒä½¿è¨ˆç®—æ©Ÿèƒ½å¤ ç†è§£ã€è§£é‡‹å’Œç”Ÿæˆäººé¡èªè¨€ã€‚ä¸»è¦æ‡‰ç”¨åŒ…æ‹¬èªéŸ³è­˜åˆ¥ã€èªè¨€ç¿»è­¯å’Œæƒ…æ„Ÿåˆ†æã€‚",
                role=<ChatRole.ASSISTANT: "assistant">,
                name=None,
                meta={
                    "model": "gpt-4-turbo-2024-04-09",
                    "index": 0,
                    "finish_reason": "stop",
                    "usage": {
                        "completion_tokens": 82,
                        "prompt_tokens": 67,
                        "total_tokens": 149
                    }
                }
            )
        ]
    }
    ```

<br>

7. å„ªåŒ–è¼¸å‡ºã€‚

    ```python
    # å„ªåŒ–è¼¸å‡ºçµæœï¼ŒåŒ…å«ç”¨æˆ¶æ¶ˆæ¯
    for msg in messages:
        if msg.role == 'user':
            print(f"ç”¨æˆ¶: {msg.content}")
        elif msg.role == 'system':
            print(f"ç³»çµ±: {msg.content}")

    # è¼¸å‡ºç”Ÿæˆçš„å›æ‡‰
    for reply in response['replies']:
        print(f"åŠ©ç†: {reply.content}")
    ```

    _çµæœï¼š_
    ```bash
    ç³»çµ±: å³ä½¿æŸäº›è¼¸å…¥è³‡æ–™æ¡ç”¨å…¶ä»–èªè¨€ï¼Œä¹Ÿå§‹çµ‚ä»¥ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚
    ç”¨æˆ¶: ä»€éº¼æ˜¯è‡ªç„¶èªè¨€è™•ç†ï¼Ÿè¦ç°¡æ½”ã€‚
    åŠ©ç†: è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹åˆ†æ”¯ï¼Œå°ˆæ³¨æ–¼è®“è¨ˆç®—æ©Ÿç†è§£ã€è§£é‡‹ã€ä¸¦ç”Ÿæˆäººé¡èªè¨€çš„æŠ€è¡“ã€‚å…¶ç›®çš„æ˜¯è®“æ©Ÿå™¨èƒ½å¤ è™•ç†å’Œå›æ‡‰èªè¨€æ•¸æ“šï¼Œä»¥æ”¯æŒå„ç¨®æ‡‰ç”¨ï¼Œå¦‚èªéŸ³è­˜åˆ¥ã€æ©Ÿå™¨ç¿»è­¯å’Œæƒ…æ„Ÿåˆ†æç­‰ã€‚
    ```

<br>

8. ç°¡ä»‹æµå¼è™•ç†ï¼š `OpenAIChatGenerator` æ”¯æŒæµå¼è™•ç†ï¼Œä»¥ä¸‹æä¾›ä¸€å€‹ `streaming_callback` å‡½æ•¸ä¸¦é‡æ–°é‹è¡Œ `chat_generator` ä¾†æŸ¥çœ‹å·®ç•°ã€‚

    ```python
    from haystack.dataclasses import ChatMessage
    from haystack.components.generators.chat import OpenAIChatGenerator
    from haystack.components.generators.utils import print_streaming_chunk

    # ä½¿ç”¨æµå¼å›èª¿å‡½æ•¸åˆå§‹åŒ– OpenAIChatGenerator
    chat_generator = OpenAIChatGenerator(
        model="gpt-4-turbo",
        streaming_callback=print_streaming_chunk
    )
    # å‚³å…¥æ¶ˆæ¯ä¸¦é‹è¡Œ
    response = chat_generator.run(messages=messages)
    ```

<br>

## è­¦å‘Šæ’é™¤

1. é‹è¡Œä¸­ï¼Œå‡å¦‚å‡ºç¾å¦‚ä¸‹çš„éŒ¯èª¤è¨Šæ¯ `TqdmWarning: IProgress not found. Please update jupyter and ipywidgets.`ï¼Œé€™æ˜¯æç¤ºäº†ä¸€å€‹èˆ‡ `tqdm` å’Œ `Jupyter` æˆ– `ipywidgets` æœ‰é—œçš„å•é¡Œã€‚

    ![](images/img_78.png)

<br>

2. é€™åªæ˜¯ä¸€å€‹åŠŸèƒ½æ€§å•é¡Œæ‰€ä»¥ä¸åŠ è´…è¿°ï¼Œå¯é€éä»¥ä¸‹æŒ‡ä»¤é€²è¡Œæ’é™¤ã€‚

    ```bash
    pip install --upgrade jupyter ipywidgets
    ```

<br>

## é€²è¡Œé–‹ç™¼

1. å»ºç«‹ç’°å¢ƒè®Šé‡ã€‚

    ```python
    from getpass import getpass
    import os
    from dotenv import load_dotenv

    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    load_dotenv()
    # å…©å€‹ API çš„å¯†é‘°
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

    if "OPENAI_API_KEY" not in os.environ:
        os.environ["OPENAI_API_KEY"] = getpass("Enter OpenAI API key:")
    ```

<br>

2. å°å…¥ç›¸é—œçµ„ä»¶å¥—ä»¶ã€‚

    ```python
    from haystack import Pipeline, Document
    from haystack.document_stores.in_memory import InMemoryDocumentStore
    from haystack.components.writers import DocumentWriter
    from haystack.components.embedders import SentenceTransformersDocumentEmbedder
    ```

<br>

3. å»ºç«‹ç°¡å–®çš„æ–‡ä»¶æ•¸æ“šã€‚

    ```python
    # å»ºç«‹æ–‡ä»¶
    documents = [
        Document(content="My name is Jean and I live in Paris."),
        Document(content="My name is Mark and I live in Berlin."),
        Document(content="My name is Giorgio and I live in Rome."),
        Document(content="My name is Marta and I live in Madrid."),
        Document(content="My name is Harry and I live in London."),
    ]
    ```

<br>

4. å»ºç«‹ç´¢å¼•ç®¡é“ã€‚

    ```python
    # å»ºç«‹ç´¢å¼•ç®¡é“
    indexing_pipeline = Pipeline()
    ```

<br>

5. åŠ å…¥ç®¡é“çµ„ä»¶ã€‚

    ```python
    # åˆå§‹åŒ–å…§å­˜æ–‡ä»¶å„²å­˜çµ„ä»¶
    document_store = InMemoryDocumentStore()    
    # åœ¨ç®¡é“ä¸­åŠ å…¥çµ„ä»¶ï¼šå°‡æ–‡ä»¶å…§å®¹è½‰æ›æˆåµŒå…¥å‘é‡
    indexing_pipeline.add_component(
        instance=SentenceTransformersDocumentEmbedder(
            model="sentence-transformers/all-MiniLM-L6-v2"
        ),
        name="doc_embedder"
    )
    # åŠ å…¥çµ„ä»¶ï¼šå°‡è™•ç†å¾Œçš„æ–‡ä»¶æ•¸æ“šå¯«å…¥åˆ°æŒ‡å®šçš„æ–‡ä»¶å„²å­˜
    # æŒ‡å®šä½¿ç”¨ `å…§å­˜æ–‡ä»¶å„²å­˜`
    indexing_pipeline.add_component(
        instance=DocumentWriter(document_store=document_store),
        name="doc_writer"
    )
    ```

<br>

6. é€£æ¥çµ„ä»¶ã€‚

    ```python
    # é€£æ¥åµŒå…¥å™¨å’Œæ–‡ä»¶å¯«å…¥å™¨
    indexing_pipeline.connect(
        "doc_embedder.documents",
        "doc_writer.documents"
    )
    ```

<br>

7. è¼¸å‡ºã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x317e07d60>
    
    ğŸš… Components
        - doc_embedder: SentenceTransformersDocumentEmbedder
        - doc_writer: DocumentWriter
    
    ğŸ›¤ï¸ Connections
        - doc_embedder.documents -> doc_writer.documents (List[Document])
    ```

<br>

8. é‹è¡Œç®¡é“ã€‚

    ```python
    # é‹è¡Œç®¡é“
    indexing_pipeline.run({
        "doc_embedder": {"documents": documents}
    })
    ```

<br>

9. é‹è¡Œå¾Œé¡¯ç¤ºã€‚

    ![](images/img_36.png)

<br>

10. èª¿ç”¨è‡ªè¨‚å‡½æ•¸ã€‚

    ```python
    from utils.draw_pipeline import draw_and_display

    draw_and_display(indexing_pipeline, "ex10_1_pipe.png")
    ```

    ![](images/img_75.png)

<br>

## å»ºç«‹ç®¡é“

_å»ºç«‹åŸºæœ¬çš„ RAG ç®¡é“_

<br>

1. å°å…¥ç›¸é—œçµ„ä»¶å¥—ä»¶ã€‚

    ```python
    from haystack.components.embedders import SentenceTransformersTextEmbedder
    from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
    from haystack.components.builders import PromptBuilder
    from haystack.components.generators import OpenAIGenerator
    ```

<br>

2. å®šç¾©æ¨¡æ¿ã€‚

    ```python
    # å®šç¾©æç¤ºæ¨¡æ¿
    template = """
    æ ¹æ“šçµ¦å®šçš„ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚

    ä¸Šä¸‹æ–‡:
    {% for document in documents %}
        {{ document.content }}
    {% endfor %}
    å•é¡Œ: {{ question }}
    ç­”æ¡ˆ:
    """
    ```

<br>

3. å»ºç«‹ç®¡é“ã€‚

    ```python
    # å»ºç«‹ RAG ç®¡é“
    rag_pipe = Pipeline()
    ```

<br>

4. å»ºç«‹çµ„ä»¶ã€‚

    ```python
    rag_pipe.add_component(
        "embedder",
        SentenceTransformersTextEmbedder(
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
    )
    rag_pipe.add_component(
        "retriever",
        InMemoryEmbeddingRetriever(
            document_store=document_store
        )
    )
    rag_pipe.add_component(
        "prompt_builder",
        PromptBuilder(
            template=template
        )
    )
    rag_pipe.add_component(
        "llm",
        OpenAIGenerator(model="gpt-4-turbo")
    )
    ```

<br>

5. é€£çµçµ„ä»¶ã€‚

    ```python
    # é€£æ¥çµ„ä»¶
    rag_pipe.connect(
        "embedder.embedding",
        "retriever.query_embedding"
    )
    rag_pipe.connect(
        "retriever",
        "prompt_builder.documents"
    )
    rag_pipe.connect(
        "prompt_builder",
        "llm"
    )
    ```

<br>

6. æœƒè¼¸å‡ºä»¥ä¸‹è¨Šæ¯ã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x3169ca710>

    ğŸš… Components
        - embedder: SentenceTransformersTextEmbedder
        - retriever: InMemoryEmbeddingRetriever
        - prompt_builder: PromptBuilder
        - llm: OpenAIGenerator

    ğŸ›¤ï¸ Connections
        - embedder.embedding -> retriever.query_embedding (List[float])
        - retriever.documents -> prompt_builder.documents (List[Document])
        - prompt_builder.prompt -> llm.prompt (str)
    ```

<br>

7. è¼¸å‡ºç®¡é“åœ–ã€‚

    ```python
    draw_and_display(rag_pipe, "ex10_2_pipe.png")
    ```

    ![](images/img_76.png)

<br>

## é‹è¡Œç®¡é“

1. ä½¿ç”¨ä¸€å€‹æŸ¥è©¢ä¾†æ¸¬è©¦ç®¡é“ï¼Œä¸¦ç¢ºä¿å®ƒæŒ‰ç…§é æœŸå·¥ä½œï¼Œç„¶å¾Œå†å°‡å…¶ç”¨ä½œå‡½æ•¸èª¿ç”¨å·¥å…·ã€‚

    ```python
    query = "Mark ä½åœ¨å“ªè£¡ï¼Ÿ"
    rag_pipe.run({
        "embedder": {"text": query},
        "prompt_builder": {"question": query}
    })
    ```

<br>

2. æœƒé¡¯ç¤ºä»¥ä¸‹çµæœã€‚

    ![](images/img_37.png)

<br>

## å°‡ Haystack ç®¡é“è½‰æ›ç‚ºå·¥å…·

1. å°‡ `rag_pipe.run` èª¿ç”¨åŒ…è£æˆä¸€å€‹å‡½æ•¸ `rag_pipeline_func`ï¼Œé€™å€‹ `rag_pipeline_func` å‡½æ•¸å°‡æ¥å—ä¸€å€‹æŸ¥è©¢ä¸¦è¿”å›ä¾†è‡ª RAG ç®¡é“çš„ LLM çš„å›æ‡‰ï¼Œç„¶å¾Œå¯å°‡æ­¤å‡½æ•¸ä½œç‚ºå·¥å…·å¼•å…¥åˆ° `OpenAIChatGenerator` ä¸­ã€‚

    ```python
    def rag_pipeline_func(query: str):
        result = rag_pipe.run({
            "embedder": {"text": query},
            "prompt_builder": {"question": query}
        })
        return {"reply": result["llm"]["replies"][0]}
    ```

<br>

## å»ºç«‹å·¥å…·åˆ—è¡¨

1. é™¤äº† `rag_pipeline_func` å·¥å…·å¤–ï¼Œé‚„å»ºç«‹ä¸€å€‹åç‚º `get_current_weather` çš„æ–°å·¥å…·ï¼Œç”¨æ–¼ç²å– `åŸå¸‚çš„å¤©æ°£ä¿¡æ¯`ï¼Œä»¥ä¸‹å‡½æ•¸ä¸­ä½¿ç”¨ç¡¬ç·¨ç¢¼çš„æ•¸æ“šä¾†å±•ç¤ºåŠŸèƒ½ã€‚

    ```python
    WEATHER_INFO = {
        "Berlin": {
            "weather": "mostly sunny", "temperature": 7, "unit": "celsius"
        },
        "Paris": {
            "weather": "mostly cloudy", "temperature": 8, "unit": "celsius"
        },
        "Rome": {
            "weather": "sunny", "temperature": 14, "unit": "celsius"
        },
        "Madrid": {
            "weather": "sunny", "temperature": 10, "unit": "celsius"
        },
        "London": {
            "weather": "cloudy", "temperature": 9, "unit": "celsius"
        },
    }

    def get_current_weather(location: str):
        if location in WEATHER_INFO:
            return WEATHER_INFO[location]
        else:
            # å›é€€æ•¸æ“š
            return {
                "weather": "sunny",
                "temperature": 21.8,
                "unit": "fahrenheit"
            }
    ```

<br>

2. æ¥ä¸‹ä¾†ï¼ŒæŒ‰ç…§ `OpenAI` çš„å·¥å…·æ¶æ§‹ç‚º `rag_pipeline_func` å’Œ `get_current_weather` æ·»åŠ å‡½æ•¸èªªæ˜ã€‚è©³ç´°èªªæ˜ `rag_pipeline_func` å’Œ `query`ï¼Œä»¥ä¾¿ OpenAI å¯ä»¥ç”Ÿæˆé©ç•¶çš„åƒæ•¸ã€‚

    ```python
    tools = [
        {
            "type": "function",
            "function": {
                "name": "rag_pipeline_func",
                "description": "ç²å–æœ‰é—œäººå€‘å±…ä½åœ°é»çš„ä¿¡æ¯",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "æœå°‹ä¸­ä½¿ç”¨çš„æŸ¥è©¢ã€‚å¾ç”¨æˆ¶çš„æ¶ˆæ¯ä¸­æ¨æ–·å‡ºé€™ä¸€é»ã€‚å®ƒæ‡‰è©²æ˜¯ä¸€å€‹å•é¡Œæˆ–ä¸€å€‹é™³è¿°ã€‚",
                        }
                    },
                    "required": ["query"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "å–å¾—ç•¶å‰å¤©æ°£",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚åŠ å·èˆŠé‡‘å±±"
                        }
                    },
                    "required": ["location"],
                },
            },
        },
    ]
    ```

<br>

## ä½¿ç”¨å·¥å…·é‹è¡Œ OpenAIChatGenerator

1. è¦ä½¿ç”¨å‡½æ•¸èª¿ç”¨åŠŸèƒ½ï¼Œæ‚¨éœ€è¦åœ¨ `OpenAIChatGenerator` çš„ `run()` æ–¹æ³•ä¸­å‚³å…¥å·¥å…·åˆ—è¡¨ä½œç‚º `generation_kwargs`ã€‚ç”¨ç³»çµ±æ¶ˆæ¯æŒ‡å°æ¨¡å‹ä½¿ç”¨æä¾›çš„å·¥å…·ï¼Œç„¶å¾Œä½œç‚ºç”¨æˆ¶æ¶ˆæ¯æä¾›ä¸€å€‹éœ€è¦å‡½æ•¸èª¿ç”¨çš„æŸ¥è©¢ã€‚

    ```python
    from haystack.dataclasses import ChatMessage
    from haystack.components.generators.chat import OpenAIChatGenerator
    from haystack.components.generators.utils import print_streaming_chunk

    # å»ºç«‹æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç³»çµ±æ¶ˆæ¯å’Œç”¨æˆ¶æŸ¥è©¢
    messages = [
        ChatMessage.from_system(
            "ä¸è¦å‡è¨­å°‡å“ªäº›å€¼æ’å…¥å‡½æ•¸ä¸­ã€‚å¦‚æœç”¨æˆ¶è¦æ±‚ä¸æ˜ç¢ºï¼Œè«‹è¦æ±‚æ¾„æ¸…ã€‚"
        ),
        ChatMessage.from_user("ä½ èƒ½å‘Šè¨´æˆ‘ Mark ä½åœ¨å“ªè£¡å—ï¼Ÿ"),
    ]

    # åˆå§‹åŒ– OpenAIChatGenerator
    chat_generator = OpenAIChatGenerator(
        model="gpt-4-turbo",
        streaming_callback=print_streaming_chunk
    )
    # å‚³å…¥æ¶ˆæ¯å’Œå·¥å…·åˆ—è¡¨ä¸¦é‹è¡Œ
    response = chat_generator.run(
        messages=messages,
        generation_kwargs={"tools": tools}
    )
    # è¼¸å‡ºæŸ¥çœ‹
    print(response)
    ```

<br>

2. å°‡ç²å¾—ä¸€å€‹ä»¥ JSON æ ¼å¼é¡¯ç¤ºä¸”åŒ…å« `å·¥å…·åç¨±` å’Œ `åƒæ•¸` çš„ `ChatMessage`ã€‚

    ```python
    {
        'replies': [
            ChatMessage(
                content='[{"index": 0, "id": "call_2ARyzeivzPqS0ETx3jVHt4ct", "function": {"arguments": "{\\"query\\":\\"Where does Mark live?\\"}", "name": "rag_pipeline_func"}, "type": "function"}]',
                role=<ChatRole.ASSISTANT: 'assistant'>,
                name=None,
                meta={
                    'model': 'gpt-4-turbo-2024-04-09',
                    'index': 0,
                    'finish_reason': 'tool_calls',
                    'usage': {}
                }
            )
        ]
    }
    ```

<br>

3. æ¥ä¸‹ä¾†å°‡æ¶ˆæ¯å…§å®¹å­—ä¸²è§£æç‚º JSON ä¸¦ä½¿ç”¨æä¾›çš„åƒæ•¸èª¿ç”¨ç›¸æ‡‰çš„å‡½æ•¸ã€‚

    ```python
    import json

    # è§£æå‡½æ•¸èª¿ç”¨ä¿¡æ¯
    # æå–ç¬¬ä¸€å€‹å›æ‡‰ä¸­çš„ content
    content = response['replies'][0].content

    # å°‡ content è§£æç‚º JSON
    # content æ˜¯ä¸€å€‹ JSON å­—ä¸²ï¼Œéœ€è¦è½‰æ›ç‚º Python å­—å…¸
    function_calls = json.loads(content)

    # æå–ç¬¬ä¸€å€‹å‡½æ•¸èª¿ç”¨ä¿¡æ¯
    # æå–å‡½æ•¸èª¿ç”¨åˆ—è¡¨ä¸­çš„ç¬¬ä¸€å€‹å…ƒç´ 
    function_call = function_calls[0]

    # ç²å–å‡½æ•¸åç¨±
    # ç²å–å‡½æ•¸åç¨±ï¼Œé€™æ˜¯æˆ‘å€‘éœ€è¦èª¿ç”¨çš„å‡½æ•¸
    function_name = function_call['function']['name']

    # è§£æå‡½æ•¸åƒæ•¸
    # å°‡åƒæ•¸è§£æç‚ºå­—å…¸æ ¼å¼
    function_args = json.loads(function_call['function']['arguments'])

    # è¼¸å‡ºå‡½æ•¸åç¨±å’Œåƒæ•¸
    print("Function Name:", function_name)
    print("Function Arguments:", function_args)

    # å¯ç”¨å‡½æ•¸å­—å…¸
    available_functions = {
        "rag_pipeline_func": rag_pipeline_func,
        "get_current_weather": get_current_weather
    }

    # æœå°‹ç›¸æ‡‰çš„å‡½æ•¸ä¸¦ä½¿ç”¨çµ¦å®šçš„åƒæ•¸èª¿ç”¨å®ƒ
    if function_name in available_functions:
        # æ ¹æ“šå‡½æ•¸åç¨±æ‰¾åˆ°å°æ‡‰çš„å‡½æ•¸
        function_to_call = available_functions[function_name]
        # ä½¿ç”¨è§£åŒ…æ“ä½œå°‡åƒæ•¸å‚³éçµ¦å‡½æ•¸
        function_response = function_to_call(**function_args)
        # è¼¸å‡ºå‡½æ•¸çš„è¿”å›å€¼
        print("Function Response:", function_response)
    else:
        # å¦‚æœå‡½æ•¸åç¨±æœªæ‰¾åˆ°ï¼Œè¼¸å‡ºéŒ¯èª¤è¨Šæ¯
        print(f"Function {function_name} not found.")
    ```

<br>

4. å¾—åˆ°ä»¥ä¸‹çš„è¼¸å‡ºçµæœã€‚

    ```bash
    Function Name: rag_pipeline_func
    Function Arguments: {'query': 'Mark lives in which location?'}
    Function Response: {'reply': 'Mark lives in Berlin, query was: Mark lives in which location?'}
    ```

<br>

5. æœ€å¾Œä¸€æ­¥ï¼Œé€šéå°‡å‡½æ•¸å›æ‡‰é™„åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä½œç‚ºæ–°æ¶ˆæ¯ï¼Œä½¿ç”¨ `ChatMessage.from_function()` ä¸¦é‡æ–°é‹è¡Œ `OpenAIChatGenerator`ï¼Œè®“æ¨¡å‹ç¸½çµçµæœã€‚

    ```python
    from haystack.dataclasses import ChatMessage

    # å»ºç«‹å‡½æ•¸å›æ‡‰æ¶ˆæ¯
    function_message = ChatMessage.from_function(
        content=json.dumps(function_response),
        name=function_name
    )
    # å°‡å‡½æ•¸å›æ‡‰æ¶ˆæ¯æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
    messages.append(function_message)

    # å†æ¬¡é‹è¡Œ OpenAIChatGenerator
    response = chat_generator.run(
        messages=messages,
        generation_kwargs={"tools": tools}
    )
    ```

<br>

6. è¼¸å‡ºã€‚

    ![](images/img_38.png)

<br>

## å»ºç«‹èŠå¤©æ‡‰ç”¨ç¨‹åº

1. `OpenAI Chat Completions API` ä¸¦ä¸æœƒç›´æ¥èª¿ç”¨å‡½æ•¸ï¼›ç›¸åï¼Œæ¨¡å‹æœƒç”Ÿæˆå¯ä»¥åœ¨ç¨‹å¼ç¢¼ä¸­èª¿ç”¨çš„ JSONã€‚å› æ­¤ï¼Œç‚ºäº†å»ºç«‹ `ç«¯åˆ°ç«¯` çš„èŠå¤©æ‡‰ç”¨ç¨‹åºï¼Œéœ€è¦åœ¨æ¯æ¬¡æ¶ˆæ¯ä¸­æª¢æŸ¥ `OpenAI` å›æ‡‰æ˜¯å¦ç‚º `å·¥å…·èª¿ç”¨`ã€‚å¦‚æœæ˜¯ï¼Œéœ€è¦ä½¿ç”¨æä¾›çš„åƒæ•¸èª¿ç”¨ç›¸æ‡‰çš„å‡½æ•¸ï¼Œä¸¦å°‡å‡½æ•¸å›æ‡‰ç™¼é€å› OpenAIã€‚å¦å‰‡ï¼Œå°‡ç”¨æˆ¶å’Œæ¶ˆæ¯éƒ½é™„åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­ï¼Œèˆ‡æ¨¡å‹é€²è¡Œå¸¸è¦å°è©±ã€‚

<br>

2. è¦ç‚ºæ‡‰ç”¨ç¨‹åºå»ºç«‹ä¸€å€‹æ¼‚äº®çš„ç”¨æˆ¶ç•Œé¢ï¼Œå¯ä»¥ä½¿ç”¨å¸¶æœ‰èŠå¤©ç•Œé¢çš„ `Gradio`ã€‚å®‰è£ `gradio`ï¼Œé‹è¡Œä¸‹é¢çš„ç¨‹å¼ç¢¼å–®å…ƒï¼Œä¸¦ä½¿ç”¨è¼¸å…¥æ¡†èˆ‡å…·æœ‰è¨ªå•æ¬Šé™çš„èŠå¤©æ‡‰ç”¨ç¨‹åºé€²è¡Œäº¤äº’ã€‚

    ```bash
    pip install gradio
    ```

<br>

3. é›–ç„¶ `gradio` å·²æˆåŠŸå®‰è£ï¼Œä½†å‡ºç¾äº†ä¸€äº›ä¾è³´æ€§è¡çªï¼Œé€™æ˜¯å› ç‚º `farm-haystack` éœ€è¦çš„ `pydantic` ç‰ˆæœ¬èˆ‡ `gradio` éœ€è¦çš„ç‰ˆæœ¬ä¸ç›¸å®¹ï¼Œ`farm-haystack` è¦æ±‚ pydantic çš„ç‰ˆæœ¬å¿…é ˆ `å°æ–¼ 2`ï¼Œä½†å®‰è£çš„ pydantic ç‰ˆæœ¬æ˜¯ `2.7.3`ï¼Œé€™è£¡å…ˆå¿½ç•¥ä¸ç®¡ï¼Œçœ‹çœ‹å¾ŒçºŒé‹ä½œæƒ…æ³å†ä¾†è™•ç†ã€‚

    ![](images/img_39.png)

<br>

4. ç¨‹å¼ç¢¼ã€‚

    ```python
    import gradio as gr
    import json

    from haystack.dataclasses import ChatMessage
    from haystack.components.generators.chat import OpenAIChatGenerator

    # å®šç¾©å·¥å…·å‡½æ•¸
    def rag_pipeline_func(query: str):
        return {"reply": f"Giorgio ä½åœ¨ Berlin, query was: {query}"}

    def get_current_weather(location: str):
        return {"weather": "sunny", "temperature": 20, "location": location}

    # å¯ç”¨å‡½æ•¸å­—å…¸
    available_functions = {
        "rag_pipeline_func": rag_pipeline_func,
        "get_current_weather": get_current_weather
    }

    chat_generator = OpenAIChatGenerator(model="gpt-3.5-turbo")
    response = None
    messages = [
        ChatMessage.from_system(
            "ä¸è¦å‡è¨­å°‡å“ªäº›å€¼æ’å…¥å‡½æ•¸ä¸­ã€‚"
            "å¦‚æœç”¨æˆ¶è¦æ±‚ä¸æ˜ç¢ºï¼Œè«‹è¦æ±‚æ¾„æ¸…ã€‚"
        )
    ]

    # å®šç¾©èŠå¤©æ©Ÿå™¨äººå‡½æ•¸
    def chatbot_with_fc(message, history):
        messages.append(ChatMessage.from_user(message))
        response = chat_generator.run(messages=messages, generation_kwargs={"tools": tools})

        while True:
            # å¦‚æœ OpenAI å›æ‡‰æ˜¯ä¸€å€‹å·¥å…·èª¿ç”¨
            if response and response["replies"][0].meta["finish_reason"] == "tool_calls":
                try:
                    function_calls = json.loads(response["replies"][0].content)
                except json.JSONDecodeError as e:
                    print(f"è§£æ JSON ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    break

                print(response["replies"][0])
                for function_call in function_calls:
                    # è§£æå‡½æ•¸èª¿ç”¨ä¿¡æ¯
                    function_name = function_call["function"]["name"]
                    function_args = json.loads(function_call["function"]["arguments"])

                    # æª¢æŸ¥å‡½æ•¸æ˜¯å¦å­˜åœ¨
                    if function_name in available_functions:
                        function_to_call = available_functions[function_name]
                        try:
                            # ä½¿ç”¨è§£åŒ…æ“ä½œå°‡åƒæ•¸å‚³éçµ¦å‡½æ•¸
                            function_response = function_to_call(**function_args)
                        except TypeError as te:
                            print(f"å‡½æ•¸èª¿ç”¨éŒ¯èª¤ï¼š{te}")
                            continue

                        # ä½¿ç”¨ `ChatMessage.from_function` å°‡å‡½æ•¸å›æ‡‰æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
                        messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))
                        response = chat_generator.run(messages=messages, generation_kwargs={"tools": tools})
                    else:
                        print(f"å‡½æ•¸ {function_name} æœªæ‰¾åˆ°")
                        continue
            else:
                if response:
                    messages.append(response["replies"][0])
                break
        return response["replies"][0].content if response else "No response generated."

    # å»ºç«‹èŠå¤©ç•Œé¢
    demo = gr.ChatInterface(
        fn=chatbot_with_fc,
        examples=[
            "ç‘å…¸çš„é¦–éƒ½æ˜¯ä»€éº¼ï¼Ÿ",
            "ä½ èƒ½å‘Šè¨´æˆ‘ Giorgio ä½åœ¨å“ªè£¡å—ï¼Ÿ",
            "é¦¬å¾·é‡Œçš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ",
            "Madrid çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ",
            "èª°ä½åœ¨ London?",
            "Mark ä½çš„åœ°æ–¹çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ",
        ],
        title="è«‹è©¢å•æœ‰é—œå¤©æ°£æˆ–äººå€‘å±…ä½çš„åœ°æ–¹ã€‚",
    )
    ```

<br>

5. å•Ÿå‹•èŠå¤©è¦–çª—ã€‚

    ```python
    # å•Ÿå‹•èŠå¤©æ‡‰ç”¨ç¨‹åº
    demo.launch()
    ```

<br>

6. å•Ÿå‹• `Gradio` çš„è¦–çª—ç•Œé¢ã€‚

    ![](images/img_40.png)

<br>

7. å¯ä»¥å˜—è©¦çš„æŸ¥è©¢ã€‚

    ```bash
    "ç‘å…¸çš„é¦–éƒ½æ˜¯ä»€éº¼ï¼Ÿ"ï¼šä¸€å€‹åŸºæœ¬çš„æŸ¥è©¢ï¼Œæ²’æœ‰ä»»ä½•å‡½æ•¸èª¿ç”¨ã€‚
    "ä½ èƒ½å‘Šè¨´æˆ‘ Giorgio ä½åœ¨å“ªè£¡å—ï¼Ÿ"ï¼šä¸€å€‹åŸºæœ¬çš„æŸ¥è©¢ï¼Œå¸¶æœ‰ä¸€æ¬¡å‡½æ•¸èª¿ç”¨ã€‚
    "é¦¬å¾·é‡Œçš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ"
    "é‚£è£¡ç¾åœ¨æ˜¯æ™´å¤©å—ï¼Ÿ"ï¼šæŸ¥çœ‹æ¶ˆæ¯æ˜¯å¦è¢«è¨˜éŒ„ä¸¦ç™¼é€ã€‚
    "Jean ä½çš„åœ°æ–¹å¤©æ°£æ€éº¼æ¨£ï¼Ÿ"ï¼šå¼·åˆ¶èª¿ç”¨å…©æ¬¡å‡½æ•¸ã€‚
    "ä»Šå¤©çš„å¤©æ°£æ€éº¼æ¨£ï¼Ÿ"ï¼šå¼·åˆ¶ OpenAI è©¢å•æ›´å¤šæ¾„æ¸…å•é¡Œã€‚
    ```

<br>

___

_END_