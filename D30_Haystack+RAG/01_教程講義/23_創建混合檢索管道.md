# å»ºç«‹æ··åˆæª¢ç´¢ç®¡é“

_å¯åƒè€ƒ [æ··åˆæ–‡ä»¶æª¢ç´¢](https://haystack.deepset.ai/blog/hybrid-retrieval)ã€‚_

<br>

## ä½¿ç”¨çš„å…ƒä»¶

1. `DocumentSplitter`

2. `SentenceTransformersDocumentEmbedder`

3. `DocumentJoiner`

4. `InMemoryDocumentStore`

5. `InMemoryBM25Retriever`

6. `InMemoryEmbeddingRetriever`

7. `TransformersSimilarityRanker`

<br>

## èªªæ˜

1. ä½¿ç”¨ [Haystack 2.0](https://haystack.deepset.ai/overview/quick-start)ï¼Œå¯åƒè€ƒå®˜æ–¹ [Haystack 2.0 æ–‡ä»¶](https://haystack.deepset.ai/docs/latest)ã€‚

<br>

2. `æ··åˆæª¢ç´¢` çµåˆäº†åŸºæ–¼ `é—œéµè©` å’Œ `åŸºæ–¼åµŒå…¥` çš„ `æª¢ç´¢æŠ€è¡“`ï¼Œå…¶ä¸­ `åŸºæ–¼åµŒå…¥` çš„æ–¹æ³•åœ¨ç†è§£æŸ¥è©¢çš„ä¸Šä¸‹æ–‡ç´°ç¯€æ–¹é¢è¡¨ç¾å‡ºè‰²ï¼Œè€ŒåŸºæ–¼é—œéµè©çš„æ–¹æ³•å‰‡åœ¨åŒ¹é…å…·é«”çš„é—œéµè©ä¸Šå…·æœ‰å„ªå‹¢ã€‚

<br>

3. åœ¨è¨±å¤šæƒ…æ³ä¸‹ï¼Œä¾‹å¦‚åœ¨ç‰¹å®šé ˜åŸŸå¦‚é†«ç™‚ä¿å¥ä¸­ï¼Œç°¡å–®çš„ `åŸºæ–¼é—œéµè©çš„æ–¹æ³•ï¼ˆå¦‚ BM25ï¼‰` è¡¨ç¾å„ªæ–¼ `å¯†é›†æª¢ç´¢`ï¼Œå› ç‚º `å¯†é›†æ¨¡å‹` éœ€è¦åœ¨æ•¸æ“šä¸Šé€²è¡Œè¨“ç·´ã€‚

<br>

4. `InMemoryDocumentStore` æ˜¯æœ€ç°¡å–®çš„ `DocumentStore`ï¼Œå®ƒä¸éœ€è¦ä»»ä½•å¤–éƒ¨ä¾è³´ï¼Œæ˜¯è¼ƒå°é …ç›®å’Œèª¿è©¦çš„ç†æƒ³é¸æ“‡ï¼Œä½†å°æ–¼è¼ƒå¤§çš„æ–‡ä»¶é›†åˆä¾†èªªï¼Œå®ƒä¸¦ä¸å¤ªé©åˆç”Ÿç”¢ç³»çµ±ã€‚`Haystack` æ”¯æŒçš„ä¸åŒé¡å‹çš„å¤–éƒ¨è³‡æ–™åº«ï¼Œè«‹åƒé–± [DocumentStore Integrations](https://haystack.deepset.ai/docs/latest/components/stores)ã€‚

<br>

## å®‰è£ Haystack

1. ä½¿ç”¨ pip å®‰è£ Haystack 2.0 åŠå…¶ä»–æ‰€éœ€çš„åŒ…ã€‚

    ```bash
    pip install haystack-ai
    pip install "datasets>=2.6.1"
    pip install "sentence-transformers>=2.2.0"
    pip install accelerate
    ```

<br>

2. åˆå§‹åŒ– DocumentStoreï¼šé€šéåˆå§‹åŒ– `DocumentStore` ä¾†é–‹å§‹å»ºç«‹å•ç­”ç³»çµ±ã€‚`DocumentStore` ç”¨æ–¼å„²å­˜ç³»çµ±ç”¨æ–¼æœå°‹å•é¡Œç­”æ¡ˆçš„æ–‡ä»¶ã€‚åœ¨æœ¬ç¯„ä¾‹ä¸­å°‡ä½¿ç”¨ `InMemoryDocumentStore`ã€‚

    ```python
    from haystack.document_stores.in_memory import InMemoryDocumentStore

    document_store = InMemoryDocumentStore()
    ```

<br>

3. åœ¨ `Hugging Face Hub` ä¸Šæœ‰è¨±å¤šä¾†è‡ª PubMed çš„æ•¸æ“šé›†ï¼Œæ‰€ä»¥ç¯„ä¾‹å°‡ä½¿ç”¨ `anakin87/medrag-pubmed-chunk`ï¼Œé€é `for-in` å¾æ•¸æ“šé›†ä¸­å»ºç«‹æ–‡ä»¶ï¼Œåœ¨ `PubMed` çš„æ•¸æ“šé›†ä¸­ï¼Œæ¯å€‹æ•¸æ“šé»éƒ½æœ‰ 4 å€‹ç‰¹å¾µã€‚

   - `pmid`

   - `title`

   - `content`: æ‘˜è¦

   - `contents`: æ‘˜è¦ + æ¨™é¡Œ

<br>

4. åœ¨æœç´¢æ™‚å°‡ä½¿ç”¨ `contents` ç‰¹å¾µï¼Œè€Œå…¶ä»–ç‰¹å¾µå°‡ä½œç‚ºå…ƒæ•¸æ“šå„²å­˜ï¼Œä¸¦ç”¨æ–¼è¼¸å‡ºæœç´¢çµæœæˆ–é€²è¡Œå…ƒæ•¸æ“šç¯©é¸ã€‚

    ```python
    from datasets import load_dataset
    from haystack import Document

    dataset = load_dataset(
        "anakin87/medrag-pubmed-chunk",
        split="train"
    )

    docs = []
    for doc in dataset:
        docs.append(
            Document(
                content=doc["contents"],
                meta={
                    "title": doc["title"],
                    "abstract": doc["content"],
                    "pmid": doc["id"]
                }
            )
        )
    ```

<br>

## ä½¿ç”¨ç®¡é“ç´¢å¼•æ–‡ä»¶

_å»ºç«‹ä¸€å€‹ç®¡é“ï¼Œå°‡æ•¸æ“šå„²å­˜åœ¨ `æ–‡ä»¶å„²å­˜ document store` ä¸­ä¸¦ç”ŸæˆåµŒå…¥_

<br>

1. ä½¿ç”¨ `DocumentSplitter` å°‡æ–‡ä»¶åˆ†å‰²ç‚º 512 å€‹è©çš„å¡Šã€‚

<br>

2. ä½¿ç”¨ `SentenceTransformersDocumentEmbedder` ä¾†å»ºç«‹ç”¨æ–¼å¯†é›†æª¢ç´¢çš„æ–‡ä»¶åµŒå…¥ï¼›ä½œç‚ºåµŒå…¥æ¨¡å‹ï¼Œå°‡ä½¿ç”¨ `Hugging Face` ä¸Šçš„ `BAAI/bge-small-en-v1.5`ã€‚å¯å˜—è©¦åœ¨ `Hugging Face` ä¸Šæ¸¬è©¦å…¶ä»– `æ¨¡å‹` æˆ–ä½¿ç”¨å…¶ä»– `åµŒå…¥å™¨` ä¾†åˆ‡æ›æ¨¡å‹æä¾›å•†ï¼›å¦‚æœå¤ªéè€—æ™‚ï¼Œå¯å°‡æ›´æ”¹è¼ƒå°çš„åµŒå…¥æ¨¡å‹ï¼Œå¦‚ `sentence-transformers/all-MiniLM-L6-v2` æˆ– `sentence-transformers/all-mpnet-base-v2`ã€‚è«‹ç¢ºä¿æ ¹æ“šæ¨¡å‹çš„ token é™åˆ¶æ›´æ–° `split_length`ã€‚

<br>

3. ä½¿ç”¨ `DocumentWriter` å°‡æ–‡ä»¶å¯«å…¥æ–‡ä»¶å„²å­˜ã€‚

<br>

4. åœ¨ MacOS ä¸Šéœ€å®‰è£ä»¥ä¸‹å¥—ä»¶ã€‚

    ```bash
    pip install torch torchvision torchaudio
    ```

<br>

6. ç¨‹å¼ç¢¼ã€‚

    ```python
    from haystack.components.writers import DocumentWriter
    from haystack.components.embedders import SentenceTransformersDocumentEmbedder
    from haystack.components.preprocessors.document_splitter import DocumentSplitter
    from haystack import Pipeline
    from haystack.utils import ComponentDevice

    document_splitter = DocumentSplitter(
        split_by="word",
        split_length=512,
        split_overlap=32
    )
    document_embedder = SentenceTransformersDocumentEmbedder(
        model="BAAI/bge-small-en-v1.5",
        # MacOS ä¸ä½¿ç”¨ CUDA
        # device=ComponentDevice.from_str("cuda:0")
        device=ComponentDevice.from_str("cpu")
    )
    document_writer = DocumentWriter(document_store)

    indexing_pipeline = Pipeline()
    indexing_pipeline.add_component("document_splitter", document_splitter)
    indexing_pipeline.add_component("document_embedder", document_embedder)
    indexing_pipeline.add_component("document_writer", document_writer)

    indexing_pipeline.connect("document_splitter", "document_embedder")
    indexing_pipeline.connect("document_embedder", "document_writer")

    indexing_pipeline.run({"document_splitter": {"documents": docs}})
    ```

    _è‡³æ­¤æ–‡ä»¶å·²å„²å­˜åœ¨ `InMemoryDocumentStore` ä¸­ä¸¦ç”Ÿæˆäº†åµŒå…¥ï¼Œæ¥ä¸‹ä¾†å¯å»ºç«‹æ··åˆæª¢ç´¢ç®¡é“_

<br>

## å»ºç«‹æ··åˆæª¢ç´¢ç®¡é“

1. æ··åˆæª¢ç´¢æ˜¯æŒ‡çµåˆå¤šç¨®æª¢ç´¢æ–¹æ³•ä»¥å¢å¼·æ•´é«”æ€§èƒ½ã€‚

<br>

2. åœ¨æœç´¢ç³»çµ±çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæ··åˆæª¢ç´¢ç®¡é“åŒæ™‚åŸ·è¡Œå‚³çµ±çš„åŸºæ–¼é—œéµè©çš„æœç´¢å’Œå¯†é›†å‘é‡æœç´¢ï¼Œéš¨å¾Œä½¿ç”¨äº¤å‰ç·¨ç¢¼å™¨æ¨¡å‹å°çµæœé€²è¡Œæ’åã€‚

<br>

3. é€™ç¨®çµ„åˆå…è¨±æœç´¢ç³»çµ±åˆ©ç”¨ä¸åŒæ–¹æ³•çš„å„ªå‹¢ï¼Œæä¾›æ›´æº–ç¢ºå’Œå¤šæ¨£çš„çµæœã€‚

<br>

## æ­¥é©Ÿ

1. åˆå§‹åŒ–æª¢ç´¢å™¨å’ŒåµŒå…¥å™¨ï¼šåˆå§‹åŒ– `InMemoryEmbeddingRetriever` å’Œ `InMemoryBM25Retriever` ä»¥åŸ·è¡Œå¯†é›†æª¢ç´¢å’ŒåŸºæ–¼é—œéµè©çš„æª¢ç´¢ã€‚å°æ–¼å¯†é›†æª¢ç´¢ï¼Œé‚„éœ€è¦ä¸€å€‹ `SentenceTransformersTextEmbedder` ä¾†ä½¿ç”¨ç›¸åŒçš„åµŒå…¥æ¨¡å‹ `BAAI/bge-small-en-v1.5` è¨ˆç®—æœç´¢æŸ¥è©¢çš„åµŒå…¥ï¼Œè©²æ¨¡å‹åœ¨ç´¢å¼•ç®¡é“ä¸­å·²ä½¿ç”¨ã€‚

    ```python
    from haystack.components.retrievers.in_memory import InMemoryBM25Retriever, InMemoryEmbeddingRetriever
    from haystack.components.embedders import SentenceTransformersTextEmbedder

    text_embedder = SentenceTransformersTextEmbedder(
        model="BAAI/bge-small-en-v1.5",
        # device=ComponentDevice.from_str("cuda:0")
        device=ComponentDevice.from_str("cpu")
    )
    embedding_retriever = InMemoryEmbeddingRetriever(document_store)
    bm25_retriever = InMemoryBM25Retriever(document_store)
    ```

<br>

2. åˆä½µæª¢ç´¢çµæœï¼šHaystack æä¾›äº†å¤šç¨®åœ¨ `DocumentJoiner` ä¸­çš„åˆä½µæ–¹æ³•ï¼Œä»¥é©æ‡‰ä¸åŒçš„ä½¿ç”¨å ´æ™¯ï¼Œå¦‚åˆä½µï¼ˆmergeï¼‰å’Œå€’æ•¸èåˆï¼ˆreciprocal_rank_fusionï¼‰ã€‚åœ¨æ­¤ç¯„ä¾‹ä¸­ï¼Œå°‡ä½¿ç”¨é»˜èªçš„ä¸²è¯æ¨¡å¼ï¼ˆconcatenateï¼‰å°‡ä¾†è‡ªå…©å€‹æª¢ç´¢å™¨çš„æ–‡ä»¶åˆä½µï¼Œå› ç‚º `Ranker` å°‡æ˜¯ä¸»è¦å…ƒä»¶ï¼Œç”¨æ–¼æŒ‰ç›¸é—œæ€§å°æ–‡ä»¶é€²è¡Œæ’åã€‚

    ```python
    from haystack.components.joiners import DocumentJoiner

    document_joiner = DocumentJoiner()
    ```

<br>

3. å°çµæœé€²è¡Œæ’åï¼šä½¿ç”¨ `TransformersSimilarityRanker` æ ¹æ“šçµ¦å®šæœç´¢æŸ¥è©¢å°æ‰€æœ‰æª¢ç´¢åˆ°çš„æ–‡ä»¶çš„ç›¸é—œæ€§é€²è¡Œæ‰“åˆ†ã€‚æ­¤ç¯„ä¾‹ä¸­ï¼Œå°‡ä½¿ç”¨ `BAAI/bge-reranker-base` æ¨¡å‹ä¾†å°æª¢ç´¢åˆ°çš„æ–‡ä»¶é€²è¡Œæ’åï¼Œä½†å¯ä»¥å°‡æ­¤æ¨¡å‹æ›´æ”¹ç‚º Hugging Face ä¸Šçš„å…¶ä»–äº¤å‰ç·¨ç¢¼å™¨æ¨¡å‹ã€‚

    ```python
    from haystack.components.rankers import TransformersSimilarityRanker

    ranker = TransformersSimilarityRanker(model="BAAI/bge-reranker-base")
    ```

<br>

4. åœ¨é€™å€‹æ­¥é©Ÿè‹¥å‡ºç¾ `ImportError` éŒ¯èª¤ã€‚

    ```json
    ImportError: Failed to import 'accelerate'. Run 'pip install transformers[torch,sentencepiece]'. Original error: No module named 'accelerate'
    ```

<br>

5. é‡å°æç¤ºå®‰è£å¥—ä»¶ï¼Œç‰¹åˆ¥æ³¨æ„ï¼Œ`zshï¼ˆZ shellï¼‰` å°æ‹¬è™Ÿæœ‰ç‰¹æ®Šè™•ç†ï¼Œå°‡å®ƒå€‘è§£é‡‹ç‚ºæ¨¡å¼åŒ¹é…ç¬¦è™Ÿï¼Œè¦ä½¿ç”¨ `å¼•è™Ÿ` ä¾†é¿å…é€™äº›ç‰¹æ®Šå­—ä¸²è¢«è§£é‡‹ç‚º `æ¨¡å¼åŒ¹é…ç¬¦è™Ÿ`ã€‚

    ```bash
    pip install accelerate 'transformers[torch,sentencepiece]'
    ```

<br>

6. å»ºç«‹æ··åˆæª¢ç´¢ç®¡é“ï¼šå°‡æ‰€æœ‰åˆå§‹åŒ–çš„å…ƒä»¶æ·»åŠ åˆ°çš„ç®¡é“ä¸­ä¸¦é€²è¡Œé€£æ¥ã€‚

    ```python
    from haystack import Pipeline

    hybrid_retrieval = Pipeline()
    hybrid_retrieval.add_component("text_embedder", text_embedder)
    hybrid_retrieval.add_component("embedding_retriever", embedding_retriever)
    hybrid_retrieval.add_component("bm25_retriever", bm25_retriever)
    hybrid_retrieval.add_component("document_joiner", document_joiner)
    hybrid_retrieval.add_component("ranker", ranker)

    hybrid_retrieval.connect("text_embedder", "embedding_retriever")
    hybrid_retrieval.connect("bm25_retriever", "document_joiner")
    hybrid_retrieval.connect("embedding_retriever", "document_joiner")
    hybrid_retrieval.connect("document_joiner", "ranker")
    ```

<br>

7. é€™å€‹æ­¥é©Ÿå°‡è¼¸å‡ºå¦‚ä¸‹è¨Šæ¯ã€‚

    ```bash
    <haystack.core.pipeline.pipeline.Pipeline object at 0x3280baef0>

    ğŸš… Components
        - text_embedder: SentenceTransformersTextEmbedder
        - embedding_retriever: InMemoryEmbeddingRetriever
        - bm25_retriever: InMemoryBM25Retriever
        - document_joiner: DocumentJoiner
        - ranker: TransformersSimilarityRanker

    ğŸ›¤ï¸ Connections
        - text_embedder.embedding -> embedding_retriever.query_embedding (List[float])
        - embedding_retriever.documents -> document_joiner.documents (List[Document])
        - bm25_retriever.documents -> document_joiner.documents (List[Document])
        - document_joiner.documents -> ranker.documents (List[Document])
    ```

<br>

8. å¯è¦–åŒ–ç®¡é“ï¼ˆå¯é¸ï¼‰ï¼šè¦äº†è§£å¦‚ä½•å»ºç«‹æ··åˆæª¢ç´¢ç®¡é“ï¼Œä½¿ç”¨ `draw()` æ–¹æ³•ã€‚å¦‚æœåœ¨ Google Colab ä¸Šé‹è¡Œæ­¤ç­†è¨˜æœ¬ï¼Œç”Ÿæˆçš„æ–‡ä»¶å°‡ä¿å­˜åœ¨å´é‚Šæ¬„çš„ `Files` éƒ¨åˆ†ã€‚

    ```python
    hybrid_retrieval.draw("hybrid-retrieval.png")
    ```

<br>

9. æœƒè¼¸å‡ºä¸€å¼µåœ–ã€‚

    ![](images/img_41.png)

<br>

## æ¸¬è©¦æ··åˆæª¢ç´¢

1. å°‡æŸ¥è©¢å‚³éçµ¦ `text_embedder`, `bm25_retriever` å’Œ `ranker`ï¼Œä¸¦é‹è¡Œæª¢ç´¢ç®¡é“ã€‚

    ```python
    # å¬°å…’å‘¼å¸æš«åœ
    query = "apnea in infants"

    result = hybrid_retrieval.run(
        {
            "text_embedder": {"text": query},
            "bm25_retriever": {"query": query},
            "ranker": {"query": query}
        }
    )
    # æŸ¥çœ‹ä¸€ä¸‹
    print(result)
    ```

    ![](images/img_07.png)


<br>

2. å»ºç«‹ä¸€å€‹å‡½æ•¸ä¾†è¼¸å‡ºæœç´¢çµæœçš„é é¢ã€‚

    ```python
    def pretty_print_results(prediction):
        for doc in prediction["documents"]:
            print(doc.meta["title"], "\t", doc.score)
            print(doc.meta["abstract"])
            print("\n", "\n")
            
    pretty_print_results(result["ranker"])
    ```

    ![](images/img_06.png)

<br>

3. é€²ä¸€æ­¥å„ªåŒ–è¼¸å‡ºæ ¼å¼ã€‚

    ```python
    def pretty_print_results(prediction):
        # æª¢æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶è¢«æª¢ç´¢åˆ°
        if not prediction["ranker"]["documents"]:
            print("æ²’æœ‰æª¢ç´¢åˆ°ç›¸é—œæ–‡ä»¶ã€‚")
            return

        # ç¾åŒ–è¼¸å‡º
        for idx, doc in enumerate(prediction["ranker"]["documents"]):
            print(f"\næ–‡ä»¶ {idx + 1}:")
            print(f"æ¨™é¡Œ: {doc.meta.get('title', 'ç„¡æ¨™é¡Œ')}")
            print(f"PMID: {doc.meta.get('pmid', 'ç„¡PMID')}")
            print(f"åˆ†æ•¸: {doc.score:.4f}")
            print(f"æ‘˜è¦: {doc.meta.get('abstract', 'ç„¡æ‘˜è¦')}")
            print("-" * 80)

    # ç¾åŒ–ä¸¦è¼¸å‡ºçµæœ
    pretty_print_results(result)
    ```

    ![](images/img_05.png)

<br>

## ä¸­æ–‡æª¢ç´¢

1. å¦‚åŒä¹‹å‰çš„ç¯„ä¾‹ä¸€æ¨£ï¼Œè‹¥è¦é€²è¡Œä¸­æ–‡æª¢ç´¢ï¼Œå¿…é ˆé€²è¡Œä¸­æ–‡åˆ†è©ï¼Œé¦–å…ˆæ˜¯å®‰è£åˆ†è©å¥—ä»¶ã€‚

    ```bash
    pip install jieba translate
    ```

<br>

2. å°å…¥åº«ï¼Œåœ¨é€²è¡Œæª¢ç´¢ä¹‹å‰ï¼Œå°æŸ¥è©¢å’Œæ–‡æœ¬é€²è¡Œåˆ†è©ã€‚

    ```python
    import jieba

    # å®šç¾©åˆ†è©å‡½æ•¸
    def tokenize(text):
        return " ".join(jieba.cut(text))

    # å°æŸ¥è©¢é€²è¡Œåˆ†è©
    query = "å¬°å…’å‘¼å¸æš«åœè©²æ€è¾¦ï¼Ÿ"
    tokenized_query = tokenize(query)
    # æŸ¥çœ‹åˆ†è©å¾Œçš„å•é¡Œ
    print(tokenized_query)

    result = hybrid_retrieval.run(
        {
            "text_embedder": {"text": tokenized_query},
            "bm25_retriever": {"query": tokenized_query},
            "ranker": {"query": tokenized_query}
        }
    )

    # è¼¸å‡ºç¾åŒ–ç‰ˆçµæœ
    pretty_print_results(result)
    ```

<br>

3. å› ç‚ºæ–‡ä»¶æ˜¯è‹±æ–‡çš„ï¼Œæ‰€ä»¥ä»¥è‹±æ–‡è¼¸å‡ºã€‚

    ![](images/img_08.png)

<br>

4. ä½¿ç”¨ç¿»è­¯åº« `translate` é€²è¡Œè‡ªå‹•ç¿»è­¯ï¼Œé¦–å…ˆå®‰è£åº«ã€‚

    ```bash
    pip install translate
    ```

<br>

5. ç¨‹å¼ç¢¼ã€‚

    ```python
    from translate import Translator
    import jieba

    # åˆå§‹åŒ–ç¿»è­¯å™¨
    translator = Translator(to_lang="zh-tw")

    # å®šç¾©åˆ†è©å‡½æ•¸
    def tokenize(text):
        return " ".join(jieba.cut(text))

    # å®šç¾©æ–‡æœ¬æˆªæ–·å‡½æ•¸
    def truncate_text(text, max_length=400):
        if len(text) > max_length:
            return text[:max_length] + "..."
        return text

    # å®šç¾©ç¿»è­¯å‡½æ•¸
    def translate_to_chinese(text):
        return translator.translate(text)

    # å°æŸ¥è©¢é€²è¡Œåˆ†è©
    query = "å¬°å…’å‘¼å¸æš«åœæ€¥æ•‘æªæ–½"
    tokenized_query = tokenize(query)

    # æŸ¥çœ‹åˆ†è©å¾Œçš„æŸ¥è©¢
    print(f"åˆ†è©å¾Œçš„æŸ¥è©¢: {tokenized_query}")

    # é€²è¡Œæª¢ç´¢
    result = hybrid_retrieval.run(
        {
            "text_embedder": {"text": tokenized_query},
            "bm25_retriever": {"query": tokenized_query},
            "ranker": {"query": tokenized_query}
        }
    )

    # æª¢æŸ¥çµæœæ˜¯å¦æœ‰å…§å®¹
    print("æª¢ç´¢çµæœ:")
    pprint.pprint(result)

    # ç¾åŒ–è¼¸å‡ºçµæœä¸¦é€²è¡Œç¿»è­¯
    def pretty_print_and_translate_results(prediction):
        if "ranker" not in prediction or not prediction["ranker"]["documents"]:
            print("æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡ä»¶ã€‚")
            return

        for idx, doc in enumerate(prediction["ranker"]["documents"], start=1):
            truncated_title = truncate_text(doc.meta['title'])
            truncated_abstract = truncate_text(doc.meta['abstract'])
            truncated_content = truncate_text(doc.content)

            translated_title = translate_to_chinese(truncated_title)
            translated_abstract = translate_to_chinese(truncated_abstract)
            translated_content = translate_to_chinese(truncated_content)

            print(f"æ–‡ä»¶ {idx}:")
            print(f"æ¨™é¡Œ: {translated_title}")
            print(f"æ‘˜è¦: {translated_abstract}")
            print(f"å…§å®¹: {translated_content}")
            print("\n" + "-" * 80 + "\n")

    # è¼¸å‡ºä¸¦ç¿»è­¯çµæœ
    pretty_print_and_translate_results(result)
    ```

<br>

6. ä½†é€™å€‹å¥—ä»¶æœ‰è«¸å¤šé™é¡ï¼Œå¦‚ä¸‹é¡¯ç¤ºçš„æ¯æ—¥é™é¡ `YOU USE ALL AVAILABLE FREE TRANSLATIONS FOR TODAY`ã€‚

    ![](images/img_09.png)

<br>

7. å¦å¤–ï¼Œæ¯å¥é•·åº¦ä¹Ÿæœ‰é™åˆ¶ `QUERY LENGTH LIMIT EXCEEDED. MAX ALLOWED QUERY : 500 CHARS`ï¼Œé€™éƒ¨åˆ†æ·»åŠ ä¸€å€‹ `æˆªæ–·å‡½æ•¸` ä¸¦è¨­ç½®åƒæ•¸ `max_length` å°‡ç¿»è­¯æ–‡æœ¬ç¸®çŸ­åˆ°é™åˆ¶ä»¥å…§ï¼›å¦å¤–ï¼Œé€™è£¡åƒ…ä½œç¤ºç¯„ï¼Œæ‰€ä»¥é€éåˆ‡ç‰‡ `[:2]`è¼¸å‡ºå‰å…©å€‹é …ç›®ã€‚

    ```python
    from translate import Translator
    import jieba

    # åˆå§‹åŒ–ç¿»è­¯å™¨
    translator = Translator(to_lang="zh-tw")

    # å®šç¾©åˆ†è©å‡½æ•¸
    def tokenize(text):
        return " ".join(jieba.cut(text))

    # å®šç¾©æ–‡æœ¬æˆªæ–·å‡½æ•¸
    def truncate_text(text, max_length=400):
        if len(text) > max_length:
            return text[:max_length] + "..."
        return text

    # å®šç¾©ç¿»è­¯å‡½æ•¸
    def translate_to_chinese(text):
        return translator.translate(text)

    # å°æŸ¥è©¢é€²è¡Œåˆ†è©
    query = "å¬°å…’å‘¼å¸æš«åœæ€¥æ•‘æªæ–½"
    tokenized_query = tokenize(query)

    # æŸ¥çœ‹åˆ†è©å¾Œçš„æŸ¥è©¢
    print(f"åˆ†è©å¾Œçš„æŸ¥è©¢: {tokenized_query}")

    # é€²è¡Œæª¢ç´¢
    result = hybrid_retrieval.run(
        {
            "text_embedder": {"text": tokenized_query},
            "bm25_retriever": {"query": tokenized_query},
            "ranker": {"query": tokenized_query}
        }
    )

    # æª¢æŸ¥çµæœæ˜¯å¦æœ‰å…§å®¹
    print("æª¢ç´¢çµæœ:")
    pprint.pprint(result)

    # ç¾åŒ–è¼¸å‡ºçµæœä¸¦é€²è¡Œç¿»è­¯
    def pretty_print_and_translate_results(prediction):
        if "ranker" not in prediction or not prediction["ranker"]["documents"]:
            print("æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡ä»¶ã€‚")
            return
        # é€™è£¡åƒ…ä½œç¤ºç¯„ï¼Œæ‰€ä»¥è¼¸å‡ºå‰å…©é …ç›®å³å¯ [:2]
        for idx, doc in enumerate(prediction["ranker"]["documents"][:2], start=1):
            truncated_title = truncate_text(doc.meta['title'])
            truncated_abstract = truncate_text(doc.meta['abstract'])
            truncated_content = truncate_text(doc.content)

            translated_title = translate_to_chinese(truncated_title)
            translated_abstract = translate_to_chinese(truncated_abstract)
            translated_content = translate_to_chinese(truncated_content)

            print(f"æ–‡ä»¶ {idx}:")
            print(f"æ¨™é¡Œ: {translated_title}")
            print(f"æ‘˜è¦: {translated_abstract}")
            print(f"å…§å®¹: {translated_content}")
            print("\n" + "-" * 80 + "\n")

    # è¼¸å‡ºä¸¦ç¿»è­¯çµæœ
    pretty_print_and_translate_results(result)

    ```

<br>

8. è¼¸å‡ºçµæœå¦‚ä¸‹ã€‚

    ![](images/img_42.png)

<br>

_æš«ä¸”åˆ°é€™_

<br>

___

_END_