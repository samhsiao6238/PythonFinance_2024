# å­¸ç¿’ç‡ `Learning Rate`

_å­¸ç¿’ç‡åœ¨æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’æ¨¡å‹è¨“ç·´ä¸­æ±ºå®šæ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¨¡å‹åƒæ•¸æ›´æ–°çš„æ­¥å¹…å¤§å°ï¼Œç›´æ¥å½±éŸ¿æ¨¡å‹æ”¶æ–‚çš„é€Ÿåº¦å’Œç©©å®šæ€§_

<br>

## å­¸ç¿’ç‡çš„è§’è‰²

1. æ§åˆ¶åƒæ•¸æ›´æ–°çš„æ­¥å¹…å¤§å°ï¼Œåœ¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼Œæ¨¡å‹åƒæ•¸çš„æ›´æ–°å…¬å¼å¦‚ä¸‹ï¼›å­¸ç¿’ç‡æ§åˆ¶äº†æ¯æ¬¡åƒæ•¸æ›´æ–°çš„æ­¥å¹…å¤§å°ï¼Œéå¤§çš„å­¸ç¿’ç‡æœƒå°è‡´æ›´æ–°æ­¥å¹…éå¤§ï¼Œå¯èƒ½è·³éæå¤±å‡½æ•¸çš„æœ€å°å€¼ï¼›éå°çš„å­¸ç¿’ç‡æœƒå°è‡´æ›´æ–°æ­¥å¹…éå°ï¼Œä½¿å¾—æ¨¡å‹æ”¶æ–‚é€Ÿåº¦è®Šæ…¢ï¼›è¼ƒå°çš„å­¸ç¿’ç‡èƒ½æä¾›æ›´ç²¾ç´°çš„åƒæ•¸èª¿æ•´ï¼Œæœ‰åŠ©æ–¼æ‰¾åˆ°æ›´ç²¾ç¢ºçš„æœ€å°å€¼ï¼Œä½†éœ€è¦æ›´å¤šçš„è¨ˆç®—è³‡æºå’Œæ™‚é–“ã€‚

    ![](images/img_55.png)

    ![](images/img_56.png)

<br>

2. ä¸åŒæ•¸æ“šé›†å¯èƒ½éœ€è¦ä¸åŒçš„å­¸ç¿’ç‡ï¼Œè€Œè¼ƒæ·±æˆ–è¼ƒè¤‡é›œçš„æ¨¡å‹å¯èƒ½éœ€è¦è¼ƒå°çš„å­¸ç¿’ç‡ï¼Œè¼ƒå¤§çš„å­¸ç¿’ç‡å¯èƒ½åŠ å¿«è¨“ç·´ï¼Œä½†éœ€é¿å…æ¨¡å‹éœ‡ç›ªã€‚

<br>

## èª¿æ•´å­¸ç¿’ç‡çš„æ–¹æ³•

1. é€šéå¯¦é©—ä¾†æ‰¾åˆ°æœ€ä½³çš„ `å›ºå®šå­¸ç¿’ç‡ï¼ˆFixed Learning Rateï¼‰` ã€‚

<br>

2. ä½¿ç”¨ `å­¸ç¿’ç‡è¡°æ¸›ï¼ˆLearning Rate Decayï¼‰` æ–¹å¼éš¨è‘—è¨“ç·´çš„é€²å±•é€æ¼¸æ¸›å°å­¸ç¿’ç‡ã€‚

<br>

3. ä½¿ç”¨ `è‡ªé©æ‡‰å­¸ç¿’ç‡æ–¹æ³•ï¼ˆAdaptive Learning Rate Methodsï¼‰`ï¼Œå¦‚ `Adagrad`ã€`RMSprop`ã€`Adam` ç­‰ä¾†è‡ªå‹•èª¿æ•´å­¸ç¿’ç‡ã€‚

<br>

4. ä½¿ç”¨ `é ç†±å­¸ç¿’ç‡ï¼ˆLearning Rate Warm-upï¼‰`ï¼Œä¹Ÿå°±æ˜¯é–‹å§‹æ™‚ä½¿ç”¨è¼ƒå°çš„å­¸ç¿’ç‡ï¼Œç„¶å¾Œé€æ¼¸å¢åŠ ã€‚

<br>

## ç¯„ä¾‹

1. ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•é€²è¡Œå„ªåŒ–ï¼Œå…¶ä¸­å±•ç¤ºäº†å­¸ç¿’ç‡åœ¨åƒæ•¸æ›´æ–°ä¸­çš„ä½œç”¨ã€‚

    ```python
    import numpy as np

    # å®šç¾©ä¸€å€‹ç°¡å–®çš„äºŒæ¬¡æå¤±å‡½æ•¸
    def loss_function(theta):
        return theta ** 2

    # å®šç¾©æå¤±å‡½æ•¸çš„æ¢¯åº¦
    def gradient(theta):
        return 2 * theta

    # æ¢¯åº¦ä¸‹é™ç®—æ³•
    def gradient_descent(initial_theta, learning_rate, num_iterations):
        theta = initial_theta
        for i in range(num_iterations):
            grad = gradient(theta)
            theta = theta - learning_rate * grad
            current_loss = loss_function(theta)
            print(f"Iteration {i+1}: theta = {theta:.4f}, loss = {current_loss:.4f}")
        return theta

    # è¨­å®šåˆå§‹æ¢ä»¶
    # åˆå§‹åƒæ•¸
    initial_theta = 10.0
    # å­¸ç¿’ç‡
    learning_rate = 0.1
    # è¿­ä»£æ¬¡æ•¸
    num_iterations = 20

    # åŸ·è¡Œæ¢¯åº¦ä¸‹é™
    final_theta = gradient_descent(initial_theta, learning_rate, num_iterations)
    print(
        f"Final theta after {num_iterations} iterations: {final_theta:.4f}"
    )
    ```

<br>

2. çµæœã€‚

    ```bash
    Iteration 1: theta = 8.0000, loss = 64.0000
    Iteration 2: theta = 6.4000, loss = 40.9600
    # çœç•¥ ...
    Iteration 18: theta = 0.1801, loss = 0.0325
    Iteration 19: theta = 0.1441, loss = 0.0208
    Iteration 20: theta = 0.1153, loss = 0.0133
    Final theta after 20 iterations: 0.1153
    ```

<br>

3. å¯è¦–åŒ–çµæœå¦‚ä¸‹ï¼Œå·¦åœ–é¡¯ç¤ºäº†æå¤±å‡½æ•¸å€¼éš¨è‘—è¿­ä»£æ¬¡æ•¸çš„æ¸›å°‘ï¼Œå±•ç¤ºäº†æ¨¡å‹é€æ¼¸æ”¶æ–‚çš„éç¨‹ã€‚
å³åœ–ï¼ˆTheta Updates over Iterationsï¼‰ é¡¯ç¤ºäº†åƒæ•¸ ğœƒ çš„è®ŠåŒ–ï¼Œå±•ç¤ºäº†åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œğœƒ å¦‚ä½•é€æ­¥æ›´æ–°ä»¥æœ€å°åŒ–æå¤±å‡½æ•¸ã€‚

    ![](images/img_57.png)

<br>

___

_END_