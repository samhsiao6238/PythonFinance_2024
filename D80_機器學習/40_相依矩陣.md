# 相依矩陣

_機器學習中的 `相依矩陣`與統計學中的 `相關矩陣（correlation matrix）` 或 `共變異矩陣（covariance matrix）` 的概念基本上相同，用於表示不同變數間的相依性或關聯性；在機器學習中通常稱變數為 `參數`，表達的名稱上是可以交換的；另外，用於表達關係的元素則稱為 `係數` 或 `相關係數`。_

## 參數間的相依性

_相關矩陣是一種 `方形矩陣`，每個矩陣的元素代表兩個參數之間的相關性_

1. 相依矩陣展示了數據集中各個參數間的相依性，可用於判斷兩個參數間關係的強弱以及方向；其相關方式包括正相關、負相關與無相關。

2. 兩參數正相關時，當一個參數增加，另一個參數也會增加，相關係數大於 `0` 而向 `1` 接近；負相關則與負相關反方向，相關係數小於 `0` 而趨近於 `-1`；當兩參數無相關時，表示兩個參數間沒有明顯的 `線性關係`，相關係數接近 `0`。

3. 可藉由這個相關性了解參數間是否存在 `多重共線性（multicollinearity）`，因為這對線性迴歸等模型會造成不良影響。

## 相關矩陣的應用

_基於前述說明，說明相關矩陣在機器學習中的應用_

1. 特徵選擇（Feature Selection）：如果兩個變數之間有很高的正負相關性，說明它們可能是冗餘的，建模時僅需加入其中一個參數，而不需要同時使用；特別注意，評估時同時考量了正負相關性，因為無論是正相關或負相關，在機器學習中的效果是等效的。

2. 數據可視化：可以用相關矩陣來繪製熱力圖，清晰地看到不同變數之間的相關程度。

3. 避免共線性：在某些機器學習模型中，例如線性迴歸，高相關性的變數可能會導致共線性問題，這會影響模型的準確性，可通過相依矩陣來識別和處理共線性參數。

## 代碼範例

1. 使用 numpy 和 pandas 計算相關矩陣。

```python
import numpy as np
import pandas as pd

# 建立樣本數據
data = {
    'X': [1, 2, 3, 4, 5],
    'Y': [2, 4, 6, 8, 10],
    'Z': [5, 4, 3, 2, 1]
}

# 將數據轉為 DataFrame
df = pd.DataFrame(data)

# 計算相關矩陣
correlation_matrix = df.corr()
print("相關矩陣:\n", correlation_matrix)
```

2. 輸出相關矩陣的結果，斜對角以外的係數就是兩者間的相關係數，表示 X 和 Y 之間有非常高的正相關，而 Z 和 X 、 Y 之間有強烈的負相關。

```bash
相關矩陣:
      X    Y    Z
X    1.0  1.0 -1.0
Y    1.0  1.0 -1.0
Z   -1.0 -1.0  1.0
```


## 共變異矩陣的應用

1. 顧名思義，`共變異矩陣` 用來衡量兩個變數間的 `共變性（covariance）`，它不僅提供了變數間的相依性程度，還能表明變數的量綱如何影響相依性；所稱的 `量綱` 是指物理量的基本屬性，通常用來表示物理量的單位關係，它描述的是物理量的性質，而不涉及具體的數值大小，例如 `速度`的量綱為長度除以時間，可用 `L/T` 表示。

2. 與相關矩陣不同，共變異矩陣中元素會基於變數的尺度變化，因此當變數的尺度改變時，矩陣中的數值也會隨之變動；反之，相關矩陣將變數的值標準化而介於 `-1 ~ 1` 之間，不受變數尺度的影響，專注於變數間的相對關聯性。

3. 共變異矩陣在 `降維技術` 中扮演重要角色，如 `主成分分析法 PCA` 用來找到數據中的主要變異方向，並進行降維處理；另外，在 `多元高斯分佈模型` 中，共變異矩陣用來描述數據的分散程度與方向，這對於一些 `混合模型（GMM）` 尤為重要。

## 範例說明

1. 安裝套件。

```bash
pip install scikit-learn seaborn
```

2. 觀察數據。

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 設定支持中文的字體，避免顯示錯誤
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
# 用來正常顯示負號
plt.rcParams['axes.unicode_minus'] = False

# 假設 df 是原始數據
df = pd.DataFrame({
    'X1': [-2, -1, 0, 1, 2],
    'X2': [-4, -2, 0, 2, 4]
})

# 顯示基本統計訊息
print(df.describe())

# 繪製原始數據的散點圖
sns.scatterplot(x='X1', y='X2', data=df)
plt.title('原始數據')
plt.show()
```

3. 輸出結果。

```bash
             X1         X2
count    5.000000    5.000000
mean     0.000000    0.000000
std      1.581139    3.162278
min     -2.000000   -4.000000
25%     -1.000000   -2.000000
50%      0.000000    0.000000
75%      1.000000    2.000000
max      2.000000    4.000000
```

![](images/img_159.png)

4. 轉換。

```python
from sklearn.decomposition import PCA

# 進行 PCA 分析，將數據降為 2 維
pca = PCA(n_components=2)
principal_components = pca.fit_transform(df)

# 將主成分轉換為 DataFrame
principal_df = pd.DataFrame(
    principal_components,
    columns=['PC1', 'PC2']
)

# 繪製 PCA 之後的數據
sns.scatterplot(x='PC1', y='PC2', data=principal_df)
plt.title('PCA 轉換後的數據')
plt.show()
```

![](images/img_160.png)

5. 共變異矩陣 (Covariance Matrix)。

```python
import numpy as np
import pandas as pd

# 建立樣本數據
data = {
    'X': [1, 2, 3, 4, 5],
    'Y': [2, 4, 6, 8, 10],
    'Z': [5, 4, 3, 2, 1]
}

# 將數據轉為 DataFrame
df = pd.DataFrame(data)
# 計算共變異矩陣
covariance_matrix = df.cov()
print("共變異矩陣:\n", covariance_matrix)
```

2. 輸出共變異矩陣，顯示數據間的變異程度，X 和 Y 有很高的共變異，顯示正相關。

```bash
共變異矩陣:
      X     Y     Z
X    2.5   5.0  -2.5
Y    5.0  10.0  -5.0
Z   -2.5  -5.0   2.5
```

3. 主成分分析（PCA） 是一種降維方法，通過共變異矩陣找到數據中主要的變異方向。在實際應用中，PCA 幫助我們將多維數據轉換為更少的變量。

```python
from sklearn.decomposition import PCA

# 進行 PCA 分析
pca = PCA(n_components=2)
principal_components = pca.fit_transform(df)

# 將主成分轉換為 DataFrame
principal_df = pd.DataFrame(
    principal_components,
    columns=['PC1', 'PC2']
)
print("主成分:\n", principal_df)
```

4. 輸出結果，PCA 將數據轉換為兩個主要成分，可以用於數據降維或視覺化； PCA 在兩個變量 X1 和 X2 中找到了一個新的坐標軸，稱為主成分軸 PC1，這個軸捕捉了數據中最大變異的方向。在這個案例中，這個主要的變異方向與原來的數據是線性相關的。也就是說，數據的變異主要集中在這個軸上；另外， PCA 將次要變異（也就是變異很小的方向）移除，這就是 PC2 的作用。你可以看到，PC2 的值非常小，接近於 0，這表明數據在這個次要方向上幾乎沒有變化；降維的目的是透過找到數據中的主要變異方向，來減少數據的維度。數據的變化主要集中在 PC1 上，因此我們可以忽略 PC2 的次要變異，這樣可以將原始的 2 維數據（X1 和 X2）轉換為 1 維數據（PC1），而幾乎不會丟失信息。

```bash
主成分:
        PC1          PC2
0   -4.472136   -3.910328e-16
1   -2.236068    1.303443e-16
2    0.000000   -0.000000e+00
3    2.236068   -1.303443e-16
4    4.472136   -2.606886e-16
```