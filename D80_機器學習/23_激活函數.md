# 激活函數

_Activation Functions；在機器學習和深度學習中，激活函數是神經網絡中非常重要的一部分。_

## 說明


1. 引入非線性：激活函數的主要功能是為神經網絡引入 `非線性特性`，使得神經網絡能夠學習和表示複雜的非線性映射。如果沒有激活函數，神經網絡層與層之間僅是 `線性變換`，無法有效學習複雜數據的特徵。

2. 影響梯度流動：激活函數影響著 `反向傳播` 時 `梯度` 的流動，選擇合適的激活函數可以`避免梯度消失` 或 `梯度爆炸`，從而有助於模型的穩定訓練。

3. 控制輸出範圍：不同的激活函數有不同的輸出範圍，例如 Sigmoid 和 Tanh 函數壓縮輸出到特定範圍內，適合於特定的應用場景（如二分類）。

4. 提高模型的表達能力：使用合適的激活函數可以提高神經網絡的表達能力，從而使得模型能夠學習到數據的複雜模式。

激活函數的選擇通常取決於具體的應用場景、數據特性和神經網絡結構，選擇合適的激活函數對於構建有效的深度學習模型至關重要。

### 1. Sigmoid 函數
- 公式:  
  \[
  \sigma(x) = \frac{1}{1 + e^{-x}}
  \]
- 範圍: (0, 1)
- 特點:
  - 將輸入壓縮到 (0, 1) 之間。
  - 常用於二元分類問題的輸出層。
  - 在極端值（很大或很小）時，梯度接近於零，會導致梯度消失問題，從而減慢神經網絡的訓練。
  
### 2. Tanh 函數（雙曲正切函數）
- 公式:  
  \[
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  \]
- 範圍: (-1, 1)
- 特點:
  - 將輸入壓縮到 (-1, 1) 之間。
  - 與 Sigmoid 函數相比，Tanh 函數的輸出範圍更大（-1 到 1），因此在 Sigmoid 函數的範圍內有更強的梯度。
  - 也會遇到梯度消失問題，但通常比 Sigmoid 更好。

### 3. ReLU 函數（Rectified Linear Unit）
- 公式:  
  \[
  \text{ReLU}(x) = \max(0, x)
  \]
- 範圍: [0, ∞)
- 特點:
  - 將所有負數輸入壓縮為0，正數輸入保持不變。
  - 計算效率高，廣泛應用於隱藏層。
  - 避免了 Sigmoid 和 Tanh 函數的梯度消失問題。
  - 可能會遇到死亡 ReLU問題：當輸入一直為負時，神經元的權重無法更新，導致神經元“死亡”。

### 4. Leaky ReLU 函數
- 公式:  
  \[
  \text{Leaky ReLU}(x) = 
  \begin{cases} 
      x & \text{if } x > 0 \\
      \alpha x & \text{if } x \leq 0
  \end{cases}
  \]
- 範圍: (-∞, ∞)
- 特點:
  - 是 ReLU 的變體，為負輸入引入了一個小斜率（通常是 0.01）。
  - 旨在解決 ReLU 的“死亡神經元”問題。

### 5. Parametric ReLU 函數（PReLU）
- 公式:  
  \[
  \text{PReLU}(x) = 
  \begin{cases} 
      x & \text{if } x > 0 \\
      \alpha x & \text{if } x \leq 0
  \end{cases}
  \]
- 範圍: (-∞, ∞)
- 特點:
  - 與 Leaky ReLU 類似，但 α 是可訓練的參數。
  - 能夠學習最佳的負斜率，提供更多的靈活性。

### 6. Softmax 函數
- 公式:  
  \[
  \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
  \]
- 範圍: (0, 1)
- 特點:
  - 將輸入轉換為概率分佈，使得輸出的所有數值加起來等於1。
  - 常用於多分類問題的輸出層。

### 7. Swish 函數
- 公式:  
  \[
  \text{Swish}(x) = x \cdot \sigma(x)
  \]
- 範圍: (-∞, ∞)
- 特點:
  - 是 Sigmoid 函數和輸入的乘積。
  - 具備自我調整的非線性特性，並且在一些深度學習模型中性能優於 ReLU。

### 激活函數的主要功能和角色
