# 優化函數：分類問題

_也稱為損失函數，用於衡量模型的預測結果與實際結果之間的差異，模型訓練的目標是最小化這個差異，以提高模型的預測能力。_


### 3. 交叉熵損失（Cross-Entropy Loss）

定義：交叉熵損失主要用於分類問題，它度量了預測概率分布與真實分布之間的差異。當真實標籤為 1 時，預測概率越接近 1，損失越小。

公式（二分類交叉熵）：
\[
\text{Cross-Entropy Loss} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
\]

Python 程式碼示例：

```python
from sklearn.metrics import log_loss

# 假設有一些真實標籤 y 和預測概率 y_pred_proba
y = np.array([1, 0, 1, 0])
y_pred_proba = np.array([0.9, 0.1, 0.8, 0.4])

# 計算交叉熵損失
cross_entropy = log_loss(y, y_pred_proba)
print(f"Cross-Entropy Loss: {cross_entropy}")
```

在機器學習中的應用：交叉熵損失用於訓練分類模型，特別是二分類或多分類的神經網絡。

### 4. 二元交叉熵（Binary Cross-Entropy Loss）

定義：這是交叉熵損失的特例，專門用於二元分類問題。它度量模型預測的概率分布和真實分布之間的差異。

Python 程式碼示例：

```python
# 對於二元分類問題，log_loss 也可用於計算二元交叉熵損失
binary_cross_entropy = log_loss(y, y_pred_proba)
print(f"Binary Cross-Entropy Loss: {binary_cross_entropy}")
```

### 6. 對比損失（Contrastive Loss）

定義：對比損失主要用於度量學習，尤其是訓練孿生網絡（Siamese Networks），它用於最小化相似對之間的距離，同時最大化不同對之間的距離。

公式：
\[
L = \frac{1}{2N} \sum_{i=1}^{N} \left[ y_i \cdot D^2 + (1 - y_i) \cdot \max(0, m - D)^2 \right]
\]

其中，\(D\) 是樣本對的距離，\(m\) 是邊界距離。

Python 程式碼示例：

```python
import torch
import torch.nn.functional as F

def contrastive_loss(y, d, margin=1.0):
    loss = 0.5 * (y * d  2 + (1 - y) * F.relu(margin - d)  2)
    return loss.mean()

# 假設有一些相似度標籤 y 和樣本對之間的距離 d
y = torch.Tensor([1, 0, 1, 0])
d = torch.Tensor([0.5, 2.0, 1.0, 3.0])

# 計算對比損失
loss = contrastive_loss(y, d)
print(f"Contrastive Loss: {loss.item()}")
```

在機器學習中的應用：對比損失主要用於訓練孿生網絡（Siamese Networks）或其他需要學習距離度量的模型。